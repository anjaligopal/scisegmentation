{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+2nBvqrQlI60wRLdscy1D"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEpmSvcq7FUa"
      },
      "source": [
        "# Classification Model for scI Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41mydFCV7P_g"
      },
      "source": [
        "This model performs classification on an single-cell immunoblotting (scI) dataset.\n",
        "\n",
        "We gratefully acknowledge Prof. Jonathan Shewchuk and the teaching assistants of the Spring 2019 offering of UC Berkeleyâ€™s CS289A course for helpful discussions and initial code for some of this work.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFGd19a-xXfo"
      },
      "source": [
        "## Notebook Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1ZWQNqtxVSR",
        "outputId": "c2646775-13ff-4d13-82eb-ddc467ebb106"
      },
      "source": [
        "## Mounting to Google Colab\n",
        "## Comment out if not using Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')  \n",
        "\n",
        "## Loading libraries\n",
        "\n",
        "# Classic libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit learn libraries\n",
        "from skimage import io, transform\n",
        "from PIL import Image\n",
        "from sklearn import metrics\n",
        "\n",
        "# Pytorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "import ipdb\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl0mCCqLyBJE"
      },
      "source": [
        "## File Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtHvjNQPxyRC"
      },
      "source": [
        "# Change this to the working directory\n",
        "os.chdir('/content/gdrive/experiments')\n",
        "\n",
        "# Change this to where the data is stored\n",
        "dataset_path = '/content/gdrive/datasets/'\n",
        "\n",
        "# Datasets\n",
        "train_file = dataset_path+'classification_train.csv';\n",
        "test_file = dataset_path+'classification_test.csv';\n",
        "val_file = dataset_path+'classification_validate.csv';\n",
        "\n",
        "# Final model output\n",
        "model_save_file = 'classification_model.ckpt'; "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls4x7Xs2HvRR"
      },
      "source": [
        "## Dataset Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSje2f_XQF7n"
      },
      "source": [
        "def imgLoad(imagePath):\n",
        "    '''\n",
        "    Loads an image using the PIL Image library\n",
        "    Converts it to int64 and to x-dim * y-dim * 1 \n",
        "    to make its size compatible with pytorch tensors.\n",
        "    '''\n",
        "\n",
        "    image = np.array(Image.open(imagePath)).astype('int64');\n",
        "    return image\n",
        "\n",
        "class roiData(data.Dataset):\n",
        "    ''' Object representing a dataset. Code adapted from\n",
        "    CS289 HW6 assignment, Spring 2019 offering @ UC Berkeley.\n",
        "\n",
        "    This object stores an input data file. When called as roidata[i], \n",
        "    then the object will return the sample name and label of the ith\n",
        "    data point.\n",
        "\n",
        "    NOTE: PyTorch kept having issues with normalization, so it's performed \n",
        "    manually.\n",
        "    '''   \n",
        "    def __init__(self, label_file, img_load_function, transform=None, dataset_path=None, normalize=None):\n",
        "        'Initialization'\n",
        "\n",
        "        self.label_file = label_file\n",
        "        self.loader = img_load_function\n",
        "        self.data = pd.read_csv(self.label_file,header=0)[['roiPath','Class']]\n",
        "        self.transform = transform;\n",
        "        self.normalize = normalize; \n",
        "\n",
        "        # Option to add a path to the image files, if located in some other\n",
        "        # folder\n",
        "        if dataset_path is not None:\n",
        "          sample_header = self.data.columns[0];\n",
        "          self.data[sample_header] = dataset_path+self.data[sample_header]\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        'Generates one sample of data'\n",
        "        path,label = self.data.iloc[idx]\n",
        "        label = int(label);\n",
        "        sample = self.loader(path)\n",
        "\n",
        "        if self.normalize is not None:\n",
        "          sample = (sample - self.normalize[0])/self.normalize[1];\n",
        "        if self.transform is not None:\n",
        "          sample = self.transform(sample);\n",
        "\n",
        "        return sample,label\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL1tEtxEHH1o"
      },
      "source": [
        "## Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m6-LPn0G7sL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e1fef91-5a9e-490d-aa7f-3b8cadeb8472"
      },
      "source": [
        "train_dataset = roiData(train_file,imgLoad,transform=None,dataset_path=dataset_path);\n",
        "\n",
        "batchSize = 10;\n",
        "train_loader = data.DataLoader(train_dataset, batch_size = batchSize, shuffle = False, num_workers = 2);\n",
        "\n",
        "mean = [];\n",
        "meansq = [];\n",
        "sample_length = [];\n",
        "\n",
        "for sample, label in train_loader:\n",
        "  mean.append(np.mean(np.array(sample)))\n",
        "  meansq.append(np.mean(np.array(sample**2)))\n",
        "  sample_length.append(len(label))\n",
        "  \n",
        "batch_p = np.array(sample_length)/len(train_dataset);\n",
        "\n",
        "sample_mean = np.sum(batch_p*np.array(mean));\n",
        "sample_var = np.sum(batch_p*np.array(meansq)) - (sample_mean ** 2);\n",
        "sample_std = np.sqrt(sample_var);\n",
        "\n",
        "print(\"Sample mean: {}\".format(sample_mean))\n",
        "print(\"Sample stdev: {}\".format(sample_std))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample mean: 12158.262775479183\n",
            "Sample stdev: 8848.8094012705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-0eCR4QO0in"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvIGUZvSOuHo"
      },
      "source": [
        "# Dataset transformations for training\n",
        "train_transform = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.RandomHorizontalFlip()\n",
        "                                       ]);\n",
        "\n",
        "# Dataset transformation to turn into a tensor\n",
        "tensor_transform = transforms.Compose([transforms.ToTensor()]);\n",
        "\n",
        "# Number of workers\n",
        "workers = 2; \n",
        "\n",
        "# Batch size\n",
        "batchSize = 16;\n",
        "\n",
        "train_dataset = roiData(train_file,imgLoad,transform=train_transform,dataset_path=dataset_path,normalize=[sample_mean,sample_std]);\n",
        "test_dataset = roiData(test_file,imgLoad,transform=tensor_transform,dataset_path=dataset_path,normalize=[sample_mean,sample_std]);\n",
        "val_dataset = roiData(val_file,imgLoad,transform=tensor_transform,dataset_path=dataset_path,normalize=[sample_mean,sample_std]);\n",
        "\n",
        "train_loader = data.DataLoader(train_dataset, batch_size = batchSize, shuffle = True, num_workers = workers);\n",
        "test_loader = data.DataLoader(test_dataset, batch_size = batchSize, shuffle = False, num_workers = workers);\n",
        "val_loader = data.DataLoader(val_dataset, batch_size = batchSize, shuffle = False, num_workers = workers);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKV3-fRj0EvL"
      },
      "source": [
        "## Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWfX9_8Kx6x_"
      },
      "source": [
        "## Model Hyperparameters\n",
        "\n",
        "## Hyperparameters\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 25;\n",
        "\n",
        "# Learning Rate\n",
        "learning_rate = 5E-6;\n",
        "\n",
        "# Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Model\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        \n",
        "        # Defining the model\n",
        "\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=2, stride = 1, padding=0)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 192, kernel_size=2, stride = 1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(192, 384, kernel_size=2, stride = 1)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.conv4 = nn.Conv2d(384, 256, kernel_size=2, stride = 1)\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = nn.Conv2d(256, 256, kernel_size=2, stride = 1)\n",
        "        self.pool5 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout();\n",
        "        \n",
        "        self.fc1 = nn.Linear(1280,2048)\n",
        "        self.fc2 = nn.Linear(2048,4096)\n",
        "        self.fc3 = nn.Linear(4096,2048)\n",
        "        self.fc4 = nn.Linear(2048,2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass\n",
        "                \n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x));\n",
        "        x = self.pool2(x);\n",
        "        x = F.relu(self.conv3(x));\n",
        "        x = self.pool3(x)\n",
        "        x = F.relu(self.conv4(x));\n",
        "        x = self.pool4(x);\n",
        "        x = F.relu(self.conv5(x));\n",
        "        x = self.pool5(x);\n",
        "          \n",
        "        # FC layers\n",
        "        x = x.view(-1, np.product(np.array(x.shape[1:]))) \n",
        "        x = self.dropout(x);\n",
        "        x = F.relu(self.fc1(x));\n",
        "        x = self.dropout(x); \n",
        "        x = F.relu(self.fc2(x));\n",
        "        x = self.dropout(x);\n",
        "        x = F.relu(self.fc3(x));\n",
        "        x = self.fc4(x);\n",
        "        return x\n",
        "\n",
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model = NeuralNet().to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79CVlom_PMx1"
      },
      "source": [
        "## Running the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o5t5yuPGyY4Q",
        "outputId": "d95ece39-e5b5-4495-96a7-f21ebd7e5673"
      },
      "source": [
        "##################################\n",
        "#                                #\n",
        "#          TRAINING              #\n",
        "#                                #\n",
        "##################################\n",
        " \n",
        "# Initializing variables\n",
        "training_loss = [];\n",
        "training_predictions = [];\n",
        "training_labels = [];\n",
        "training_accuracy = [];\n",
        "\n",
        "print('Beginning training')\n",
        "total_step = len(train_loader) # To calculate total number of steps. \n",
        "start = time.time();\n",
        "\n",
        "for epoch in np.arange(num_epochs):\n",
        "  \n",
        "    # Batch training\n",
        "    model.train()  \n",
        "    print('epoch {}'.format(epoch+1))\n",
        "\n",
        "    for i, (local_batch,local_labels) in enumerate(train_loader):\n",
        "        local_batch = local_batch.float();\n",
        "\n",
        "        # Transfer to GPU\n",
        "        local_ims, local_labels = local_batch.to(device), local_labels.to(device)  \n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model.forward(local_ims)\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(outputs, local_labels)\n",
        "        training_loss.append(loss.tolist())\n",
        "\n",
        "        # Predictions\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Accuracy score\n",
        "        score = metrics.accuracy_score(predicted.cpu().numpy(),local_labels.cpu().numpy());\n",
        "        training_accuracy.append(score)\n",
        "\n",
        "        # If last epoch, save the predictions\n",
        "        if epoch == num_epochs-1:\n",
        "          training_predictions.extend(predicted);\n",
        "          training_labels.extend(local_labels); \n",
        "        \n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Printing results\n",
        "        if (i+1) % 5 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "            print('Training accuracy {} %'.format(100*score))\n",
        "            print('Time: '+str(time.time()-start))\n",
        "    \n",
        "\n",
        "##################################\n",
        "#                                #\n",
        "#          VALIDATION            #\n",
        "#                                #\n",
        "##################################\n",
        " \n",
        "model.eval()\n",
        "\n",
        "val_loss = [];\n",
        "val_labels = [];\n",
        "val_pred = [];\n",
        "val_p = [];\n",
        "\n",
        "print(\"Starting validation\")\n",
        "for i, (local_batch,local_labels) in enumerate(val_loader):\n",
        "    \n",
        "    # Loading data\n",
        "    local_batch = local_batch.float();\n",
        "    local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
        "    \n",
        "    # Evaluation\n",
        "    outputs = model.forward(local_ims)\n",
        "\n",
        "    # Getting probabilities\n",
        "    val_p.extend(F.softmax(outputs.data,dim=1).cpu().numpy());\n",
        "    \n",
        "    # Validation loss\n",
        "    loss = criterion(outputs, local_labels)\n",
        "    val_loss.append(loss.tolist())\n",
        "\n",
        "    # Predictions via max     \n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    val_pred.extend(predicted.cpu().numpy())\n",
        "    val_labels.extend(local_labels.cpu().numpy()) # Don't need this if we never shuffle the validation set, but added for consistency\n",
        "\n",
        "    if (i+1) % 5 == 0:\n",
        "      print(\"Validation \"+str(i+1))\n",
        "\n",
        "  # Accuracy Score\n",
        "val_score = metrics.accuracy_score(val_pred,val_labels);\n",
        "print('Validation accuracy {:.4f} %'.format(100 * val_score))\n",
        "\n",
        "end = time.time()\n",
        "torch.save(model.state_dict(), model_save_file)\n",
        "\n",
        "print(\"Validation ROC Curve\")\n",
        "\n",
        "\n",
        "val_p_zeroclass = np.matrix(val_p)[:,0];\n",
        "\n",
        "[val_tpr,val_fpr,val_thresholds] = metrics.roc_curve(val_labels,val_p_zeroclass)\n",
        "plt.plot(val_fpr,val_tpr)\n",
        "plt.plot(np.linspace(0,1,len(val_tpr)),np.linspace(0,1,len(val_fpr)),'--k')\n",
        "plt.title(\"Validation ROC Curve \\n AUC: {:.4f}\".format(metrics.auc(val_fpr,val_tpr)));\n",
        "plt.show()\n",
        "\n",
        "J_threshold = val_thresholds[np.argmax(val_tpr - val_fpr)];\n",
        "print(\"Threshold: {:.4f}\".format(J_threshold));\n",
        "\n",
        "val_pred_threshold = np.array(val_p_zeroclass<=J_threshold).astype(int);\n",
        "\n",
        "print(metrics.classification_report(val_labels,val_pred_threshold)); \n",
        "\n",
        "\n",
        "##################################\n",
        "#                                #\n",
        "#            TESTING             #\n",
        "#                                #\n",
        "##################################\n",
        "model.eval()\n",
        "\n",
        "test_labels = [];\n",
        "test_pred = [];\n",
        "test_accuracy = [];\n",
        "test_p = [];\n",
        "\n",
        "predicted_list = []\n",
        "groundtruth_list = []\n",
        "test_p = [];\n",
        "\n",
        "print(\"Starting testing\")\n",
        "for i, (local_batch,local_labels) in enumerate(test_loader):\n",
        "\n",
        "\n",
        "    # Loading data\n",
        "    local_batch = local_batch.float();\n",
        "    local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
        "    \n",
        "    # Model evaluation\n",
        "    outputs = model.forward(local_ims)\n",
        "\n",
        "    # Getting probabilities\n",
        "    test_p.extend(F.softmax(outputs.data,dim=1).cpu().numpy());\n",
        "    \n",
        "    # Predictions via max\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    test_pred.extend(predicted.cpu().numpy())\n",
        "    test_labels.extend(local_labels.cpu().numpy())\n",
        "\n",
        "    if (i+1) % 5 == 0:\n",
        "      print(\"Testing \"+str(i+1))\n",
        "\n",
        "test_score = metrics.accuracy_score(test_pred,test_labels);      \n",
        "print('Testing accuracy {:.4f} %'.format(100 * test_score))\n",
        "\n",
        "print(\"Testing ROC Curve\")\n",
        "\n",
        "test_p_zeroclass = np.matrix(test_p)[:,0];\n",
        "\n",
        "[test_tpr,test_fpr,test_thresholds] = metrics.roc_curve(test_labels,test_p_zeroclass)\n",
        "plt.plot(test_fpr,test_tpr)\n",
        "plt.plot(np.linspace(0,1,len(test_tpr)),np.linspace(0,1,len(test_fpr)),'--k')\n",
        "plt.title(\"Testing ROC Curve \\n AUC: {:.4f}\".format(metrics.auc(test_fpr,test_tpr)));\n",
        "plt.show()\n",
        "\n",
        "print(\"Using Validation Threshold: {:.4f}\".format(J_threshold));\n",
        "\n",
        "test_pred_threshold = np.array(test_p_zeroclass<=J_threshold).astype(int);\n",
        "\n",
        "print(metrics.classification_report(test_labels,test_pred_threshold)); "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training accuracy 93.75 %\n",
            "Time: 277.38463854789734\n",
            "Epoch [12/25], Step [185/607], Loss: 0.0144\n",
            "Training accuracy 100.0 %\n",
            "Time: 277.59599900245667\n",
            "Epoch [12/25], Step [190/607], Loss: 0.0057\n",
            "Training accuracy 100.0 %\n",
            "Time: 277.7769958972931\n",
            "Epoch [12/25], Step [195/607], Loss: 0.0283\n",
            "Training accuracy 100.0 %\n",
            "Time: 277.9717900753021\n",
            "Epoch [12/25], Step [200/607], Loss: 0.0144\n",
            "Training accuracy 100.0 %\n",
            "Time: 278.1748948097229\n",
            "Epoch [12/25], Step [205/607], Loss: 0.2043\n",
            "Training accuracy 93.75 %\n",
            "Time: 278.3627598285675\n",
            "Epoch [12/25], Step [210/607], Loss: 0.0432\n",
            "Training accuracy 100.0 %\n",
            "Time: 278.5712342262268\n",
            "Epoch [12/25], Step [215/607], Loss: 0.2250\n",
            "Training accuracy 93.75 %\n",
            "Time: 278.7823996543884\n",
            "Epoch [12/25], Step [220/607], Loss: 0.0230\n",
            "Training accuracy 100.0 %\n",
            "Time: 278.9726719856262\n",
            "Epoch [12/25], Step [225/607], Loss: 0.0533\n",
            "Training accuracy 100.0 %\n",
            "Time: 279.1933856010437\n",
            "Epoch [12/25], Step [230/607], Loss: 0.0521\n",
            "Training accuracy 100.0 %\n",
            "Time: 279.39424872398376\n",
            "Epoch [12/25], Step [235/607], Loss: 0.0672\n",
            "Training accuracy 93.75 %\n",
            "Time: 279.611848115921\n",
            "Epoch [12/25], Step [240/607], Loss: 0.0262\n",
            "Training accuracy 100.0 %\n",
            "Time: 279.8149456977844\n",
            "Epoch [12/25], Step [245/607], Loss: 0.2399\n",
            "Training accuracy 87.5 %\n",
            "Time: 280.0212023258209\n",
            "Epoch [12/25], Step [250/607], Loss: 0.1178\n",
            "Training accuracy 93.75 %\n",
            "Time: 280.25897455215454\n",
            "Epoch [12/25], Step [255/607], Loss: 0.0586\n",
            "Training accuracy 93.75 %\n",
            "Time: 280.4533746242523\n",
            "Epoch [12/25], Step [260/607], Loss: 0.0536\n",
            "Training accuracy 100.0 %\n",
            "Time: 280.6571307182312\n",
            "Epoch [12/25], Step [265/607], Loss: 0.0681\n",
            "Training accuracy 93.75 %\n",
            "Time: 280.86207723617554\n",
            "Epoch [12/25], Step [270/607], Loss: 0.0251\n",
            "Training accuracy 100.0 %\n",
            "Time: 281.05189085006714\n",
            "Epoch [12/25], Step [275/607], Loss: 0.0514\n",
            "Training accuracy 100.0 %\n",
            "Time: 281.2641131877899\n",
            "Epoch [12/25], Step [280/607], Loss: 0.0622\n",
            "Training accuracy 100.0 %\n",
            "Time: 281.46627593040466\n",
            "Epoch [12/25], Step [285/607], Loss: 0.1370\n",
            "Training accuracy 93.75 %\n",
            "Time: 281.64861249923706\n",
            "Epoch [12/25], Step [290/607], Loss: 0.1247\n",
            "Training accuracy 93.75 %\n",
            "Time: 281.8350148200989\n",
            "Epoch [12/25], Step [295/607], Loss: 0.0522\n",
            "Training accuracy 100.0 %\n",
            "Time: 282.0316092967987\n",
            "Epoch [12/25], Step [300/607], Loss: 0.0276\n",
            "Training accuracy 100.0 %\n",
            "Time: 282.22220396995544\n",
            "Epoch [12/25], Step [305/607], Loss: 0.1057\n",
            "Training accuracy 93.75 %\n",
            "Time: 282.4140360355377\n",
            "Epoch [12/25], Step [310/607], Loss: 0.1123\n",
            "Training accuracy 87.5 %\n",
            "Time: 282.61320447921753\n",
            "Epoch [12/25], Step [315/607], Loss: 0.0113\n",
            "Training accuracy 100.0 %\n",
            "Time: 282.812219619751\n",
            "Epoch [12/25], Step [320/607], Loss: 0.0223\n",
            "Training accuracy 100.0 %\n",
            "Time: 283.0003488063812\n",
            "Epoch [12/25], Step [325/607], Loss: 0.0295\n",
            "Training accuracy 100.0 %\n",
            "Time: 283.22204422950745\n",
            "Epoch [12/25], Step [330/607], Loss: 0.0141\n",
            "Training accuracy 100.0 %\n",
            "Time: 283.4195730686188\n",
            "Epoch [12/25], Step [335/607], Loss: 0.0241\n",
            "Training accuracy 100.0 %\n",
            "Time: 283.65789270401\n",
            "Epoch [12/25], Step [340/607], Loss: 0.0213\n",
            "Training accuracy 100.0 %\n",
            "Time: 283.8548974990845\n",
            "Epoch [12/25], Step [345/607], Loss: 0.1316\n",
            "Training accuracy 93.75 %\n",
            "Time: 284.08936405181885\n",
            "Epoch [12/25], Step [350/607], Loss: 0.0348\n",
            "Training accuracy 100.0 %\n",
            "Time: 284.29560136795044\n",
            "Epoch [12/25], Step [355/607], Loss: 0.1126\n",
            "Training accuracy 93.75 %\n",
            "Time: 284.4856412410736\n",
            "Epoch [12/25], Step [360/607], Loss: 0.0211\n",
            "Training accuracy 100.0 %\n",
            "Time: 284.69687008857727\n",
            "Epoch [12/25], Step [365/607], Loss: 0.1710\n",
            "Training accuracy 93.75 %\n",
            "Time: 284.9101481437683\n",
            "Epoch [12/25], Step [370/607], Loss: 0.0080\n",
            "Training accuracy 100.0 %\n",
            "Time: 285.1036522388458\n",
            "Epoch [12/25], Step [375/607], Loss: 0.0127\n",
            "Training accuracy 100.0 %\n",
            "Time: 285.31244254112244\n",
            "Epoch [12/25], Step [380/607], Loss: 0.0375\n",
            "Training accuracy 100.0 %\n",
            "Time: 285.51244258880615\n",
            "Epoch [12/25], Step [385/607], Loss: 0.0112\n",
            "Training accuracy 100.0 %\n",
            "Time: 285.70316195487976\n",
            "Epoch [12/25], Step [390/607], Loss: 0.1783\n",
            "Training accuracy 93.75 %\n",
            "Time: 285.901654958725\n",
            "Epoch [12/25], Step [395/607], Loss: 0.0278\n",
            "Training accuracy 100.0 %\n",
            "Time: 286.110032081604\n",
            "Epoch [12/25], Step [400/607], Loss: 0.0078\n",
            "Training accuracy 100.0 %\n",
            "Time: 286.31877040863037\n",
            "Epoch [12/25], Step [405/607], Loss: 0.3519\n",
            "Training accuracy 93.75 %\n",
            "Time: 286.51667761802673\n",
            "Epoch [12/25], Step [410/607], Loss: 0.0599\n",
            "Training accuracy 100.0 %\n",
            "Time: 286.7322266101837\n",
            "Epoch [12/25], Step [415/607], Loss: 0.0081\n",
            "Training accuracy 100.0 %\n",
            "Time: 286.91795587539673\n",
            "Epoch [12/25], Step [420/607], Loss: 0.0114\n",
            "Training accuracy 100.0 %\n",
            "Time: 287.1285173892975\n",
            "Epoch [12/25], Step [425/607], Loss: 0.0209\n",
            "Training accuracy 100.0 %\n",
            "Time: 287.3180968761444\n",
            "Epoch [12/25], Step [430/607], Loss: 0.0313\n",
            "Training accuracy 100.0 %\n",
            "Time: 287.5267312526703\n",
            "Epoch [12/25], Step [435/607], Loss: 0.0460\n",
            "Training accuracy 100.0 %\n",
            "Time: 287.70818543434143\n",
            "Epoch [12/25], Step [440/607], Loss: 0.0151\n",
            "Training accuracy 100.0 %\n",
            "Time: 287.88972091674805\n",
            "Epoch [12/25], Step [445/607], Loss: 0.0028\n",
            "Training accuracy 100.0 %\n",
            "Time: 288.0958571434021\n",
            "Epoch [12/25], Step [450/607], Loss: 0.0795\n",
            "Training accuracy 93.75 %\n",
            "Time: 288.3016631603241\n",
            "Epoch [12/25], Step [455/607], Loss: 0.0675\n",
            "Training accuracy 100.0 %\n",
            "Time: 288.5180633068085\n",
            "Epoch [12/25], Step [460/607], Loss: 0.1708\n",
            "Training accuracy 93.75 %\n",
            "Time: 288.7109603881836\n",
            "Epoch [12/25], Step [465/607], Loss: 0.0265\n",
            "Training accuracy 100.0 %\n",
            "Time: 288.91269755363464\n",
            "Epoch [12/25], Step [470/607], Loss: 0.0141\n",
            "Training accuracy 100.0 %\n",
            "Time: 289.1000852584839\n",
            "Epoch [12/25], Step [475/607], Loss: 0.0092\n",
            "Training accuracy 100.0 %\n",
            "Time: 289.32516407966614\n",
            "Epoch [12/25], Step [480/607], Loss: 0.0301\n",
            "Training accuracy 100.0 %\n",
            "Time: 289.54107904434204\n",
            "Epoch [12/25], Step [485/607], Loss: 0.0262\n",
            "Training accuracy 100.0 %\n",
            "Time: 289.7274568080902\n",
            "Epoch [12/25], Step [490/607], Loss: 0.0320\n",
            "Training accuracy 100.0 %\n",
            "Time: 289.963276386261\n",
            "Epoch [12/25], Step [495/607], Loss: 0.0431\n",
            "Training accuracy 100.0 %\n",
            "Time: 290.15437865257263\n",
            "Epoch [12/25], Step [500/607], Loss: 0.6209\n",
            "Training accuracy 81.25 %\n",
            "Time: 290.3994472026825\n",
            "Epoch [12/25], Step [505/607], Loss: 0.0419\n",
            "Training accuracy 100.0 %\n",
            "Time: 290.59273195266724\n",
            "Epoch [12/25], Step [510/607], Loss: 0.0440\n",
            "Training accuracy 100.0 %\n",
            "Time: 290.81064891815186\n",
            "Epoch [12/25], Step [515/607], Loss: 0.0294\n",
            "Training accuracy 100.0 %\n",
            "Time: 290.9925560951233\n",
            "Epoch [12/25], Step [520/607], Loss: 0.0446\n",
            "Training accuracy 100.0 %\n",
            "Time: 291.20230746269226\n",
            "Epoch [12/25], Step [525/607], Loss: 0.0119\n",
            "Training accuracy 100.0 %\n",
            "Time: 291.39175176620483\n",
            "Epoch [12/25], Step [530/607], Loss: 0.1050\n",
            "Training accuracy 93.75 %\n",
            "Time: 291.60342931747437\n",
            "Epoch [12/25], Step [535/607], Loss: 0.2090\n",
            "Training accuracy 93.75 %\n",
            "Time: 291.8013048171997\n",
            "Epoch [12/25], Step [540/607], Loss: 0.0525\n",
            "Training accuracy 100.0 %\n",
            "Time: 292.0297577381134\n",
            "Epoch [12/25], Step [545/607], Loss: 0.0983\n",
            "Training accuracy 100.0 %\n",
            "Time: 292.2244246006012\n",
            "Epoch [12/25], Step [550/607], Loss: 0.0323\n",
            "Training accuracy 100.0 %\n",
            "Time: 292.43992137908936\n",
            "Epoch [12/25], Step [555/607], Loss: 1.4693\n",
            "Training accuracy 93.75 %\n",
            "Time: 292.62563157081604\n",
            "Epoch [12/25], Step [560/607], Loss: 0.0170\n",
            "Training accuracy 100.0 %\n",
            "Time: 292.83517932891846\n",
            "Epoch [12/25], Step [565/607], Loss: 0.0697\n",
            "Training accuracy 100.0 %\n",
            "Time: 293.0272026062012\n",
            "Epoch [12/25], Step [570/607], Loss: 0.0196\n",
            "Training accuracy 100.0 %\n",
            "Time: 293.2434780597687\n",
            "Epoch [12/25], Step [575/607], Loss: 0.1570\n",
            "Training accuracy 87.5 %\n",
            "Time: 293.4473805427551\n",
            "Epoch [12/25], Step [580/607], Loss: 0.0477\n",
            "Training accuracy 100.0 %\n",
            "Time: 293.6584041118622\n",
            "Epoch [12/25], Step [585/607], Loss: 0.1166\n",
            "Training accuracy 87.5 %\n",
            "Time: 293.87577509880066\n",
            "Epoch [12/25], Step [590/607], Loss: 0.0142\n",
            "Training accuracy 100.0 %\n",
            "Time: 294.0666811466217\n",
            "Epoch [12/25], Step [595/607], Loss: 0.0566\n",
            "Training accuracy 100.0 %\n",
            "Time: 294.2436270713806\n",
            "Epoch [12/25], Step [600/607], Loss: 0.9830\n",
            "Training accuracy 87.5 %\n",
            "Time: 294.432373046875\n",
            "Epoch [12/25], Step [605/607], Loss: 0.0189\n",
            "Training accuracy 100.0 %\n",
            "Time: 294.6226267814636\n",
            "epoch 13\n",
            "Epoch [13/25], Step [5/607], Loss: 0.0233\n",
            "Training accuracy 100.0 %\n",
            "Time: 295.04059386253357\n",
            "Epoch [13/25], Step [10/607], Loss: 0.0392\n",
            "Training accuracy 100.0 %\n",
            "Time: 295.23550176620483\n",
            "Epoch [13/25], Step [15/607], Loss: 0.0277\n",
            "Training accuracy 100.0 %\n",
            "Time: 295.47003722190857\n",
            "Epoch [13/25], Step [20/607], Loss: 0.0256\n",
            "Training accuracy 100.0 %\n",
            "Time: 295.6506314277649\n",
            "Epoch [13/25], Step [25/607], Loss: 0.1133\n",
            "Training accuracy 93.75 %\n",
            "Time: 295.8587040901184\n",
            "Epoch [13/25], Step [30/607], Loss: 0.0272\n",
            "Training accuracy 100.0 %\n",
            "Time: 296.04688119888306\n",
            "Epoch [13/25], Step [35/607], Loss: 0.0127\n",
            "Training accuracy 100.0 %\n",
            "Time: 296.2440855503082\n",
            "Epoch [13/25], Step [40/607], Loss: 0.0188\n",
            "Training accuracy 100.0 %\n",
            "Time: 296.4426734447479\n",
            "Epoch [13/25], Step [45/607], Loss: 0.0572\n",
            "Training accuracy 100.0 %\n",
            "Time: 296.6661841869354\n",
            "Epoch [13/25], Step [50/607], Loss: 0.0072\n",
            "Training accuracy 100.0 %\n",
            "Time: 296.85708808898926\n",
            "Epoch [13/25], Step [55/607], Loss: 0.3301\n",
            "Training accuracy 93.75 %\n",
            "Time: 297.0647602081299\n",
            "Epoch [13/25], Step [60/607], Loss: 0.1253\n",
            "Training accuracy 93.75 %\n",
            "Time: 297.2547972202301\n",
            "Epoch [13/25], Step [65/607], Loss: 0.2084\n",
            "Training accuracy 93.75 %\n",
            "Time: 297.4755189418793\n",
            "Epoch [13/25], Step [70/607], Loss: 0.0582\n",
            "Training accuracy 100.0 %\n",
            "Time: 297.6630206108093\n",
            "Epoch [13/25], Step [75/607], Loss: 0.0307\n",
            "Training accuracy 100.0 %\n",
            "Time: 297.8880934715271\n",
            "Epoch [13/25], Step [80/607], Loss: 0.0174\n",
            "Training accuracy 100.0 %\n",
            "Time: 298.07775259017944\n",
            "Epoch [13/25], Step [85/607], Loss: 0.0629\n",
            "Training accuracy 100.0 %\n",
            "Time: 298.303786277771\n",
            "Epoch [13/25], Step [90/607], Loss: 0.0048\n",
            "Training accuracy 100.0 %\n",
            "Time: 298.50115942955017\n",
            "Epoch [13/25], Step [95/607], Loss: 0.0206\n",
            "Training accuracy 100.0 %\n",
            "Time: 298.7167983055115\n",
            "Epoch [13/25], Step [100/607], Loss: 0.0509\n",
            "Training accuracy 100.0 %\n",
            "Time: 298.89721846580505\n",
            "Epoch [13/25], Step [105/607], Loss: 0.0244\n",
            "Training accuracy 100.0 %\n",
            "Time: 299.12208580970764\n",
            "Epoch [13/25], Step [110/607], Loss: 0.0068\n",
            "Training accuracy 100.0 %\n",
            "Time: 299.29361605644226\n",
            "Epoch [13/25], Step [115/607], Loss: 0.0169\n",
            "Training accuracy 100.0 %\n",
            "Time: 299.5113036632538\n",
            "Epoch [13/25], Step [120/607], Loss: 0.1737\n",
            "Training accuracy 93.75 %\n",
            "Time: 299.7012765407562\n",
            "Epoch [13/25], Step [125/607], Loss: 0.2390\n",
            "Training accuracy 93.75 %\n",
            "Time: 299.9140074253082\n",
            "Epoch [13/25], Step [130/607], Loss: 0.0136\n",
            "Training accuracy 100.0 %\n",
            "Time: 300.118275642395\n",
            "Epoch [13/25], Step [135/607], Loss: 0.0616\n",
            "Training accuracy 100.0 %\n",
            "Time: 300.3321430683136\n",
            "Epoch [13/25], Step [140/607], Loss: 0.0403\n",
            "Training accuracy 100.0 %\n",
            "Time: 300.5571403503418\n",
            "Epoch [13/25], Step [145/607], Loss: 0.0103\n",
            "Training accuracy 100.0 %\n",
            "Time: 300.77033972740173\n",
            "Epoch [13/25], Step [150/607], Loss: 0.0575\n",
            "Training accuracy 93.75 %\n",
            "Time: 300.9814336299896\n",
            "Epoch [13/25], Step [155/607], Loss: 0.4557\n",
            "Training accuracy 93.75 %\n",
            "Time: 301.17662620544434\n",
            "Epoch [13/25], Step [160/607], Loss: 0.0467\n",
            "Training accuracy 100.0 %\n",
            "Time: 301.37919569015503\n",
            "Epoch [13/25], Step [165/607], Loss: 0.0162\n",
            "Training accuracy 100.0 %\n",
            "Time: 301.5995192527771\n",
            "Epoch [13/25], Step [170/607], Loss: 0.0408\n",
            "Training accuracy 100.0 %\n",
            "Time: 301.7867109775543\n",
            "Epoch [13/25], Step [175/607], Loss: 0.0126\n",
            "Training accuracy 100.0 %\n",
            "Time: 302.00644731521606\n",
            "Epoch [13/25], Step [180/607], Loss: 0.0213\n",
            "Training accuracy 100.0 %\n",
            "Time: 302.1829378604889\n",
            "Epoch [13/25], Step [185/607], Loss: 0.0075\n",
            "Training accuracy 100.0 %\n",
            "Time: 302.37234449386597\n",
            "Epoch [13/25], Step [190/607], Loss: 0.0407\n",
            "Training accuracy 100.0 %\n",
            "Time: 302.5679819583893\n",
            "Epoch [13/25], Step [195/607], Loss: 0.0377\n",
            "Training accuracy 100.0 %\n",
            "Time: 302.7743675708771\n",
            "Epoch [13/25], Step [200/607], Loss: 0.0310\n",
            "Training accuracy 100.0 %\n",
            "Time: 302.95461916923523\n",
            "Epoch [13/25], Step [205/607], Loss: 0.3378\n",
            "Training accuracy 87.5 %\n",
            "Time: 303.1784107685089\n",
            "Epoch [13/25], Step [210/607], Loss: 0.0074\n",
            "Training accuracy 100.0 %\n",
            "Time: 303.3640356063843\n",
            "Epoch [13/25], Step [215/607], Loss: 0.0370\n",
            "Training accuracy 100.0 %\n",
            "Time: 303.5859942436218\n",
            "Epoch [13/25], Step [220/607], Loss: 0.0030\n",
            "Training accuracy 100.0 %\n",
            "Time: 303.7710244655609\n",
            "Epoch [13/25], Step [225/607], Loss: 0.0126\n",
            "Training accuracy 100.0 %\n",
            "Time: 303.96149706840515\n",
            "Epoch [13/25], Step [230/607], Loss: 0.0401\n",
            "Training accuracy 100.0 %\n",
            "Time: 304.1537883281708\n",
            "Epoch [13/25], Step [235/607], Loss: 0.0073\n",
            "Training accuracy 100.0 %\n",
            "Time: 304.35553884506226\n",
            "Epoch [13/25], Step [240/607], Loss: 0.0454\n",
            "Training accuracy 100.0 %\n",
            "Time: 304.56051301956177\n",
            "Epoch [13/25], Step [245/607], Loss: 0.0160\n",
            "Training accuracy 100.0 %\n",
            "Time: 304.7620394229889\n",
            "Epoch [13/25], Step [250/607], Loss: 0.0102\n",
            "Training accuracy 100.0 %\n",
            "Time: 304.9992980957031\n",
            "Epoch [13/25], Step [255/607], Loss: 0.0104\n",
            "Training accuracy 100.0 %\n",
            "Time: 305.21051359176636\n",
            "Epoch [13/25], Step [260/607], Loss: 0.1296\n",
            "Training accuracy 93.75 %\n",
            "Time: 305.4280095100403\n",
            "Epoch [13/25], Step [265/607], Loss: 0.0279\n",
            "Training accuracy 100.0 %\n",
            "Time: 305.6525762081146\n",
            "Epoch [13/25], Step [270/607], Loss: 0.0286\n",
            "Training accuracy 100.0 %\n",
            "Time: 305.8725628852844\n",
            "Epoch [13/25], Step [275/607], Loss: 0.0336\n",
            "Training accuracy 100.0 %\n",
            "Time: 306.04864835739136\n",
            "Epoch [13/25], Step [280/607], Loss: 0.1225\n",
            "Training accuracy 93.75 %\n",
            "Time: 306.2230350971222\n",
            "Epoch [13/25], Step [285/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 306.4028699398041\n",
            "Epoch [13/25], Step [290/607], Loss: 0.0719\n",
            "Training accuracy 100.0 %\n",
            "Time: 306.6067445278168\n",
            "Epoch [13/25], Step [295/607], Loss: 0.9851\n",
            "Training accuracy 93.75 %\n",
            "Time: 306.7832407951355\n",
            "Epoch [13/25], Step [300/607], Loss: 0.0346\n",
            "Training accuracy 100.0 %\n",
            "Time: 306.9832980632782\n",
            "Epoch [13/25], Step [305/607], Loss: 0.2299\n",
            "Training accuracy 87.5 %\n",
            "Time: 307.1630358695984\n",
            "Epoch [13/25], Step [310/607], Loss: 0.0634\n",
            "Training accuracy 93.75 %\n",
            "Time: 307.3586666584015\n",
            "Epoch [13/25], Step [315/607], Loss: 0.0078\n",
            "Training accuracy 100.0 %\n",
            "Time: 307.5683798789978\n",
            "Epoch [13/25], Step [320/607], Loss: 0.0365\n",
            "Training accuracy 100.0 %\n",
            "Time: 307.76588106155396\n",
            "Epoch [13/25], Step [325/607], Loss: 0.0670\n",
            "Training accuracy 100.0 %\n",
            "Time: 307.98109555244446\n",
            "Epoch [13/25], Step [330/607], Loss: 0.0373\n",
            "Training accuracy 100.0 %\n",
            "Time: 308.1723823547363\n",
            "Epoch [13/25], Step [335/607], Loss: 0.0050\n",
            "Training accuracy 100.0 %\n",
            "Time: 308.3719081878662\n",
            "Epoch [13/25], Step [340/607], Loss: 0.0105\n",
            "Training accuracy 100.0 %\n",
            "Time: 308.5570373535156\n",
            "Epoch [13/25], Step [345/607], Loss: 0.0425\n",
            "Training accuracy 100.0 %\n",
            "Time: 308.7577738761902\n",
            "Epoch [13/25], Step [350/607], Loss: 0.0104\n",
            "Training accuracy 100.0 %\n",
            "Time: 308.95642733573914\n",
            "Epoch [13/25], Step [355/607], Loss: 0.0117\n",
            "Training accuracy 100.0 %\n",
            "Time: 309.17130637168884\n",
            "Epoch [13/25], Step [360/607], Loss: 0.1324\n",
            "Training accuracy 93.75 %\n",
            "Time: 309.3642499446869\n",
            "Epoch [13/25], Step [365/607], Loss: 0.0509\n",
            "Training accuracy 100.0 %\n",
            "Time: 309.56932401657104\n",
            "Epoch [13/25], Step [370/607], Loss: 0.3197\n",
            "Training accuracy 81.25 %\n",
            "Time: 309.75287532806396\n",
            "Epoch [13/25], Step [375/607], Loss: 0.0274\n",
            "Training accuracy 100.0 %\n",
            "Time: 309.96022176742554\n",
            "Epoch [13/25], Step [380/607], Loss: 0.1189\n",
            "Training accuracy 93.75 %\n",
            "Time: 310.17569184303284\n",
            "Epoch [13/25], Step [385/607], Loss: 0.0948\n",
            "Training accuracy 93.75 %\n",
            "Time: 310.38831853866577\n",
            "Epoch [13/25], Step [390/607], Loss: 0.0102\n",
            "Training accuracy 100.0 %\n",
            "Time: 310.5993151664734\n",
            "Epoch [13/25], Step [395/607], Loss: 0.0109\n",
            "Training accuracy 100.0 %\n",
            "Time: 310.8097677230835\n",
            "Epoch [13/25], Step [400/607], Loss: 0.0114\n",
            "Training accuracy 100.0 %\n",
            "Time: 311.02690386772156\n",
            "Epoch [13/25], Step [405/607], Loss: 0.0758\n",
            "Training accuracy 100.0 %\n",
            "Time: 311.2379238605499\n",
            "Epoch [13/25], Step [410/607], Loss: 0.0775\n",
            "Training accuracy 93.75 %\n",
            "Time: 311.4274032115936\n",
            "Epoch [13/25], Step [415/607], Loss: 0.1037\n",
            "Training accuracy 93.75 %\n",
            "Time: 311.63981461524963\n",
            "Epoch [13/25], Step [420/607], Loss: 0.0053\n",
            "Training accuracy 100.0 %\n",
            "Time: 311.84333848953247\n",
            "Epoch [13/25], Step [425/607], Loss: 0.0444\n",
            "Training accuracy 100.0 %\n",
            "Time: 312.0546324253082\n",
            "Epoch [13/25], Step [430/607], Loss: 0.2995\n",
            "Training accuracy 87.5 %\n",
            "Time: 312.2443730831146\n",
            "Epoch [13/25], Step [435/607], Loss: 0.0432\n",
            "Training accuracy 100.0 %\n",
            "Time: 312.4468457698822\n",
            "Epoch [13/25], Step [440/607], Loss: 0.0123\n",
            "Training accuracy 100.0 %\n",
            "Time: 312.65098428726196\n",
            "Epoch [13/25], Step [445/607], Loss: 0.0371\n",
            "Training accuracy 100.0 %\n",
            "Time: 312.85360741615295\n",
            "Epoch [13/25], Step [450/607], Loss: 0.0297\n",
            "Training accuracy 100.0 %\n",
            "Time: 313.0443067550659\n",
            "Epoch [13/25], Step [455/607], Loss: 0.5287\n",
            "Training accuracy 87.5 %\n",
            "Time: 313.24375438690186\n",
            "Epoch [13/25], Step [460/607], Loss: 0.0196\n",
            "Training accuracy 100.0 %\n",
            "Time: 313.46826577186584\n",
            "Epoch [13/25], Step [465/607], Loss: 0.0851\n",
            "Training accuracy 93.75 %\n",
            "Time: 313.6658570766449\n",
            "Epoch [13/25], Step [470/607], Loss: 0.0164\n",
            "Training accuracy 100.0 %\n",
            "Time: 313.8825740814209\n",
            "Epoch [13/25], Step [475/607], Loss: 0.0117\n",
            "Training accuracy 100.0 %\n",
            "Time: 314.07365894317627\n",
            "Epoch [13/25], Step [480/607], Loss: 0.1414\n",
            "Training accuracy 87.5 %\n",
            "Time: 314.2847168445587\n",
            "Epoch [13/25], Step [485/607], Loss: 0.0103\n",
            "Training accuracy 100.0 %\n",
            "Time: 314.47056794166565\n",
            "Epoch [13/25], Step [490/607], Loss: 0.1745\n",
            "Training accuracy 93.75 %\n",
            "Time: 314.67831349372864\n",
            "Epoch [13/25], Step [495/607], Loss: 0.0165\n",
            "Training accuracy 100.0 %\n",
            "Time: 314.8561301231384\n",
            "Epoch [13/25], Step [500/607], Loss: 0.0219\n",
            "Training accuracy 100.0 %\n",
            "Time: 315.0804889202118\n",
            "Epoch [13/25], Step [505/607], Loss: 0.0314\n",
            "Training accuracy 100.0 %\n",
            "Time: 315.3003044128418\n",
            "Epoch [13/25], Step [510/607], Loss: 0.0249\n",
            "Training accuracy 100.0 %\n",
            "Time: 315.51441168785095\n",
            "Epoch [13/25], Step [515/607], Loss: 0.0493\n",
            "Training accuracy 100.0 %\n",
            "Time: 315.7262897491455\n",
            "Epoch [13/25], Step [520/607], Loss: 0.0272\n",
            "Training accuracy 100.0 %\n",
            "Time: 315.93550419807434\n",
            "Epoch [13/25], Step [525/607], Loss: 0.0483\n",
            "Training accuracy 100.0 %\n",
            "Time: 316.1605501174927\n",
            "Epoch [13/25], Step [530/607], Loss: 0.0341\n",
            "Training accuracy 100.0 %\n",
            "Time: 316.35527324676514\n",
            "Epoch [13/25], Step [535/607], Loss: 0.0186\n",
            "Training accuracy 100.0 %\n",
            "Time: 316.5861859321594\n",
            "Epoch [13/25], Step [540/607], Loss: 0.0382\n",
            "Training accuracy 100.0 %\n",
            "Time: 316.7846338748932\n",
            "Epoch [13/25], Step [545/607], Loss: 0.0625\n",
            "Training accuracy 93.75 %\n",
            "Time: 317.00719690322876\n",
            "Epoch [13/25], Step [550/607], Loss: 0.0129\n",
            "Training accuracy 100.0 %\n",
            "Time: 317.1974506378174\n",
            "Epoch [13/25], Step [555/607], Loss: 0.0062\n",
            "Training accuracy 100.0 %\n",
            "Time: 317.4225995540619\n",
            "Epoch [13/25], Step [560/607], Loss: 0.0649\n",
            "Training accuracy 100.0 %\n",
            "Time: 317.610107421875\n",
            "Epoch [13/25], Step [565/607], Loss: 0.3018\n",
            "Training accuracy 93.75 %\n",
            "Time: 317.83581829071045\n",
            "Epoch [13/25], Step [570/607], Loss: 0.0461\n",
            "Training accuracy 100.0 %\n",
            "Time: 318.02107334136963\n",
            "Epoch [13/25], Step [575/607], Loss: 0.0465\n",
            "Training accuracy 100.0 %\n",
            "Time: 318.26404762268066\n",
            "Epoch [13/25], Step [580/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 318.4570391178131\n",
            "Epoch [13/25], Step [585/607], Loss: 0.0086\n",
            "Training accuracy 100.0 %\n",
            "Time: 318.6657540798187\n",
            "Epoch [13/25], Step [590/607], Loss: 0.0439\n",
            "Training accuracy 100.0 %\n",
            "Time: 318.8424232006073\n",
            "Epoch [13/25], Step [595/607], Loss: 0.0034\n",
            "Training accuracy 100.0 %\n",
            "Time: 319.0229687690735\n",
            "Epoch [13/25], Step [600/607], Loss: 0.1752\n",
            "Training accuracy 93.75 %\n",
            "Time: 319.20440220832825\n",
            "Epoch [13/25], Step [605/607], Loss: 0.0249\n",
            "Training accuracy 100.0 %\n",
            "Time: 319.3914544582367\n",
            "epoch 14\n",
            "Epoch [14/25], Step [5/607], Loss: 1.3381\n",
            "Training accuracy 93.75 %\n",
            "Time: 319.817378282547\n",
            "Epoch [14/25], Step [10/607], Loss: 0.0072\n",
            "Training accuracy 100.0 %\n",
            "Time: 320.02054619789124\n",
            "Epoch [14/25], Step [15/607], Loss: 0.0177\n",
            "Training accuracy 100.0 %\n",
            "Time: 320.23258662223816\n",
            "Epoch [14/25], Step [20/607], Loss: 0.0030\n",
            "Training accuracy 100.0 %\n",
            "Time: 320.4418888092041\n",
            "Epoch [14/25], Step [25/607], Loss: 0.0330\n",
            "Training accuracy 100.0 %\n",
            "Time: 320.66531920433044\n",
            "Epoch [14/25], Step [30/607], Loss: 0.0181\n",
            "Training accuracy 100.0 %\n",
            "Time: 320.88913917541504\n",
            "Epoch [14/25], Step [35/607], Loss: 0.0228\n",
            "Training accuracy 100.0 %\n",
            "Time: 321.1010353565216\n",
            "Epoch [14/25], Step [40/607], Loss: 0.0296\n",
            "Training accuracy 100.0 %\n",
            "Time: 321.2843942642212\n",
            "Epoch [14/25], Step [45/607], Loss: 0.0314\n",
            "Training accuracy 100.0 %\n",
            "Time: 321.48559975624084\n",
            "Epoch [14/25], Step [50/607], Loss: 0.0722\n",
            "Training accuracy 93.75 %\n",
            "Time: 321.6892340183258\n",
            "Epoch [14/25], Step [55/607], Loss: 0.0441\n",
            "Training accuracy 100.0 %\n",
            "Time: 321.9111371040344\n",
            "Epoch [14/25], Step [60/607], Loss: 0.1269\n",
            "Training accuracy 93.75 %\n",
            "Time: 322.1300892829895\n",
            "Epoch [14/25], Step [65/607], Loss: 0.0402\n",
            "Training accuracy 100.0 %\n",
            "Time: 322.3539307117462\n",
            "Epoch [14/25], Step [70/607], Loss: 0.6970\n",
            "Training accuracy 93.75 %\n",
            "Time: 322.5502681732178\n",
            "Epoch [14/25], Step [75/607], Loss: 0.0555\n",
            "Training accuracy 100.0 %\n",
            "Time: 322.7728650569916\n",
            "Epoch [14/25], Step [80/607], Loss: 0.0340\n",
            "Training accuracy 100.0 %\n",
            "Time: 322.96586441993713\n",
            "Epoch [14/25], Step [85/607], Loss: 0.6652\n",
            "Training accuracy 93.75 %\n",
            "Time: 323.1979100704193\n",
            "Epoch [14/25], Step [90/607], Loss: 0.0251\n",
            "Training accuracy 100.0 %\n",
            "Time: 323.38853931427\n",
            "Epoch [14/25], Step [95/607], Loss: 0.0077\n",
            "Training accuracy 100.0 %\n",
            "Time: 323.5975339412689\n",
            "Epoch [14/25], Step [100/607], Loss: 0.0425\n",
            "Training accuracy 100.0 %\n",
            "Time: 323.7880072593689\n",
            "Epoch [14/25], Step [105/607], Loss: 0.0413\n",
            "Training accuracy 100.0 %\n",
            "Time: 324.00589418411255\n",
            "Epoch [14/25], Step [110/607], Loss: 0.0936\n",
            "Training accuracy 100.0 %\n",
            "Time: 324.19562673568726\n",
            "Epoch [14/25], Step [115/607], Loss: 0.0216\n",
            "Training accuracy 100.0 %\n",
            "Time: 324.4154279232025\n",
            "Epoch [14/25], Step [120/607], Loss: 0.0425\n",
            "Training accuracy 100.0 %\n",
            "Time: 324.604261636734\n",
            "Epoch [14/25], Step [125/607], Loss: 0.2212\n",
            "Training accuracy 87.5 %\n",
            "Time: 324.8036286830902\n",
            "Epoch [14/25], Step [130/607], Loss: 0.0433\n",
            "Training accuracy 100.0 %\n",
            "Time: 324.98784494400024\n",
            "Epoch [14/25], Step [135/607], Loss: 0.0418\n",
            "Training accuracy 100.0 %\n",
            "Time: 325.20854926109314\n",
            "Epoch [14/25], Step [140/607], Loss: 0.1370\n",
            "Training accuracy 93.75 %\n",
            "Time: 325.39902663230896\n",
            "Epoch [14/25], Step [145/607], Loss: 0.0380\n",
            "Training accuracy 100.0 %\n",
            "Time: 325.61810851097107\n",
            "Epoch [14/25], Step [150/607], Loss: 0.0064\n",
            "Training accuracy 100.0 %\n",
            "Time: 325.81095147132874\n",
            "Epoch [14/25], Step [155/607], Loss: 0.0141\n",
            "Training accuracy 100.0 %\n",
            "Time: 326.03109669685364\n",
            "Epoch [14/25], Step [160/607], Loss: 0.1225\n",
            "Training accuracy 93.75 %\n",
            "Time: 326.22815108299255\n",
            "Epoch [14/25], Step [165/607], Loss: 0.0086\n",
            "Training accuracy 100.0 %\n",
            "Time: 326.4207065105438\n",
            "Epoch [14/25], Step [170/607], Loss: 0.0078\n",
            "Training accuracy 100.0 %\n",
            "Time: 326.62818670272827\n",
            "Epoch [14/25], Step [175/607], Loss: 0.0428\n",
            "Training accuracy 100.0 %\n",
            "Time: 326.83135175704956\n",
            "Epoch [14/25], Step [180/607], Loss: 0.0341\n",
            "Training accuracy 100.0 %\n",
            "Time: 327.0189347267151\n",
            "Epoch [14/25], Step [185/607], Loss: 0.0063\n",
            "Training accuracy 100.0 %\n",
            "Time: 327.1977062225342\n",
            "Epoch [14/25], Step [190/607], Loss: 0.0094\n",
            "Training accuracy 100.0 %\n",
            "Time: 327.3933115005493\n",
            "Epoch [14/25], Step [195/607], Loss: 0.0305\n",
            "Training accuracy 100.0 %\n",
            "Time: 327.5914833545685\n",
            "Epoch [14/25], Step [200/607], Loss: 0.0170\n",
            "Training accuracy 100.0 %\n",
            "Time: 327.79918098449707\n",
            "Epoch [14/25], Step [205/607], Loss: 0.0148\n",
            "Training accuracy 100.0 %\n",
            "Time: 327.99176573753357\n",
            "Epoch [14/25], Step [210/607], Loss: 0.0157\n",
            "Training accuracy 100.0 %\n",
            "Time: 328.17068362236023\n",
            "Epoch [14/25], Step [215/607], Loss: 0.0443\n",
            "Training accuracy 100.0 %\n",
            "Time: 328.37693905830383\n",
            "Epoch [14/25], Step [220/607], Loss: 0.0193\n",
            "Training accuracy 100.0 %\n",
            "Time: 328.5773985385895\n",
            "Epoch [14/25], Step [225/607], Loss: 0.0413\n",
            "Training accuracy 100.0 %\n",
            "Time: 328.7736213207245\n",
            "Epoch [14/25], Step [230/607], Loss: 0.0164\n",
            "Training accuracy 100.0 %\n",
            "Time: 328.96352553367615\n",
            "Epoch [14/25], Step [235/607], Loss: 0.1198\n",
            "Training accuracy 87.5 %\n",
            "Time: 329.16548705101013\n",
            "Epoch [14/25], Step [240/607], Loss: 0.0126\n",
            "Training accuracy 100.0 %\n",
            "Time: 329.35435032844543\n",
            "Epoch [14/25], Step [245/607], Loss: 0.0217\n",
            "Training accuracy 100.0 %\n",
            "Time: 329.54181027412415\n",
            "Epoch [14/25], Step [250/607], Loss: 0.0951\n",
            "Training accuracy 93.75 %\n",
            "Time: 329.7211401462555\n",
            "Epoch [14/25], Step [255/607], Loss: 0.0255\n",
            "Training accuracy 100.0 %\n",
            "Time: 329.9290587902069\n",
            "Epoch [14/25], Step [260/607], Loss: 0.0719\n",
            "Training accuracy 100.0 %\n",
            "Time: 330.11027216911316\n",
            "Epoch [14/25], Step [265/607], Loss: 0.0155\n",
            "Training accuracy 100.0 %\n",
            "Time: 330.32758116722107\n",
            "Epoch [14/25], Step [270/607], Loss: 0.0208\n",
            "Training accuracy 100.0 %\n",
            "Time: 330.5310289859772\n",
            "Epoch [14/25], Step [275/607], Loss: 0.0227\n",
            "Training accuracy 100.0 %\n",
            "Time: 330.72711634635925\n",
            "Epoch [14/25], Step [280/607], Loss: 0.1151\n",
            "Training accuracy 87.5 %\n",
            "Time: 330.9298393726349\n",
            "Epoch [14/25], Step [285/607], Loss: 0.1694\n",
            "Training accuracy 87.5 %\n",
            "Time: 331.1791763305664\n",
            "Epoch [14/25], Step [290/607], Loss: 0.0120\n",
            "Training accuracy 100.0 %\n",
            "Time: 331.35878348350525\n",
            "Epoch [14/25], Step [295/607], Loss: 0.0177\n",
            "Training accuracy 100.0 %\n",
            "Time: 331.54021072387695\n",
            "Epoch [14/25], Step [300/607], Loss: 0.3130\n",
            "Training accuracy 87.5 %\n",
            "Time: 331.7210443019867\n",
            "Epoch [14/25], Step [305/607], Loss: 0.0147\n",
            "Training accuracy 100.0 %\n",
            "Time: 331.9276490211487\n",
            "Epoch [14/25], Step [310/607], Loss: 0.0147\n",
            "Training accuracy 100.0 %\n",
            "Time: 332.1164667606354\n",
            "Epoch [14/25], Step [315/607], Loss: 0.0228\n",
            "Training accuracy 100.0 %\n",
            "Time: 332.3270688056946\n",
            "Epoch [14/25], Step [320/607], Loss: 0.0090\n",
            "Training accuracy 100.0 %\n",
            "Time: 332.5150136947632\n",
            "Epoch [14/25], Step [325/607], Loss: 0.0077\n",
            "Training accuracy 100.0 %\n",
            "Time: 332.71196460723877\n",
            "Epoch [14/25], Step [330/607], Loss: 0.1384\n",
            "Training accuracy 93.75 %\n",
            "Time: 332.9044666290283\n",
            "Epoch [14/25], Step [335/607], Loss: 0.0480\n",
            "Training accuracy 100.0 %\n",
            "Time: 333.11924839019775\n",
            "Epoch [14/25], Step [340/607], Loss: 0.0017\n",
            "Training accuracy 100.0 %\n",
            "Time: 333.29855608940125\n",
            "Epoch [14/25], Step [345/607], Loss: 0.0473\n",
            "Training accuracy 100.0 %\n",
            "Time: 333.5004322528839\n",
            "Epoch [14/25], Step [350/607], Loss: 0.0023\n",
            "Training accuracy 100.0 %\n",
            "Time: 333.6923871040344\n",
            "Epoch [14/25], Step [355/607], Loss: 0.0239\n",
            "Training accuracy 100.0 %\n",
            "Time: 333.9115526676178\n",
            "Epoch [14/25], Step [360/607], Loss: 0.0069\n",
            "Training accuracy 100.0 %\n",
            "Time: 334.09087681770325\n",
            "Epoch [14/25], Step [365/607], Loss: 0.0120\n",
            "Training accuracy 100.0 %\n",
            "Time: 334.28477025032043\n",
            "Epoch [14/25], Step [370/607], Loss: 0.0371\n",
            "Training accuracy 100.0 %\n",
            "Time: 334.472421169281\n",
            "Epoch [14/25], Step [375/607], Loss: 0.0140\n",
            "Training accuracy 100.0 %\n",
            "Time: 334.665002822876\n",
            "Epoch [14/25], Step [380/607], Loss: 0.1725\n",
            "Training accuracy 93.75 %\n",
            "Time: 334.85901498794556\n",
            "Epoch [14/25], Step [385/607], Loss: 0.0777\n",
            "Training accuracy 100.0 %\n",
            "Time: 335.05275678634644\n",
            "Epoch [14/25], Step [390/607], Loss: 0.6292\n",
            "Training accuracy 93.75 %\n",
            "Time: 335.2367248535156\n",
            "Epoch [14/25], Step [395/607], Loss: 0.0070\n",
            "Training accuracy 100.0 %\n",
            "Time: 335.44963479042053\n",
            "Epoch [14/25], Step [400/607], Loss: 0.0085\n",
            "Training accuracy 100.0 %\n",
            "Time: 335.63827180862427\n",
            "Epoch [14/25], Step [405/607], Loss: 0.2025\n",
            "Training accuracy 93.75 %\n",
            "Time: 335.8673903942108\n",
            "Epoch [14/25], Step [410/607], Loss: 0.0390\n",
            "Training accuracy 100.0 %\n",
            "Time: 336.1302056312561\n",
            "Epoch [14/25], Step [415/607], Loss: 0.0081\n",
            "Training accuracy 100.0 %\n",
            "Time: 336.3452742099762\n",
            "Epoch [14/25], Step [420/607], Loss: 0.0139\n",
            "Training accuracy 100.0 %\n",
            "Time: 336.5510971546173\n",
            "Epoch [14/25], Step [425/607], Loss: 0.0090\n",
            "Training accuracy 100.0 %\n",
            "Time: 336.75269865989685\n",
            "Epoch [14/25], Step [430/607], Loss: 0.0107\n",
            "Training accuracy 100.0 %\n",
            "Time: 336.9921269416809\n",
            "Epoch [14/25], Step [435/607], Loss: 0.0839\n",
            "Training accuracy 93.75 %\n",
            "Time: 337.1933572292328\n",
            "Epoch [14/25], Step [440/607], Loss: 0.0048\n",
            "Training accuracy 100.0 %\n",
            "Time: 337.4218952655792\n",
            "Epoch [14/25], Step [445/607], Loss: 0.0457\n",
            "Training accuracy 100.0 %\n",
            "Time: 337.6047263145447\n",
            "Epoch [14/25], Step [450/607], Loss: 0.1344\n",
            "Training accuracy 93.75 %\n",
            "Time: 337.8148846626282\n",
            "Epoch [14/25], Step [455/607], Loss: 0.0590\n",
            "Training accuracy 93.75 %\n",
            "Time: 338.00076937675476\n",
            "Epoch [14/25], Step [460/607], Loss: 0.1729\n",
            "Training accuracy 93.75 %\n",
            "Time: 338.1974515914917\n",
            "Epoch [14/25], Step [465/607], Loss: 0.0115\n",
            "Training accuracy 100.0 %\n",
            "Time: 338.389684677124\n",
            "Epoch [14/25], Step [470/607], Loss: 0.1313\n",
            "Training accuracy 93.75 %\n",
            "Time: 338.58991289138794\n",
            "Epoch [14/25], Step [475/607], Loss: 0.0094\n",
            "Training accuracy 100.0 %\n",
            "Time: 338.78142762184143\n",
            "Epoch [14/25], Step [480/607], Loss: 0.2152\n",
            "Training accuracy 87.5 %\n",
            "Time: 339.0130388736725\n",
            "Epoch [14/25], Step [485/607], Loss: 0.0290\n",
            "Training accuracy 100.0 %\n",
            "Time: 339.21555161476135\n",
            "Epoch [14/25], Step [490/607], Loss: 0.0960\n",
            "Training accuracy 100.0 %\n",
            "Time: 339.411500453949\n",
            "Epoch [14/25], Step [495/607], Loss: 0.0109\n",
            "Training accuracy 100.0 %\n",
            "Time: 339.6155836582184\n",
            "Epoch [14/25], Step [500/607], Loss: 0.0196\n",
            "Training accuracy 100.0 %\n",
            "Time: 339.81404519081116\n",
            "Epoch [14/25], Step [505/607], Loss: 0.0461\n",
            "Training accuracy 100.0 %\n",
            "Time: 340.0199685096741\n",
            "Epoch [14/25], Step [510/607], Loss: 0.0348\n",
            "Training accuracy 100.0 %\n",
            "Time: 340.23052287101746\n",
            "Epoch [14/25], Step [515/607], Loss: 0.0057\n",
            "Training accuracy 100.0 %\n",
            "Time: 340.4154829978943\n",
            "Epoch [14/25], Step [520/607], Loss: 0.0308\n",
            "Training accuracy 100.0 %\n",
            "Time: 340.64844822883606\n",
            "Epoch [14/25], Step [525/607], Loss: 0.0501\n",
            "Training accuracy 93.75 %\n",
            "Time: 340.84076046943665\n",
            "Epoch [14/25], Step [530/607], Loss: 0.0027\n",
            "Training accuracy 100.0 %\n",
            "Time: 341.0785961151123\n",
            "Epoch [14/25], Step [535/607], Loss: 0.0296\n",
            "Training accuracy 100.0 %\n",
            "Time: 341.2910258769989\n",
            "Epoch [14/25], Step [540/607], Loss: 0.0144\n",
            "Training accuracy 100.0 %\n",
            "Time: 341.5036952495575\n",
            "Epoch [14/25], Step [545/607], Loss: 0.0086\n",
            "Training accuracy 100.0 %\n",
            "Time: 341.70467591285706\n",
            "Epoch [14/25], Step [550/607], Loss: 0.0081\n",
            "Training accuracy 100.0 %\n",
            "Time: 341.91902351379395\n",
            "Epoch [14/25], Step [555/607], Loss: 0.0181\n",
            "Training accuracy 100.0 %\n",
            "Time: 342.1108021736145\n",
            "Epoch [14/25], Step [560/607], Loss: 0.0079\n",
            "Training accuracy 100.0 %\n",
            "Time: 342.28502798080444\n",
            "Epoch [14/25], Step [565/607], Loss: 0.0138\n",
            "Training accuracy 100.0 %\n",
            "Time: 342.45859241485596\n",
            "Epoch [14/25], Step [570/607], Loss: 0.2149\n",
            "Training accuracy 87.5 %\n",
            "Time: 342.65421319007874\n",
            "Epoch [14/25], Step [575/607], Loss: 0.0132\n",
            "Training accuracy 100.0 %\n",
            "Time: 342.8497505187988\n",
            "Epoch [14/25], Step [580/607], Loss: 0.0039\n",
            "Training accuracy 100.0 %\n",
            "Time: 343.06346321105957\n",
            "Epoch [14/25], Step [585/607], Loss: 0.2127\n",
            "Training accuracy 87.5 %\n",
            "Time: 343.2560646533966\n",
            "Epoch [14/25], Step [590/607], Loss: 0.0393\n",
            "Training accuracy 100.0 %\n",
            "Time: 343.4698078632355\n",
            "Epoch [14/25], Step [595/607], Loss: 0.0521\n",
            "Training accuracy 100.0 %\n",
            "Time: 343.6562180519104\n",
            "Epoch [14/25], Step [600/607], Loss: 0.0277\n",
            "Training accuracy 100.0 %\n",
            "Time: 343.86473512649536\n",
            "Epoch [14/25], Step [605/607], Loss: 0.0040\n",
            "Training accuracy 100.0 %\n",
            "Time: 344.05610370635986\n",
            "epoch 15\n",
            "Epoch [15/25], Step [5/607], Loss: 0.0185\n",
            "Training accuracy 100.0 %\n",
            "Time: 344.48728251457214\n",
            "Epoch [15/25], Step [10/607], Loss: 0.0170\n",
            "Training accuracy 100.0 %\n",
            "Time: 344.6793348789215\n",
            "Epoch [15/25], Step [15/607], Loss: 0.0243\n",
            "Training accuracy 100.0 %\n",
            "Time: 344.90352964401245\n",
            "Epoch [15/25], Step [20/607], Loss: 0.0002\n",
            "Training accuracy 100.0 %\n",
            "Time: 345.0949602127075\n",
            "Epoch [15/25], Step [25/607], Loss: 0.0160\n",
            "Training accuracy 100.0 %\n",
            "Time: 345.31476044654846\n",
            "Epoch [15/25], Step [30/607], Loss: 0.0130\n",
            "Training accuracy 100.0 %\n",
            "Time: 345.5049481391907\n",
            "Epoch [15/25], Step [35/607], Loss: 0.0284\n",
            "Training accuracy 100.0 %\n",
            "Time: 345.72913241386414\n",
            "Epoch [15/25], Step [40/607], Loss: 0.0194\n",
            "Training accuracy 100.0 %\n",
            "Time: 345.90567779541016\n",
            "Epoch [15/25], Step [45/607], Loss: 0.0055\n",
            "Training accuracy 100.0 %\n",
            "Time: 346.1396040916443\n",
            "Epoch [15/25], Step [50/607], Loss: 0.0111\n",
            "Training accuracy 100.0 %\n",
            "Time: 346.36269664764404\n",
            "Epoch [15/25], Step [55/607], Loss: 0.0691\n",
            "Training accuracy 100.0 %\n",
            "Time: 346.57371187210083\n",
            "Epoch [15/25], Step [60/607], Loss: 0.0039\n",
            "Training accuracy 100.0 %\n",
            "Time: 346.7853741645813\n",
            "Epoch [15/25], Step [65/607], Loss: 0.0192\n",
            "Training accuracy 100.0 %\n",
            "Time: 346.9943599700928\n",
            "Epoch [15/25], Step [70/607], Loss: 0.0441\n",
            "Training accuracy 100.0 %\n",
            "Time: 347.2069139480591\n",
            "Epoch [15/25], Step [75/607], Loss: 0.0176\n",
            "Training accuracy 100.0 %\n",
            "Time: 347.4176378250122\n",
            "Epoch [15/25], Step [80/607], Loss: 0.2032\n",
            "Training accuracy 87.5 %\n",
            "Time: 347.6110849380493\n",
            "Epoch [15/25], Step [85/607], Loss: 0.0068\n",
            "Training accuracy 100.0 %\n",
            "Time: 347.8023886680603\n",
            "Epoch [15/25], Step [90/607], Loss: 0.0347\n",
            "Training accuracy 100.0 %\n",
            "Time: 347.97825288772583\n",
            "Epoch [15/25], Step [95/607], Loss: 0.0066\n",
            "Training accuracy 100.0 %\n",
            "Time: 348.18434715270996\n",
            "Epoch [15/25], Step [100/607], Loss: 0.0178\n",
            "Training accuracy 100.0 %\n",
            "Time: 348.3935339450836\n",
            "Epoch [15/25], Step [105/607], Loss: 0.1201\n",
            "Training accuracy 93.75 %\n",
            "Time: 348.6137363910675\n",
            "Epoch [15/25], Step [110/607], Loss: 0.0036\n",
            "Training accuracy 100.0 %\n",
            "Time: 348.82510447502136\n",
            "Epoch [15/25], Step [115/607], Loss: 0.0134\n",
            "Training accuracy 100.0 %\n",
            "Time: 349.03322100639343\n",
            "Epoch [15/25], Step [120/607], Loss: 0.0770\n",
            "Training accuracy 100.0 %\n",
            "Time: 349.22110652923584\n",
            "Epoch [15/25], Step [125/607], Loss: 0.0382\n",
            "Training accuracy 100.0 %\n",
            "Time: 349.4275071620941\n",
            "Epoch [15/25], Step [130/607], Loss: 0.8439\n",
            "Training accuracy 93.75 %\n",
            "Time: 349.61500000953674\n",
            "Epoch [15/25], Step [135/607], Loss: 0.0202\n",
            "Training accuracy 100.0 %\n",
            "Time: 349.81848526000977\n",
            "Epoch [15/25], Step [140/607], Loss: 0.0616\n",
            "Training accuracy 93.75 %\n",
            "Time: 350.02375960350037\n",
            "Epoch [15/25], Step [145/607], Loss: 0.0098\n",
            "Training accuracy 100.0 %\n",
            "Time: 350.2426109313965\n",
            "Epoch [15/25], Step [150/607], Loss: 0.1286\n",
            "Training accuracy 93.75 %\n",
            "Time: 350.43972277641296\n",
            "Epoch [15/25], Step [155/607], Loss: 0.0299\n",
            "Training accuracy 100.0 %\n",
            "Time: 350.64182138442993\n",
            "Epoch [15/25], Step [160/607], Loss: 0.0055\n",
            "Training accuracy 100.0 %\n",
            "Time: 350.81497740745544\n",
            "Epoch [15/25], Step [165/607], Loss: 0.0197\n",
            "Training accuracy 100.0 %\n",
            "Time: 351.04031586647034\n",
            "Epoch [15/25], Step [170/607], Loss: 0.1690\n",
            "Training accuracy 93.75 %\n",
            "Time: 351.21070861816406\n",
            "Epoch [15/25], Step [175/607], Loss: 0.0256\n",
            "Training accuracy 100.0 %\n",
            "Time: 351.44162106513977\n",
            "Epoch [15/25], Step [180/607], Loss: 0.0477\n",
            "Training accuracy 100.0 %\n",
            "Time: 351.63077759742737\n",
            "Epoch [15/25], Step [185/607], Loss: 0.0029\n",
            "Training accuracy 100.0 %\n",
            "Time: 351.8297610282898\n",
            "Epoch [15/25], Step [190/607], Loss: 0.0067\n",
            "Training accuracy 100.0 %\n",
            "Time: 352.02275681495667\n",
            "Epoch [15/25], Step [195/607], Loss: 0.0287\n",
            "Training accuracy 100.0 %\n",
            "Time: 352.2366352081299\n",
            "Epoch [15/25], Step [200/607], Loss: 0.0127\n",
            "Training accuracy 100.0 %\n",
            "Time: 352.4420053958893\n",
            "Epoch [15/25], Step [205/607], Loss: 0.0073\n",
            "Training accuracy 100.0 %\n",
            "Time: 352.61685943603516\n",
            "Epoch [15/25], Step [210/607], Loss: 0.0092\n",
            "Training accuracy 100.0 %\n",
            "Time: 352.81311321258545\n",
            "Epoch [15/25], Step [215/607], Loss: 0.0304\n",
            "Training accuracy 100.0 %\n",
            "Time: 353.0265061855316\n",
            "Epoch [15/25], Step [220/607], Loss: 0.0221\n",
            "Training accuracy 100.0 %\n",
            "Time: 353.20337319374084\n",
            "Epoch [15/25], Step [225/607], Loss: 0.0253\n",
            "Training accuracy 100.0 %\n",
            "Time: 353.39527344703674\n",
            "Epoch [15/25], Step [230/607], Loss: 0.0355\n",
            "Training accuracy 100.0 %\n",
            "Time: 353.594922542572\n",
            "Epoch [15/25], Step [235/607], Loss: 0.0077\n",
            "Training accuracy 100.0 %\n",
            "Time: 353.8013925552368\n",
            "Epoch [15/25], Step [240/607], Loss: 0.0123\n",
            "Training accuracy 100.0 %\n",
            "Time: 353.99417901039124\n",
            "Epoch [15/25], Step [245/607], Loss: 0.0315\n",
            "Training accuracy 100.0 %\n",
            "Time: 354.2035827636719\n",
            "Epoch [15/25], Step [250/607], Loss: 0.1914\n",
            "Training accuracy 93.75 %\n",
            "Time: 354.3963553905487\n",
            "Epoch [15/25], Step [255/607], Loss: 0.1309\n",
            "Training accuracy 87.5 %\n",
            "Time: 354.59532856941223\n",
            "Epoch [15/25], Step [260/607], Loss: 0.0333\n",
            "Training accuracy 100.0 %\n",
            "Time: 354.7807538509369\n",
            "Epoch [15/25], Step [265/607], Loss: 0.0227\n",
            "Training accuracy 100.0 %\n",
            "Time: 354.9922935962677\n",
            "Epoch [15/25], Step [270/607], Loss: 0.0123\n",
            "Training accuracy 100.0 %\n",
            "Time: 355.1834092140198\n",
            "Epoch [15/25], Step [275/607], Loss: 0.0072\n",
            "Training accuracy 100.0 %\n",
            "Time: 355.36666464805603\n",
            "Epoch [15/25], Step [280/607], Loss: 0.0188\n",
            "Training accuracy 100.0 %\n",
            "Time: 355.5551013946533\n",
            "Epoch [15/25], Step [285/607], Loss: 0.0417\n",
            "Training accuracy 100.0 %\n",
            "Time: 355.78113079071045\n",
            "Epoch [15/25], Step [290/607], Loss: 0.0244\n",
            "Training accuracy 100.0 %\n",
            "Time: 355.99749279022217\n",
            "Epoch [15/25], Step [295/607], Loss: 0.0135\n",
            "Training accuracy 100.0 %\n",
            "Time: 356.23081159591675\n",
            "Epoch [15/25], Step [300/607], Loss: 0.0546\n",
            "Training accuracy 100.0 %\n",
            "Time: 356.42304491996765\n",
            "Epoch [15/25], Step [305/607], Loss: 0.0172\n",
            "Training accuracy 100.0 %\n",
            "Time: 356.61419343948364\n",
            "Epoch [15/25], Step [310/607], Loss: 0.0070\n",
            "Training accuracy 100.0 %\n",
            "Time: 356.803382396698\n",
            "Epoch [15/25], Step [315/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 356.9811658859253\n",
            "Epoch [15/25], Step [320/607], Loss: 0.0086\n",
            "Training accuracy 100.0 %\n",
            "Time: 357.15854954719543\n",
            "Epoch [15/25], Step [325/607], Loss: 0.0145\n",
            "Training accuracy 100.0 %\n",
            "Time: 357.3630213737488\n",
            "Epoch [15/25], Step [330/607], Loss: 0.0224\n",
            "Training accuracy 100.0 %\n",
            "Time: 357.54991388320923\n",
            "Epoch [15/25], Step [335/607], Loss: 0.0732\n",
            "Training accuracy 93.75 %\n",
            "Time: 357.74985337257385\n",
            "Epoch [15/25], Step [340/607], Loss: 0.0102\n",
            "Training accuracy 100.0 %\n",
            "Time: 357.9221622943878\n",
            "Epoch [15/25], Step [345/607], Loss: 0.0067\n",
            "Training accuracy 100.0 %\n",
            "Time: 358.1620786190033\n",
            "Epoch [15/25], Step [350/607], Loss: 0.0033\n",
            "Training accuracy 100.0 %\n",
            "Time: 358.3359169960022\n",
            "Epoch [15/25], Step [355/607], Loss: 0.0078\n",
            "Training accuracy 100.0 %\n",
            "Time: 358.55058574676514\n",
            "Epoch [15/25], Step [360/607], Loss: 0.0691\n",
            "Training accuracy 100.0 %\n",
            "Time: 358.73481822013855\n",
            "Epoch [15/25], Step [365/607], Loss: 0.0359\n",
            "Training accuracy 100.0 %\n",
            "Time: 358.9423372745514\n",
            "Epoch [15/25], Step [370/607], Loss: 0.0195\n",
            "Training accuracy 100.0 %\n",
            "Time: 359.1569457054138\n",
            "Epoch [15/25], Step [375/607], Loss: 0.0658\n",
            "Training accuracy 93.75 %\n",
            "Time: 359.3730022907257\n",
            "Epoch [15/25], Step [380/607], Loss: 0.0378\n",
            "Training accuracy 100.0 %\n",
            "Time: 359.58503341674805\n",
            "Epoch [15/25], Step [385/607], Loss: 0.1041\n",
            "Training accuracy 93.75 %\n",
            "Time: 359.7979898452759\n",
            "Epoch [15/25], Step [390/607], Loss: 0.4047\n",
            "Training accuracy 87.5 %\n",
            "Time: 359.9917199611664\n",
            "Epoch [15/25], Step [395/607], Loss: 0.0660\n",
            "Training accuracy 93.75 %\n",
            "Time: 360.2242748737335\n",
            "Epoch [15/25], Step [400/607], Loss: 0.0281\n",
            "Training accuracy 100.0 %\n",
            "Time: 360.41355538368225\n",
            "Epoch [15/25], Step [405/607], Loss: 0.0030\n",
            "Training accuracy 100.0 %\n",
            "Time: 360.60324573516846\n",
            "Epoch [15/25], Step [410/607], Loss: 0.0088\n",
            "Training accuracy 100.0 %\n",
            "Time: 360.8025095462799\n",
            "Epoch [15/25], Step [415/607], Loss: 0.0033\n",
            "Training accuracy 100.0 %\n",
            "Time: 360.9859902858734\n",
            "Epoch [15/25], Step [420/607], Loss: 0.0090\n",
            "Training accuracy 100.0 %\n",
            "Time: 361.1804769039154\n",
            "Epoch [15/25], Step [425/607], Loss: 0.0896\n",
            "Training accuracy 93.75 %\n",
            "Time: 361.4113256931305\n",
            "Epoch [15/25], Step [430/607], Loss: 0.0167\n",
            "Training accuracy 100.0 %\n",
            "Time: 361.62172651290894\n",
            "Epoch [15/25], Step [435/607], Loss: 0.0205\n",
            "Training accuracy 100.0 %\n",
            "Time: 361.83595848083496\n",
            "Epoch [15/25], Step [440/607], Loss: 0.0222\n",
            "Training accuracy 100.0 %\n",
            "Time: 362.03909373283386\n",
            "Epoch [15/25], Step [445/607], Loss: 0.0054\n",
            "Training accuracy 100.0 %\n",
            "Time: 362.25733375549316\n",
            "Epoch [15/25], Step [450/607], Loss: 0.0128\n",
            "Training accuracy 100.0 %\n",
            "Time: 362.45520305633545\n",
            "Epoch [15/25], Step [455/607], Loss: 0.0073\n",
            "Training accuracy 100.0 %\n",
            "Time: 362.66724371910095\n",
            "Epoch [15/25], Step [460/607], Loss: 0.0236\n",
            "Training accuracy 100.0 %\n",
            "Time: 362.8666706085205\n",
            "Epoch [15/25], Step [465/607], Loss: 0.1148\n",
            "Training accuracy 93.75 %\n",
            "Time: 363.08906984329224\n",
            "Epoch [15/25], Step [470/607], Loss: 0.0152\n",
            "Training accuracy 100.0 %\n",
            "Time: 363.27700996398926\n",
            "Epoch [15/25], Step [475/607], Loss: 0.0128\n",
            "Training accuracy 100.0 %\n",
            "Time: 363.49002623558044\n",
            "Epoch [15/25], Step [480/607], Loss: 0.0837\n",
            "Training accuracy 93.75 %\n",
            "Time: 363.66168761253357\n",
            "Epoch [15/25], Step [485/607], Loss: 0.0039\n",
            "Training accuracy 100.0 %\n",
            "Time: 363.872727394104\n",
            "Epoch [15/25], Step [490/607], Loss: 0.1689\n",
            "Training accuracy 93.75 %\n",
            "Time: 364.0603997707367\n",
            "Epoch [15/25], Step [495/607], Loss: 0.0115\n",
            "Training accuracy 100.0 %\n",
            "Time: 364.25798630714417\n",
            "Epoch [15/25], Step [500/607], Loss: 1.2913\n",
            "Training accuracy 93.75 %\n",
            "Time: 364.44421315193176\n",
            "Epoch [15/25], Step [505/607], Loss: 0.0088\n",
            "Training accuracy 100.0 %\n",
            "Time: 364.672509431839\n",
            "Epoch [15/25], Step [510/607], Loss: 0.0263\n",
            "Training accuracy 100.0 %\n",
            "Time: 364.8586313724518\n",
            "Epoch [15/25], Step [515/607], Loss: 0.0010\n",
            "Training accuracy 100.0 %\n",
            "Time: 365.0586893558502\n",
            "Epoch [15/25], Step [520/607], Loss: 0.0656\n",
            "Training accuracy 100.0 %\n",
            "Time: 365.2679569721222\n",
            "Epoch [15/25], Step [525/607], Loss: 0.0906\n",
            "Training accuracy 93.75 %\n",
            "Time: 365.47779273986816\n",
            "Epoch [15/25], Step [530/607], Loss: 0.0168\n",
            "Training accuracy 100.0 %\n",
            "Time: 365.6732804775238\n",
            "Epoch [15/25], Step [535/607], Loss: 0.0648\n",
            "Training accuracy 100.0 %\n",
            "Time: 365.9037172794342\n",
            "Epoch [15/25], Step [540/607], Loss: 0.1141\n",
            "Training accuracy 93.75 %\n",
            "Time: 366.1029052734375\n",
            "Epoch [15/25], Step [545/607], Loss: 0.0204\n",
            "Training accuracy 100.0 %\n",
            "Time: 366.3289794921875\n",
            "Epoch [15/25], Step [550/607], Loss: 0.0362\n",
            "Training accuracy 100.0 %\n",
            "Time: 366.5023455619812\n",
            "Epoch [15/25], Step [555/607], Loss: 0.0087\n",
            "Training accuracy 100.0 %\n",
            "Time: 366.6821804046631\n",
            "Epoch [15/25], Step [560/607], Loss: 0.0173\n",
            "Training accuracy 100.0 %\n",
            "Time: 366.86815214157104\n",
            "Epoch [15/25], Step [565/607], Loss: 0.0914\n",
            "Training accuracy 93.75 %\n",
            "Time: 367.07307386398315\n",
            "Epoch [15/25], Step [570/607], Loss: 0.1765\n",
            "Training accuracy 87.5 %\n",
            "Time: 367.2621006965637\n",
            "Epoch [15/25], Step [575/607], Loss: 0.0203\n",
            "Training accuracy 100.0 %\n",
            "Time: 367.4672358036041\n",
            "Epoch [15/25], Step [580/607], Loss: 0.0254\n",
            "Training accuracy 100.0 %\n",
            "Time: 367.66355323791504\n",
            "Epoch [15/25], Step [585/607], Loss: 0.0072\n",
            "Training accuracy 100.0 %\n",
            "Time: 367.8690414428711\n",
            "Epoch [15/25], Step [590/607], Loss: 0.0610\n",
            "Training accuracy 93.75 %\n",
            "Time: 368.05006289482117\n",
            "Epoch [15/25], Step [595/607], Loss: 0.0289\n",
            "Training accuracy 100.0 %\n",
            "Time: 368.2650032043457\n",
            "Epoch [15/25], Step [600/607], Loss: 0.0489\n",
            "Training accuracy 100.0 %\n",
            "Time: 368.4502954483032\n",
            "Epoch [15/25], Step [605/607], Loss: 0.0132\n",
            "Training accuracy 100.0 %\n",
            "Time: 368.63327074050903\n",
            "epoch 16\n",
            "Epoch [16/25], Step [5/607], Loss: 0.0198\n",
            "Training accuracy 100.0 %\n",
            "Time: 369.0622594356537\n",
            "Epoch [16/25], Step [10/607], Loss: 0.0223\n",
            "Training accuracy 100.0 %\n",
            "Time: 369.2598080635071\n",
            "Epoch [16/25], Step [15/607], Loss: 0.1452\n",
            "Training accuracy 87.5 %\n",
            "Time: 369.47060990333557\n",
            "Epoch [16/25], Step [20/607], Loss: 0.0247\n",
            "Training accuracy 100.0 %\n",
            "Time: 369.6457986831665\n",
            "Epoch [16/25], Step [25/607], Loss: 0.0051\n",
            "Training accuracy 100.0 %\n",
            "Time: 369.8345491886139\n",
            "Epoch [16/25], Step [30/607], Loss: 0.0033\n",
            "Training accuracy 100.0 %\n",
            "Time: 370.02083802223206\n",
            "Epoch [16/25], Step [35/607], Loss: 0.0015\n",
            "Training accuracy 100.0 %\n",
            "Time: 370.2335216999054\n",
            "Epoch [16/25], Step [40/607], Loss: 0.0331\n",
            "Training accuracy 100.0 %\n",
            "Time: 370.42770528793335\n",
            "Epoch [16/25], Step [45/607], Loss: 0.0495\n",
            "Training accuracy 100.0 %\n",
            "Time: 370.64059805870056\n",
            "Epoch [16/25], Step [50/607], Loss: 0.0040\n",
            "Training accuracy 100.0 %\n",
            "Time: 370.81867003440857\n",
            "Epoch [16/25], Step [55/607], Loss: 0.0231\n",
            "Training accuracy 100.0 %\n",
            "Time: 371.0237810611725\n",
            "Epoch [16/25], Step [60/607], Loss: 0.0383\n",
            "Training accuracy 100.0 %\n",
            "Time: 371.22277665138245\n",
            "Epoch [16/25], Step [65/607], Loss: 0.0375\n",
            "Training accuracy 100.0 %\n",
            "Time: 371.43695759773254\n",
            "Epoch [16/25], Step [70/607], Loss: 0.0115\n",
            "Training accuracy 100.0 %\n",
            "Time: 371.6245777606964\n",
            "Epoch [16/25], Step [75/607], Loss: 0.0066\n",
            "Training accuracy 100.0 %\n",
            "Time: 371.84194326400757\n",
            "Epoch [16/25], Step [80/607], Loss: 0.0386\n",
            "Training accuracy 100.0 %\n",
            "Time: 372.02793550491333\n",
            "Epoch [16/25], Step [85/607], Loss: 0.0053\n",
            "Training accuracy 100.0 %\n",
            "Time: 372.21960711479187\n",
            "Epoch [16/25], Step [90/607], Loss: 0.0252\n",
            "Training accuracy 100.0 %\n",
            "Time: 372.4339063167572\n",
            "Epoch [16/25], Step [95/607], Loss: 0.0063\n",
            "Training accuracy 100.0 %\n",
            "Time: 372.6310896873474\n",
            "Epoch [16/25], Step [100/607], Loss: 0.0729\n",
            "Training accuracy 93.75 %\n",
            "Time: 372.8527412414551\n",
            "Epoch [16/25], Step [105/607], Loss: 0.1283\n",
            "Training accuracy 93.75 %\n",
            "Time: 373.02799940109253\n",
            "Epoch [16/25], Step [110/607], Loss: 0.0039\n",
            "Training accuracy 100.0 %\n",
            "Time: 373.2336497306824\n",
            "Epoch [16/25], Step [115/607], Loss: 0.0091\n",
            "Training accuracy 100.0 %\n",
            "Time: 373.4396514892578\n",
            "Epoch [16/25], Step [120/607], Loss: 0.0420\n",
            "Training accuracy 100.0 %\n",
            "Time: 373.6563968658447\n",
            "Epoch [16/25], Step [125/607], Loss: 0.0022\n",
            "Training accuracy 100.0 %\n",
            "Time: 373.86242842674255\n",
            "Epoch [16/25], Step [130/607], Loss: 0.0233\n",
            "Training accuracy 100.0 %\n",
            "Time: 374.06100606918335\n",
            "Epoch [16/25], Step [135/607], Loss: 0.0567\n",
            "Training accuracy 93.75 %\n",
            "Time: 374.271222114563\n",
            "Epoch [16/25], Step [140/607], Loss: 0.0098\n",
            "Training accuracy 100.0 %\n",
            "Time: 374.4805145263672\n",
            "Epoch [16/25], Step [145/607], Loss: 0.0423\n",
            "Training accuracy 100.0 %\n",
            "Time: 374.67575454711914\n",
            "Epoch [16/25], Step [150/607], Loss: 0.0335\n",
            "Training accuracy 100.0 %\n",
            "Time: 374.9004991054535\n",
            "Epoch [16/25], Step [155/607], Loss: 0.0059\n",
            "Training accuracy 100.0 %\n",
            "Time: 375.08350586891174\n",
            "Epoch [16/25], Step [160/607], Loss: 0.0363\n",
            "Training accuracy 100.0 %\n",
            "Time: 375.28172874450684\n",
            "Epoch [16/25], Step [165/607], Loss: 0.0766\n",
            "Training accuracy 93.75 %\n",
            "Time: 375.4714689254761\n",
            "Epoch [16/25], Step [170/607], Loss: 0.0848\n",
            "Training accuracy 93.75 %\n",
            "Time: 375.68754148483276\n",
            "Epoch [16/25], Step [175/607], Loss: 0.2562\n",
            "Training accuracy 93.75 %\n",
            "Time: 375.8665370941162\n",
            "Epoch [16/25], Step [180/607], Loss: 0.0116\n",
            "Training accuracy 100.0 %\n",
            "Time: 376.043719291687\n",
            "Epoch [16/25], Step [185/607], Loss: 0.0266\n",
            "Training accuracy 100.0 %\n",
            "Time: 376.2590389251709\n",
            "Epoch [16/25], Step [190/607], Loss: 0.0400\n",
            "Training accuracy 100.0 %\n",
            "Time: 376.477338552475\n",
            "Epoch [16/25], Step [195/607], Loss: 0.0098\n",
            "Training accuracy 100.0 %\n",
            "Time: 376.70956158638\n",
            "Epoch [16/25], Step [200/607], Loss: 0.0328\n",
            "Training accuracy 100.0 %\n",
            "Time: 376.92532896995544\n",
            "Epoch [16/25], Step [205/607], Loss: 0.0212\n",
            "Training accuracy 100.0 %\n",
            "Time: 377.1224548816681\n",
            "Epoch [16/25], Step [210/607], Loss: 0.0060\n",
            "Training accuracy 100.0 %\n",
            "Time: 377.31922149658203\n",
            "Epoch [16/25], Step [215/607], Loss: 0.0676\n",
            "Training accuracy 93.75 %\n",
            "Time: 377.5125117301941\n",
            "Epoch [16/25], Step [220/607], Loss: 0.0057\n",
            "Training accuracy 100.0 %\n",
            "Time: 377.7007484436035\n",
            "Epoch [16/25], Step [225/607], Loss: 0.3959\n",
            "Training accuracy 93.75 %\n",
            "Time: 377.91011333465576\n",
            "Epoch [16/25], Step [230/607], Loss: 0.0539\n",
            "Training accuracy 100.0 %\n",
            "Time: 378.10708236694336\n",
            "Epoch [16/25], Step [235/607], Loss: 0.0779\n",
            "Training accuracy 100.0 %\n",
            "Time: 378.30319023132324\n",
            "Epoch [16/25], Step [240/607], Loss: 0.2543\n",
            "Training accuracy 93.75 %\n",
            "Time: 378.47977447509766\n",
            "Epoch [16/25], Step [245/607], Loss: 0.0196\n",
            "Training accuracy 100.0 %\n",
            "Time: 378.67731761932373\n",
            "Epoch [16/25], Step [250/607], Loss: 0.0058\n",
            "Training accuracy 100.0 %\n",
            "Time: 378.8659677505493\n",
            "Epoch [16/25], Step [255/607], Loss: 0.0806\n",
            "Training accuracy 93.75 %\n",
            "Time: 379.0677638053894\n",
            "Epoch [16/25], Step [260/607], Loss: 0.1316\n",
            "Training accuracy 93.75 %\n",
            "Time: 379.27250242233276\n",
            "Epoch [16/25], Step [265/607], Loss: 0.0128\n",
            "Training accuracy 100.0 %\n",
            "Time: 379.4705693721771\n",
            "Epoch [16/25], Step [270/607], Loss: 0.0318\n",
            "Training accuracy 100.0 %\n",
            "Time: 379.68088030815125\n",
            "Epoch [16/25], Step [275/607], Loss: 0.0093\n",
            "Training accuracy 100.0 %\n",
            "Time: 379.8663787841797\n",
            "Epoch [16/25], Step [280/607], Loss: 0.0128\n",
            "Training accuracy 100.0 %\n",
            "Time: 380.0960941314697\n",
            "Epoch [16/25], Step [285/607], Loss: 0.2135\n",
            "Training accuracy 93.75 %\n",
            "Time: 380.28465366363525\n",
            "Epoch [16/25], Step [290/607], Loss: 0.0115\n",
            "Training accuracy 100.0 %\n",
            "Time: 380.47858691215515\n",
            "Epoch [16/25], Step [295/607], Loss: 0.0061\n",
            "Training accuracy 100.0 %\n",
            "Time: 380.6723964214325\n",
            "Epoch [16/25], Step [300/607], Loss: 0.0072\n",
            "Training accuracy 100.0 %\n",
            "Time: 380.8939471244812\n",
            "Epoch [16/25], Step [305/607], Loss: 0.1172\n",
            "Training accuracy 93.75 %\n",
            "Time: 381.0877785682678\n",
            "Epoch [16/25], Step [310/607], Loss: 0.0459\n",
            "Training accuracy 100.0 %\n",
            "Time: 381.30455899238586\n",
            "Epoch [16/25], Step [315/607], Loss: 0.0270\n",
            "Training accuracy 100.0 %\n",
            "Time: 381.5151889324188\n",
            "Epoch [16/25], Step [320/607], Loss: 0.0250\n",
            "Training accuracy 100.0 %\n",
            "Time: 381.73068952560425\n",
            "Epoch [16/25], Step [325/607], Loss: 0.0178\n",
            "Training accuracy 100.0 %\n",
            "Time: 381.94753789901733\n",
            "Epoch [16/25], Step [330/607], Loss: 0.0217\n",
            "Training accuracy 100.0 %\n",
            "Time: 382.16008973121643\n",
            "Epoch [16/25], Step [335/607], Loss: 0.0422\n",
            "Training accuracy 100.0 %\n",
            "Time: 382.3580393791199\n",
            "Epoch [16/25], Step [340/607], Loss: 0.0470\n",
            "Training accuracy 100.0 %\n",
            "Time: 382.58393359184265\n",
            "Epoch [16/25], Step [345/607], Loss: 0.0008\n",
            "Training accuracy 100.0 %\n",
            "Time: 382.7757270336151\n",
            "Epoch [16/25], Step [350/607], Loss: 0.3325\n",
            "Training accuracy 87.5 %\n",
            "Time: 382.97468423843384\n",
            "Epoch [16/25], Step [355/607], Loss: 0.0180\n",
            "Training accuracy 100.0 %\n",
            "Time: 383.16427302360535\n",
            "Epoch [16/25], Step [360/607], Loss: 0.0544\n",
            "Training accuracy 100.0 %\n",
            "Time: 383.38832545280457\n",
            "Epoch [16/25], Step [365/607], Loss: 0.0046\n",
            "Training accuracy 100.0 %\n",
            "Time: 383.59110283851624\n",
            "Epoch [16/25], Step [370/607], Loss: 0.3860\n",
            "Training accuracy 93.75 %\n",
            "Time: 383.7976882457733\n",
            "Epoch [16/25], Step [375/607], Loss: 0.0576\n",
            "Training accuracy 100.0 %\n",
            "Time: 383.9925711154938\n",
            "Epoch [16/25], Step [380/607], Loss: 0.0335\n",
            "Training accuracy 100.0 %\n",
            "Time: 384.2063217163086\n",
            "Epoch [16/25], Step [385/607], Loss: 0.0552\n",
            "Training accuracy 100.0 %\n",
            "Time: 384.40038084983826\n",
            "Epoch [16/25], Step [390/607], Loss: 0.0145\n",
            "Training accuracy 100.0 %\n",
            "Time: 384.6197235584259\n",
            "Epoch [16/25], Step [395/607], Loss: 0.0302\n",
            "Training accuracy 100.0 %\n",
            "Time: 384.8054766654968\n",
            "Epoch [16/25], Step [400/607], Loss: 0.0917\n",
            "Training accuracy 93.75 %\n",
            "Time: 385.0242609977722\n",
            "Epoch [16/25], Step [405/607], Loss: 0.0173\n",
            "Training accuracy 100.0 %\n",
            "Time: 385.2014698982239\n",
            "Epoch [16/25], Step [410/607], Loss: 0.0179\n",
            "Training accuracy 100.0 %\n",
            "Time: 385.4029166698456\n",
            "Epoch [16/25], Step [415/607], Loss: 0.0205\n",
            "Training accuracy 100.0 %\n",
            "Time: 385.59575366973877\n",
            "Epoch [16/25], Step [420/607], Loss: 0.0066\n",
            "Training accuracy 100.0 %\n",
            "Time: 385.79908776283264\n",
            "Epoch [16/25], Step [425/607], Loss: 0.2831\n",
            "Training accuracy 87.5 %\n",
            "Time: 385.97470021247864\n",
            "Epoch [16/25], Step [430/607], Loss: 0.0798\n",
            "Training accuracy 93.75 %\n",
            "Time: 386.2103626728058\n",
            "Epoch [16/25], Step [435/607], Loss: 0.0068\n",
            "Training accuracy 100.0 %\n",
            "Time: 386.3864426612854\n",
            "Epoch [16/25], Step [440/607], Loss: 0.0084\n",
            "Training accuracy 100.0 %\n",
            "Time: 386.6110327243805\n",
            "Epoch [16/25], Step [445/607], Loss: 0.0628\n",
            "Training accuracy 93.75 %\n",
            "Time: 386.7990891933441\n",
            "Epoch [16/25], Step [450/607], Loss: 0.0055\n",
            "Training accuracy 100.0 %\n",
            "Time: 387.0234041213989\n",
            "Epoch [16/25], Step [455/607], Loss: 0.0268\n",
            "Training accuracy 100.0 %\n",
            "Time: 387.20855379104614\n",
            "Epoch [16/25], Step [460/607], Loss: 0.0095\n",
            "Training accuracy 100.0 %\n",
            "Time: 387.42372131347656\n",
            "Epoch [16/25], Step [465/607], Loss: 0.0126\n",
            "Training accuracy 100.0 %\n",
            "Time: 387.6144211292267\n",
            "Epoch [16/25], Step [470/607], Loss: 0.0017\n",
            "Training accuracy 100.0 %\n",
            "Time: 387.8099949359894\n",
            "Epoch [16/25], Step [475/607], Loss: 0.4568\n",
            "Training accuracy 93.75 %\n",
            "Time: 388.00168347358704\n",
            "Epoch [16/25], Step [480/607], Loss: 0.0107\n",
            "Training accuracy 100.0 %\n",
            "Time: 388.217812538147\n",
            "Epoch [16/25], Step [485/607], Loss: 0.0130\n",
            "Training accuracy 100.0 %\n",
            "Time: 388.40540075302124\n",
            "Epoch [16/25], Step [490/607], Loss: 0.0163\n",
            "Training accuracy 100.0 %\n",
            "Time: 388.6331114768982\n",
            "Epoch [16/25], Step [495/607], Loss: 0.0049\n",
            "Training accuracy 100.0 %\n",
            "Time: 388.82236337661743\n",
            "Epoch [16/25], Step [500/607], Loss: 0.0122\n",
            "Training accuracy 100.0 %\n",
            "Time: 389.0305061340332\n",
            "Epoch [16/25], Step [505/607], Loss: 0.0103\n",
            "Training accuracy 100.0 %\n",
            "Time: 389.217027425766\n",
            "Epoch [16/25], Step [510/607], Loss: 0.0266\n",
            "Training accuracy 100.0 %\n",
            "Time: 389.4253969192505\n",
            "Epoch [16/25], Step [515/607], Loss: 0.1946\n",
            "Training accuracy 87.5 %\n",
            "Time: 389.62097668647766\n",
            "Epoch [16/25], Step [520/607], Loss: 0.0117\n",
            "Training accuracy 100.0 %\n",
            "Time: 389.83780121803284\n",
            "Epoch [16/25], Step [525/607], Loss: 0.0737\n",
            "Training accuracy 93.75 %\n",
            "Time: 390.0204248428345\n",
            "Epoch [16/25], Step [530/607], Loss: 0.0135\n",
            "Training accuracy 100.0 %\n",
            "Time: 390.24411487579346\n",
            "Epoch [16/25], Step [535/607], Loss: 0.0055\n",
            "Training accuracy 100.0 %\n",
            "Time: 390.43679213523865\n",
            "Epoch [16/25], Step [540/607], Loss: 0.0063\n",
            "Training accuracy 100.0 %\n",
            "Time: 390.6633040904999\n",
            "Epoch [16/25], Step [545/607], Loss: 0.2175\n",
            "Training accuracy 93.75 %\n",
            "Time: 390.847975730896\n",
            "Epoch [16/25], Step [550/607], Loss: 0.0197\n",
            "Training accuracy 100.0 %\n",
            "Time: 391.03208565711975\n",
            "Epoch [16/25], Step [555/607], Loss: 0.0590\n",
            "Training accuracy 93.75 %\n",
            "Time: 391.2247369289398\n",
            "Epoch [16/25], Step [560/607], Loss: 0.0161\n",
            "Training accuracy 100.0 %\n",
            "Time: 391.45003271102905\n",
            "Epoch [16/25], Step [565/607], Loss: 0.0048\n",
            "Training accuracy 100.0 %\n",
            "Time: 391.63185811042786\n",
            "Epoch [16/25], Step [570/607], Loss: 0.0106\n",
            "Training accuracy 100.0 %\n",
            "Time: 391.8435516357422\n",
            "Epoch [16/25], Step [575/607], Loss: 0.0030\n",
            "Training accuracy 100.0 %\n",
            "Time: 392.01684403419495\n",
            "Epoch [16/25], Step [580/607], Loss: 0.0151\n",
            "Training accuracy 100.0 %\n",
            "Time: 392.23357605934143\n",
            "Epoch [16/25], Step [585/607], Loss: 0.0428\n",
            "Training accuracy 100.0 %\n",
            "Time: 392.4440836906433\n",
            "Epoch [16/25], Step [590/607], Loss: 0.0184\n",
            "Training accuracy 100.0 %\n",
            "Time: 392.65257811546326\n",
            "Epoch [16/25], Step [595/607], Loss: 0.0429\n",
            "Training accuracy 100.0 %\n",
            "Time: 392.8569996356964\n",
            "Epoch [16/25], Step [600/607], Loss: 0.0052\n",
            "Training accuracy 100.0 %\n",
            "Time: 393.0712184906006\n",
            "Epoch [16/25], Step [605/607], Loss: 0.0239\n",
            "Training accuracy 100.0 %\n",
            "Time: 393.26779985427856\n",
            "epoch 17\n",
            "Epoch [17/25], Step [5/607], Loss: 0.0050\n",
            "Training accuracy 100.0 %\n",
            "Time: 393.68411803245544\n",
            "Epoch [17/25], Step [10/607], Loss: 0.0220\n",
            "Training accuracy 100.0 %\n",
            "Time: 393.8796076774597\n",
            "Epoch [17/25], Step [15/607], Loss: 0.0050\n",
            "Training accuracy 100.0 %\n",
            "Time: 394.09975242614746\n",
            "Epoch [17/25], Step [20/607], Loss: 0.0099\n",
            "Training accuracy 100.0 %\n",
            "Time: 394.2917950153351\n",
            "Epoch [17/25], Step [25/607], Loss: 0.0104\n",
            "Training accuracy 100.0 %\n",
            "Time: 394.5201749801636\n",
            "Epoch [17/25], Step [30/607], Loss: 0.0430\n",
            "Training accuracy 100.0 %\n",
            "Time: 394.7157988548279\n",
            "Epoch [17/25], Step [35/607], Loss: 0.0347\n",
            "Training accuracy 100.0 %\n",
            "Time: 394.9298188686371\n",
            "Epoch [17/25], Step [40/607], Loss: 0.0207\n",
            "Training accuracy 100.0 %\n",
            "Time: 395.1023225784302\n",
            "Epoch [17/25], Step [45/607], Loss: 0.0064\n",
            "Training accuracy 100.0 %\n",
            "Time: 395.2928831577301\n",
            "Epoch [17/25], Step [50/607], Loss: 0.0097\n",
            "Training accuracy 100.0 %\n",
            "Time: 395.4806716442108\n",
            "Epoch [17/25], Step [55/607], Loss: 0.0267\n",
            "Training accuracy 100.0 %\n",
            "Time: 395.6838984489441\n",
            "Epoch [17/25], Step [60/607], Loss: 0.0426\n",
            "Training accuracy 100.0 %\n",
            "Time: 395.8727855682373\n",
            "Epoch [17/25], Step [65/607], Loss: 0.0233\n",
            "Training accuracy 100.0 %\n",
            "Time: 396.099059343338\n",
            "Epoch [17/25], Step [70/607], Loss: 0.0094\n",
            "Training accuracy 100.0 %\n",
            "Time: 396.3134140968323\n",
            "Epoch [17/25], Step [75/607], Loss: 0.0047\n",
            "Training accuracy 100.0 %\n",
            "Time: 396.50205540657043\n",
            "Epoch [17/25], Step [80/607], Loss: 0.0062\n",
            "Training accuracy 100.0 %\n",
            "Time: 396.6981408596039\n",
            "Epoch [17/25], Step [85/607], Loss: 0.0395\n",
            "Training accuracy 100.0 %\n",
            "Time: 396.91727924346924\n",
            "Epoch [17/25], Step [90/607], Loss: 0.0140\n",
            "Training accuracy 100.0 %\n",
            "Time: 397.1292943954468\n",
            "Epoch [17/25], Step [95/607], Loss: 0.0736\n",
            "Training accuracy 93.75 %\n",
            "Time: 397.3709452152252\n",
            "Epoch [17/25], Step [100/607], Loss: 0.0143\n",
            "Training accuracy 100.0 %\n",
            "Time: 397.55742263793945\n",
            "Epoch [17/25], Step [105/607], Loss: 0.0076\n",
            "Training accuracy 100.0 %\n",
            "Time: 397.7663366794586\n",
            "Epoch [17/25], Step [110/607], Loss: 0.0256\n",
            "Training accuracy 100.0 %\n",
            "Time: 397.9524350166321\n",
            "Epoch [17/25], Step [115/607], Loss: 0.0071\n",
            "Training accuracy 100.0 %\n",
            "Time: 398.15828466415405\n",
            "Epoch [17/25], Step [120/607], Loss: 0.0259\n",
            "Training accuracy 100.0 %\n",
            "Time: 398.36393213272095\n",
            "Epoch [17/25], Step [125/607], Loss: 0.1484\n",
            "Training accuracy 87.5 %\n",
            "Time: 398.58191990852356\n",
            "Epoch [17/25], Step [130/607], Loss: 0.0535\n",
            "Training accuracy 100.0 %\n",
            "Time: 398.7860107421875\n",
            "Epoch [17/25], Step [135/607], Loss: 0.0152\n",
            "Training accuracy 100.0 %\n",
            "Time: 399.0130703449249\n",
            "Epoch [17/25], Step [140/607], Loss: 0.0098\n",
            "Training accuracy 100.0 %\n",
            "Time: 399.223845243454\n",
            "Epoch [17/25], Step [145/607], Loss: 0.0271\n",
            "Training accuracy 100.0 %\n",
            "Time: 399.4268915653229\n",
            "Epoch [17/25], Step [150/607], Loss: 0.0153\n",
            "Training accuracy 100.0 %\n",
            "Time: 399.64458751678467\n",
            "Epoch [17/25], Step [155/607], Loss: 0.0037\n",
            "Training accuracy 100.0 %\n",
            "Time: 399.8500323295593\n",
            "Epoch [17/25], Step [160/607], Loss: 0.0221\n",
            "Training accuracy 100.0 %\n",
            "Time: 400.0531916618347\n",
            "Epoch [17/25], Step [165/607], Loss: 0.1528\n",
            "Training accuracy 93.75 %\n",
            "Time: 400.25713896751404\n",
            "Epoch [17/25], Step [170/607], Loss: 0.0258\n",
            "Training accuracy 100.0 %\n",
            "Time: 400.44026494026184\n",
            "Epoch [17/25], Step [175/607], Loss: 0.0047\n",
            "Training accuracy 100.0 %\n",
            "Time: 400.62501788139343\n",
            "Epoch [17/25], Step [180/607], Loss: 0.0130\n",
            "Training accuracy 100.0 %\n",
            "Time: 400.8106052875519\n",
            "Epoch [17/25], Step [185/607], Loss: 0.0042\n",
            "Training accuracy 100.0 %\n",
            "Time: 401.0181498527527\n",
            "Epoch [17/25], Step [190/607], Loss: 0.0149\n",
            "Training accuracy 100.0 %\n",
            "Time: 401.2073757648468\n",
            "Epoch [17/25], Step [195/607], Loss: 0.0052\n",
            "Training accuracy 100.0 %\n",
            "Time: 401.41190814971924\n",
            "Epoch [17/25], Step [200/607], Loss: 0.0082\n",
            "Training accuracy 100.0 %\n",
            "Time: 401.6106209754944\n",
            "Epoch [17/25], Step [205/607], Loss: 0.0087\n",
            "Training accuracy 100.0 %\n",
            "Time: 401.8370134830475\n",
            "Epoch [17/25], Step [210/607], Loss: 0.0505\n",
            "Training accuracy 100.0 %\n",
            "Time: 402.0227289199829\n",
            "Epoch [17/25], Step [215/607], Loss: 0.0240\n",
            "Training accuracy 100.0 %\n",
            "Time: 402.25329089164734\n",
            "Epoch [17/25], Step [220/607], Loss: 0.0509\n",
            "Training accuracy 100.0 %\n",
            "Time: 402.4671308994293\n",
            "Epoch [17/25], Step [225/607], Loss: 0.0670\n",
            "Training accuracy 93.75 %\n",
            "Time: 402.70089650154114\n",
            "Epoch [17/25], Step [230/607], Loss: 0.0213\n",
            "Training accuracy 100.0 %\n",
            "Time: 402.91213512420654\n",
            "Epoch [17/25], Step [235/607], Loss: 0.0064\n",
            "Training accuracy 100.0 %\n",
            "Time: 403.10460019111633\n",
            "Epoch [17/25], Step [240/607], Loss: 0.0920\n",
            "Training accuracy 93.75 %\n",
            "Time: 403.3184127807617\n",
            "Epoch [17/25], Step [245/607], Loss: 0.0038\n",
            "Training accuracy 100.0 %\n",
            "Time: 403.5435016155243\n",
            "Epoch [17/25], Step [250/607], Loss: 0.0430\n",
            "Training accuracy 100.0 %\n",
            "Time: 403.75855445861816\n",
            "Epoch [17/25], Step [255/607], Loss: 0.0132\n",
            "Training accuracy 100.0 %\n",
            "Time: 403.9890208244324\n",
            "Epoch [17/25], Step [260/607], Loss: 0.0404\n",
            "Training accuracy 100.0 %\n",
            "Time: 404.1996018886566\n",
            "Epoch [17/25], Step [265/607], Loss: 0.0220\n",
            "Training accuracy 100.0 %\n",
            "Time: 404.39089941978455\n",
            "Epoch [17/25], Step [270/607], Loss: 0.0305\n",
            "Training accuracy 100.0 %\n",
            "Time: 404.5735836029053\n",
            "Epoch [17/25], Step [275/607], Loss: 0.0302\n",
            "Training accuracy 100.0 %\n",
            "Time: 404.7778367996216\n",
            "Epoch [17/25], Step [280/607], Loss: 0.0489\n",
            "Training accuracy 100.0 %\n",
            "Time: 404.96960377693176\n",
            "Epoch [17/25], Step [285/607], Loss: 0.0493\n",
            "Training accuracy 93.75 %\n",
            "Time: 405.2042946815491\n",
            "Epoch [17/25], Step [290/607], Loss: 0.0389\n",
            "Training accuracy 100.0 %\n",
            "Time: 405.3971748352051\n",
            "Epoch [17/25], Step [295/607], Loss: 0.0172\n",
            "Training accuracy 100.0 %\n",
            "Time: 405.6046464443207\n",
            "Epoch [17/25], Step [300/607], Loss: 0.0436\n",
            "Training accuracy 100.0 %\n",
            "Time: 405.7970736026764\n",
            "Epoch [17/25], Step [305/607], Loss: 0.0188\n",
            "Training accuracy 100.0 %\n",
            "Time: 405.995689868927\n",
            "Epoch [17/25], Step [310/607], Loss: 0.0222\n",
            "Training accuracy 100.0 %\n",
            "Time: 406.1728467941284\n",
            "Epoch [17/25], Step [315/607], Loss: 0.0374\n",
            "Training accuracy 100.0 %\n",
            "Time: 406.40159726142883\n",
            "Epoch [17/25], Step [320/607], Loss: 0.0102\n",
            "Training accuracy 100.0 %\n",
            "Time: 406.6099672317505\n",
            "Epoch [17/25], Step [325/607], Loss: 0.0090\n",
            "Training accuracy 100.0 %\n",
            "Time: 406.8193824291229\n",
            "Epoch [17/25], Step [330/607], Loss: 0.0091\n",
            "Training accuracy 100.0 %\n",
            "Time: 407.0173110961914\n",
            "Epoch [17/25], Step [335/607], Loss: 0.0119\n",
            "Training accuracy 100.0 %\n",
            "Time: 407.23085474967957\n",
            "Epoch [17/25], Step [340/607], Loss: 0.0187\n",
            "Training accuracy 100.0 %\n",
            "Time: 407.4363582134247\n",
            "Epoch [17/25], Step [345/607], Loss: 0.0495\n",
            "Training accuracy 100.0 %\n",
            "Time: 407.6532599925995\n",
            "Epoch [17/25], Step [350/607], Loss: 0.1444\n",
            "Training accuracy 93.75 %\n",
            "Time: 407.8613951206207\n",
            "Epoch [17/25], Step [355/607], Loss: 0.0295\n",
            "Training accuracy 100.0 %\n",
            "Time: 408.07241797447205\n",
            "Epoch [17/25], Step [360/607], Loss: 0.0006\n",
            "Training accuracy 100.0 %\n",
            "Time: 408.2481355667114\n",
            "Epoch [17/25], Step [365/607], Loss: 0.0124\n",
            "Training accuracy 100.0 %\n",
            "Time: 408.42913246154785\n",
            "Epoch [17/25], Step [370/607], Loss: 0.0456\n",
            "Training accuracy 100.0 %\n",
            "Time: 408.6405177116394\n",
            "Epoch [17/25], Step [375/607], Loss: 0.0113\n",
            "Training accuracy 100.0 %\n",
            "Time: 408.8456184864044\n",
            "Epoch [17/25], Step [380/607], Loss: 0.0355\n",
            "Training accuracy 100.0 %\n",
            "Time: 409.06424355506897\n",
            "Epoch [17/25], Step [385/607], Loss: 0.0092\n",
            "Training accuracy 100.0 %\n",
            "Time: 409.2651994228363\n",
            "Epoch [17/25], Step [390/607], Loss: 0.0125\n",
            "Training accuracy 100.0 %\n",
            "Time: 409.4622871875763\n",
            "Epoch [17/25], Step [395/607], Loss: 0.0101\n",
            "Training accuracy 100.0 %\n",
            "Time: 409.6610360145569\n",
            "Epoch [17/25], Step [400/607], Loss: 0.4823\n",
            "Training accuracy 87.5 %\n",
            "Time: 409.8689692020416\n",
            "Epoch [17/25], Step [405/607], Loss: 0.0376\n",
            "Training accuracy 100.0 %\n",
            "Time: 410.06267857551575\n",
            "Epoch [17/25], Step [410/607], Loss: 0.1018\n",
            "Training accuracy 93.75 %\n",
            "Time: 410.25231432914734\n",
            "Epoch [17/25], Step [415/607], Loss: 0.0032\n",
            "Training accuracy 100.0 %\n",
            "Time: 410.43632531166077\n",
            "Epoch [17/25], Step [420/607], Loss: 0.0173\n",
            "Training accuracy 100.0 %\n",
            "Time: 410.65367364883423\n",
            "Epoch [17/25], Step [425/607], Loss: 0.0177\n",
            "Training accuracy 100.0 %\n",
            "Time: 410.83732318878174\n",
            "Epoch [17/25], Step [430/607], Loss: 0.0129\n",
            "Training accuracy 100.0 %\n",
            "Time: 411.04417991638184\n",
            "Epoch [17/25], Step [435/607], Loss: 0.6064\n",
            "Training accuracy 93.75 %\n",
            "Time: 411.2338647842407\n",
            "Epoch [17/25], Step [440/607], Loss: 0.0376\n",
            "Training accuracy 100.0 %\n",
            "Time: 411.44777965545654\n",
            "Epoch [17/25], Step [445/607], Loss: 0.0138\n",
            "Training accuracy 100.0 %\n",
            "Time: 411.63604164123535\n",
            "Epoch [17/25], Step [450/607], Loss: 0.0016\n",
            "Training accuracy 100.0 %\n",
            "Time: 411.8527617454529\n",
            "Epoch [17/25], Step [455/607], Loss: 0.0218\n",
            "Training accuracy 100.0 %\n",
            "Time: 412.0388102531433\n",
            "Epoch [17/25], Step [460/607], Loss: 0.0088\n",
            "Training accuracy 100.0 %\n",
            "Time: 412.23357009887695\n",
            "Epoch [17/25], Step [465/607], Loss: 0.0643\n",
            "Training accuracy 93.75 %\n",
            "Time: 412.42046570777893\n",
            "Epoch [17/25], Step [470/607], Loss: 0.0084\n",
            "Training accuracy 100.0 %\n",
            "Time: 412.6384816169739\n",
            "Epoch [17/25], Step [475/607], Loss: 0.0023\n",
            "Training accuracy 100.0 %\n",
            "Time: 412.8422040939331\n",
            "Epoch [17/25], Step [480/607], Loss: 0.1466\n",
            "Training accuracy 93.75 %\n",
            "Time: 413.0558879375458\n",
            "Epoch [17/25], Step [485/607], Loss: 0.0106\n",
            "Training accuracy 100.0 %\n",
            "Time: 413.26573753356934\n",
            "Epoch [17/25], Step [490/607], Loss: 0.0736\n",
            "Training accuracy 93.75 %\n",
            "Time: 413.46017718315125\n",
            "Epoch [17/25], Step [495/607], Loss: 0.0036\n",
            "Training accuracy 100.0 %\n",
            "Time: 413.6694300174713\n",
            "Epoch [17/25], Step [500/607], Loss: 0.0197\n",
            "Training accuracy 100.0 %\n",
            "Time: 413.85601782798767\n",
            "Epoch [17/25], Step [505/607], Loss: 0.0042\n",
            "Training accuracy 100.0 %\n",
            "Time: 414.05371856689453\n",
            "Epoch [17/25], Step [510/607], Loss: 0.0124\n",
            "Training accuracy 100.0 %\n",
            "Time: 414.2414162158966\n",
            "Epoch [17/25], Step [515/607], Loss: 0.1432\n",
            "Training accuracy 93.75 %\n",
            "Time: 414.4357831478119\n",
            "Epoch [17/25], Step [520/607], Loss: 0.0323\n",
            "Training accuracy 100.0 %\n",
            "Time: 414.6518211364746\n",
            "Epoch [17/25], Step [525/607], Loss: 0.0208\n",
            "Training accuracy 100.0 %\n",
            "Time: 414.8532555103302\n",
            "Epoch [17/25], Step [530/607], Loss: 0.0160\n",
            "Training accuracy 100.0 %\n",
            "Time: 415.06408858299255\n",
            "Epoch [17/25], Step [535/607], Loss: 0.0211\n",
            "Training accuracy 100.0 %\n",
            "Time: 415.25425243377686\n",
            "Epoch [17/25], Step [540/607], Loss: 0.0314\n",
            "Training accuracy 100.0 %\n",
            "Time: 415.4885730743408\n",
            "Epoch [17/25], Step [545/607], Loss: 0.0118\n",
            "Training accuracy 100.0 %\n",
            "Time: 415.6776797771454\n",
            "Epoch [17/25], Step [550/607], Loss: 0.1270\n",
            "Training accuracy 93.75 %\n",
            "Time: 415.8875997066498\n",
            "Epoch [17/25], Step [555/607], Loss: 0.0294\n",
            "Training accuracy 100.0 %\n",
            "Time: 416.07096004486084\n",
            "Epoch [17/25], Step [560/607], Loss: 0.0111\n",
            "Training accuracy 100.0 %\n",
            "Time: 416.2641818523407\n",
            "Epoch [17/25], Step [565/607], Loss: 0.0283\n",
            "Training accuracy 100.0 %\n",
            "Time: 416.4442286491394\n",
            "Epoch [17/25], Step [570/607], Loss: 0.0233\n",
            "Training accuracy 100.0 %\n",
            "Time: 416.67174792289734\n",
            "Epoch [17/25], Step [575/607], Loss: 0.0244\n",
            "Training accuracy 100.0 %\n",
            "Time: 416.86113595962524\n",
            "Epoch [17/25], Step [580/607], Loss: 0.0298\n",
            "Training accuracy 100.0 %\n",
            "Time: 417.0876262187958\n",
            "Epoch [17/25], Step [585/607], Loss: 0.1939\n",
            "Training accuracy 93.75 %\n",
            "Time: 417.26247239112854\n",
            "Epoch [17/25], Step [590/607], Loss: 0.0174\n",
            "Training accuracy 100.0 %\n",
            "Time: 417.4664328098297\n",
            "Epoch [17/25], Step [595/607], Loss: 0.0083\n",
            "Training accuracy 100.0 %\n",
            "Time: 417.6600978374481\n",
            "Epoch [17/25], Step [600/607], Loss: 0.0037\n",
            "Training accuracy 100.0 %\n",
            "Time: 417.881196975708\n",
            "Epoch [17/25], Step [605/607], Loss: 0.0116\n",
            "Training accuracy 100.0 %\n",
            "Time: 418.0706317424774\n",
            "epoch 18\n",
            "Epoch [18/25], Step [5/607], Loss: 0.0017\n",
            "Training accuracy 100.0 %\n",
            "Time: 418.4933032989502\n",
            "Epoch [18/25], Step [10/607], Loss: 0.0069\n",
            "Training accuracy 100.0 %\n",
            "Time: 418.6813712120056\n",
            "Epoch [18/25], Step [15/607], Loss: 0.0528\n",
            "Training accuracy 93.75 %\n",
            "Time: 418.8970773220062\n",
            "Epoch [18/25], Step [20/607], Loss: 0.1330\n",
            "Training accuracy 93.75 %\n",
            "Time: 419.0840964317322\n",
            "Epoch [18/25], Step [25/607], Loss: 0.0038\n",
            "Training accuracy 100.0 %\n",
            "Time: 419.30979204177856\n",
            "Epoch [18/25], Step [30/607], Loss: 0.0262\n",
            "Training accuracy 100.0 %\n",
            "Time: 419.49861121177673\n",
            "Epoch [18/25], Step [35/607], Loss: 0.0148\n",
            "Training accuracy 100.0 %\n",
            "Time: 419.711142539978\n",
            "Epoch [18/25], Step [40/607], Loss: 0.1483\n",
            "Training accuracy 87.5 %\n",
            "Time: 419.895959854126\n",
            "Epoch [18/25], Step [45/607], Loss: 0.0853\n",
            "Training accuracy 93.75 %\n",
            "Time: 420.1053841114044\n",
            "Epoch [18/25], Step [50/607], Loss: 0.0066\n",
            "Training accuracy 100.0 %\n",
            "Time: 420.2968816757202\n",
            "Epoch [18/25], Step [55/607], Loss: 0.0073\n",
            "Training accuracy 100.0 %\n",
            "Time: 420.51385521888733\n",
            "Epoch [18/25], Step [60/607], Loss: 0.0388\n",
            "Training accuracy 100.0 %\n",
            "Time: 420.6976339817047\n",
            "Epoch [18/25], Step [65/607], Loss: 0.0107\n",
            "Training accuracy 100.0 %\n",
            "Time: 420.89458203315735\n",
            "Epoch [18/25], Step [70/607], Loss: 0.0121\n",
            "Training accuracy 100.0 %\n",
            "Time: 421.0948135852814\n",
            "Epoch [18/25], Step [75/607], Loss: 0.0082\n",
            "Training accuracy 100.0 %\n",
            "Time: 421.3285720348358\n",
            "Epoch [18/25], Step [80/607], Loss: 0.0047\n",
            "Training accuracy 100.0 %\n",
            "Time: 421.54326009750366\n",
            "Epoch [18/25], Step [85/607], Loss: 0.0088\n",
            "Training accuracy 100.0 %\n",
            "Time: 421.75907802581787\n",
            "Epoch [18/25], Step [90/607], Loss: 0.0085\n",
            "Training accuracy 100.0 %\n",
            "Time: 421.968621969223\n",
            "Epoch [18/25], Step [95/607], Loss: 0.0557\n",
            "Training accuracy 93.75 %\n",
            "Time: 422.14715027809143\n",
            "Epoch [18/25], Step [100/607], Loss: 0.0096\n",
            "Training accuracy 100.0 %\n",
            "Time: 422.3204708099365\n",
            "Epoch [18/25], Step [105/607], Loss: 0.0077\n",
            "Training accuracy 100.0 %\n",
            "Time: 422.5042576789856\n",
            "Epoch [18/25], Step [110/607], Loss: 0.0072\n",
            "Training accuracy 100.0 %\n",
            "Time: 422.7319886684418\n",
            "Epoch [18/25], Step [115/607], Loss: 0.0721\n",
            "Training accuracy 93.75 %\n",
            "Time: 422.9329056739807\n",
            "Epoch [18/25], Step [120/607], Loss: 0.0743\n",
            "Training accuracy 93.75 %\n",
            "Time: 423.1571819782257\n",
            "Epoch [18/25], Step [125/607], Loss: 0.1346\n",
            "Training accuracy 93.75 %\n",
            "Time: 423.3525388240814\n",
            "Epoch [18/25], Step [130/607], Loss: 0.0456\n",
            "Training accuracy 100.0 %\n",
            "Time: 423.56946563720703\n",
            "Epoch [18/25], Step [135/607], Loss: 0.0344\n",
            "Training accuracy 100.0 %\n",
            "Time: 423.75150752067566\n",
            "Epoch [18/25], Step [140/607], Loss: 0.0184\n",
            "Training accuracy 100.0 %\n",
            "Time: 423.97120809555054\n",
            "Epoch [18/25], Step [145/607], Loss: 0.0412\n",
            "Training accuracy 100.0 %\n",
            "Time: 424.1673893928528\n",
            "Epoch [18/25], Step [150/607], Loss: 0.0074\n",
            "Training accuracy 100.0 %\n",
            "Time: 424.3664519786835\n",
            "Epoch [18/25], Step [155/607], Loss: 0.0076\n",
            "Training accuracy 100.0 %\n",
            "Time: 424.56679010391235\n",
            "Epoch [18/25], Step [160/607], Loss: 0.0023\n",
            "Training accuracy 100.0 %\n",
            "Time: 424.76606011390686\n",
            "Epoch [18/25], Step [165/607], Loss: 0.0043\n",
            "Training accuracy 100.0 %\n",
            "Time: 424.95968866348267\n",
            "Epoch [18/25], Step [170/607], Loss: 0.0014\n",
            "Training accuracy 100.0 %\n",
            "Time: 425.1745388507843\n",
            "Epoch [18/25], Step [175/607], Loss: 0.0041\n",
            "Training accuracy 100.0 %\n",
            "Time: 425.3798189163208\n",
            "Epoch [18/25], Step [180/607], Loss: 0.0066\n",
            "Training accuracy 100.0 %\n",
            "Time: 425.59136033058167\n",
            "Epoch [18/25], Step [185/607], Loss: 0.0030\n",
            "Training accuracy 100.0 %\n",
            "Time: 425.7902891635895\n",
            "Epoch [18/25], Step [190/607], Loss: 0.0065\n",
            "Training accuracy 100.0 %\n",
            "Time: 426.01026368141174\n",
            "Epoch [18/25], Step [195/607], Loss: 0.0049\n",
            "Training accuracy 100.0 %\n",
            "Time: 426.19427394866943\n",
            "Epoch [18/25], Step [200/607], Loss: 0.0294\n",
            "Training accuracy 100.0 %\n",
            "Time: 426.4090664386749\n",
            "Epoch [18/25], Step [205/607], Loss: 0.0004\n",
            "Training accuracy 100.0 %\n",
            "Time: 426.5974886417389\n",
            "Epoch [18/25], Step [210/607], Loss: 0.0006\n",
            "Training accuracy 100.0 %\n",
            "Time: 426.83738446235657\n",
            "Epoch [18/25], Step [215/607], Loss: 0.0770\n",
            "Training accuracy 93.75 %\n",
            "Time: 427.02557587623596\n",
            "Epoch [18/25], Step [220/607], Loss: 0.0246\n",
            "Training accuracy 100.0 %\n",
            "Time: 427.2536177635193\n",
            "Epoch [18/25], Step [225/607], Loss: 0.0155\n",
            "Training accuracy 100.0 %\n",
            "Time: 427.44737458229065\n",
            "Epoch [18/25], Step [230/607], Loss: 0.0057\n",
            "Training accuracy 100.0 %\n",
            "Time: 427.6633780002594\n",
            "Epoch [18/25], Step [235/607], Loss: 0.0046\n",
            "Training accuracy 100.0 %\n",
            "Time: 427.8783447742462\n",
            "Epoch [18/25], Step [240/607], Loss: 0.0091\n",
            "Training accuracy 100.0 %\n",
            "Time: 428.07959747314453\n",
            "Epoch [18/25], Step [245/607], Loss: 0.0243\n",
            "Training accuracy 100.0 %\n",
            "Time: 428.27281165122986\n",
            "Epoch [18/25], Step [250/607], Loss: 0.0439\n",
            "Training accuracy 100.0 %\n",
            "Time: 428.4895098209381\n",
            "Epoch [18/25], Step [255/607], Loss: 0.2590\n",
            "Training accuracy 93.75 %\n",
            "Time: 428.68030428886414\n",
            "Epoch [18/25], Step [260/607], Loss: 0.0144\n",
            "Training accuracy 100.0 %\n",
            "Time: 428.88960242271423\n",
            "Epoch [18/25], Step [265/607], Loss: 0.0162\n",
            "Training accuracy 100.0 %\n",
            "Time: 429.09347009658813\n",
            "Epoch [18/25], Step [270/607], Loss: 0.0023\n",
            "Training accuracy 100.0 %\n",
            "Time: 429.3000273704529\n",
            "Epoch [18/25], Step [275/607], Loss: 0.0127\n",
            "Training accuracy 100.0 %\n",
            "Time: 429.5051209926605\n",
            "Epoch [18/25], Step [280/607], Loss: 0.0059\n",
            "Training accuracy 100.0 %\n",
            "Time: 429.7206127643585\n",
            "Epoch [18/25], Step [285/607], Loss: 0.0199\n",
            "Training accuracy 100.0 %\n",
            "Time: 429.9164502620697\n",
            "Epoch [18/25], Step [290/607], Loss: 0.0230\n",
            "Training accuracy 100.0 %\n",
            "Time: 430.13618063926697\n",
            "Epoch [18/25], Step [295/607], Loss: 0.0140\n",
            "Training accuracy 100.0 %\n",
            "Time: 430.3172652721405\n",
            "Epoch [18/25], Step [300/607], Loss: 0.0744\n",
            "Training accuracy 93.75 %\n",
            "Time: 430.53078174591064\n",
            "Epoch [18/25], Step [305/607], Loss: 0.0059\n",
            "Training accuracy 100.0 %\n",
            "Time: 430.7284553050995\n",
            "Epoch [18/25], Step [310/607], Loss: 0.0007\n",
            "Training accuracy 100.0 %\n",
            "Time: 430.94427490234375\n",
            "Epoch [18/25], Step [315/607], Loss: 1.4684\n",
            "Training accuracy 93.75 %\n",
            "Time: 431.1313099861145\n",
            "Epoch [18/25], Step [320/607], Loss: 0.0064\n",
            "Training accuracy 100.0 %\n",
            "Time: 431.355051279068\n",
            "Epoch [18/25], Step [325/607], Loss: 0.0439\n",
            "Training accuracy 100.0 %\n",
            "Time: 431.5299246311188\n",
            "Epoch [18/25], Step [330/607], Loss: 0.0132\n",
            "Training accuracy 100.0 %\n",
            "Time: 431.740238904953\n",
            "Epoch [18/25], Step [335/607], Loss: 0.0010\n",
            "Training accuracy 100.0 %\n",
            "Time: 431.9306740760803\n",
            "Epoch [18/25], Step [340/607], Loss: 0.0046\n",
            "Training accuracy 100.0 %\n",
            "Time: 432.1336553096771\n",
            "Epoch [18/25], Step [345/607], Loss: 0.0257\n",
            "Training accuracy 100.0 %\n",
            "Time: 432.323846578598\n",
            "Epoch [18/25], Step [350/607], Loss: 0.0211\n",
            "Training accuracy 100.0 %\n",
            "Time: 432.54032731056213\n",
            "Epoch [18/25], Step [355/607], Loss: 0.0135\n",
            "Training accuracy 100.0 %\n",
            "Time: 432.72660851478577\n",
            "Epoch [18/25], Step [360/607], Loss: 0.0564\n",
            "Training accuracy 93.75 %\n",
            "Time: 432.97301721572876\n",
            "Epoch [18/25], Step [365/607], Loss: 0.0079\n",
            "Training accuracy 100.0 %\n",
            "Time: 433.17769408226013\n",
            "Epoch [18/25], Step [370/607], Loss: 0.0014\n",
            "Training accuracy 100.0 %\n",
            "Time: 433.4005846977234\n",
            "Epoch [18/25], Step [375/607], Loss: 0.0273\n",
            "Training accuracy 100.0 %\n",
            "Time: 433.5820617675781\n",
            "Epoch [18/25], Step [380/607], Loss: 0.0024\n",
            "Training accuracy 100.0 %\n",
            "Time: 433.78139877319336\n",
            "Epoch [18/25], Step [385/607], Loss: 0.0306\n",
            "Training accuracy 100.0 %\n",
            "Time: 433.96396470069885\n",
            "Epoch [18/25], Step [390/607], Loss: 0.0071\n",
            "Training accuracy 100.0 %\n",
            "Time: 434.17304968833923\n",
            "Epoch [18/25], Step [395/607], Loss: 0.0315\n",
            "Training accuracy 100.0 %\n",
            "Time: 434.35697960853577\n",
            "Epoch [18/25], Step [400/607], Loss: 0.0292\n",
            "Training accuracy 100.0 %\n",
            "Time: 434.555330991745\n",
            "Epoch [18/25], Step [405/607], Loss: 0.0127\n",
            "Training accuracy 100.0 %\n",
            "Time: 434.7443618774414\n",
            "Epoch [18/25], Step [410/607], Loss: 0.0092\n",
            "Training accuracy 100.0 %\n",
            "Time: 434.94277143478394\n",
            "Epoch [18/25], Step [415/607], Loss: 0.0103\n",
            "Training accuracy 100.0 %\n",
            "Time: 435.14717960357666\n",
            "Epoch [18/25], Step [420/607], Loss: 0.0208\n",
            "Training accuracy 100.0 %\n",
            "Time: 435.33689546585083\n",
            "Epoch [18/25], Step [425/607], Loss: 0.0115\n",
            "Training accuracy 100.0 %\n",
            "Time: 435.54326915740967\n",
            "Epoch [18/25], Step [430/607], Loss: 0.0262\n",
            "Training accuracy 100.0 %\n",
            "Time: 435.7402229309082\n",
            "Epoch [18/25], Step [435/607], Loss: 0.0018\n",
            "Training accuracy 100.0 %\n",
            "Time: 435.9344437122345\n",
            "Epoch [18/25], Step [440/607], Loss: 0.0516\n",
            "Training accuracy 93.75 %\n",
            "Time: 436.1295864582062\n",
            "Epoch [18/25], Step [445/607], Loss: 0.0327\n",
            "Training accuracy 100.0 %\n",
            "Time: 436.34640741348267\n",
            "Epoch [18/25], Step [450/607], Loss: 0.0009\n",
            "Training accuracy 100.0 %\n",
            "Time: 436.5560042858124\n",
            "Epoch [18/25], Step [455/607], Loss: 0.0099\n",
            "Training accuracy 100.0 %\n",
            "Time: 436.7667896747589\n",
            "Epoch [18/25], Step [460/607], Loss: 0.0078\n",
            "Training accuracy 100.0 %\n",
            "Time: 436.96221351623535\n",
            "Epoch [18/25], Step [465/607], Loss: 0.0181\n",
            "Training accuracy 100.0 %\n",
            "Time: 437.1420364379883\n",
            "Epoch [18/25], Step [470/607], Loss: 0.0052\n",
            "Training accuracy 100.0 %\n",
            "Time: 437.35376024246216\n",
            "Epoch [18/25], Step [475/607], Loss: 0.0045\n",
            "Training accuracy 100.0 %\n",
            "Time: 437.56027913093567\n",
            "Epoch [18/25], Step [480/607], Loss: 0.0030\n",
            "Training accuracy 100.0 %\n",
            "Time: 437.7561266422272\n",
            "Epoch [18/25], Step [485/607], Loss: 0.0293\n",
            "Training accuracy 100.0 %\n",
            "Time: 437.9832136631012\n",
            "Epoch [18/25], Step [490/607], Loss: 0.0021\n",
            "Training accuracy 100.0 %\n",
            "Time: 438.2023069858551\n",
            "Epoch [18/25], Step [495/607], Loss: 0.0222\n",
            "Training accuracy 100.0 %\n",
            "Time: 438.39254665374756\n",
            "Epoch [18/25], Step [500/607], Loss: 0.0085\n",
            "Training accuracy 100.0 %\n",
            "Time: 438.60568857192993\n",
            "Epoch [18/25], Step [505/607], Loss: 0.0110\n",
            "Training accuracy 100.0 %\n",
            "Time: 438.78259325027466\n",
            "Epoch [18/25], Step [510/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 438.97376823425293\n",
            "Epoch [18/25], Step [515/607], Loss: 0.0858\n",
            "Training accuracy 93.75 %\n",
            "Time: 439.1743175983429\n",
            "Epoch [18/25], Step [520/607], Loss: 0.0427\n",
            "Training accuracy 100.0 %\n",
            "Time: 439.3935251235962\n",
            "Epoch [18/25], Step [525/607], Loss: 0.0128\n",
            "Training accuracy 100.0 %\n",
            "Time: 439.58591771125793\n",
            "Epoch [18/25], Step [530/607], Loss: 0.0048\n",
            "Training accuracy 100.0 %\n",
            "Time: 439.78375911712646\n",
            "Epoch [18/25], Step [535/607], Loss: 0.0037\n",
            "Training accuracy 100.0 %\n",
            "Time: 439.96119356155396\n",
            "Epoch [18/25], Step [540/607], Loss: 0.0070\n",
            "Training accuracy 100.0 %\n",
            "Time: 440.15189504623413\n",
            "Epoch [18/25], Step [545/607], Loss: 0.0020\n",
            "Training accuracy 100.0 %\n",
            "Time: 440.339058637619\n",
            "Epoch [18/25], Step [550/607], Loss: 0.0287\n",
            "Training accuracy 100.0 %\n",
            "Time: 440.55678844451904\n",
            "Epoch [18/25], Step [555/607], Loss: 0.0113\n",
            "Training accuracy 100.0 %\n",
            "Time: 440.7398507595062\n",
            "Epoch [18/25], Step [560/607], Loss: 0.0125\n",
            "Training accuracy 100.0 %\n",
            "Time: 440.95069217681885\n",
            "Epoch [18/25], Step [565/607], Loss: 0.0635\n",
            "Training accuracy 93.75 %\n",
            "Time: 441.13696670532227\n",
            "Epoch [18/25], Step [570/607], Loss: 0.0015\n",
            "Training accuracy 100.0 %\n",
            "Time: 441.3348054885864\n",
            "Epoch [18/25], Step [575/607], Loss: 0.0006\n",
            "Training accuracy 100.0 %\n",
            "Time: 441.51304173469543\n",
            "Epoch [18/25], Step [580/607], Loss: 0.0124\n",
            "Training accuracy 100.0 %\n",
            "Time: 441.71065068244934\n",
            "Epoch [18/25], Step [585/607], Loss: 0.4431\n",
            "Training accuracy 81.25 %\n",
            "Time: 441.8975577354431\n",
            "Epoch [18/25], Step [590/607], Loss: 0.0290\n",
            "Training accuracy 100.0 %\n",
            "Time: 442.1073246002197\n",
            "Epoch [18/25], Step [595/607], Loss: 0.0065\n",
            "Training accuracy 100.0 %\n",
            "Time: 442.30036187171936\n",
            "Epoch [18/25], Step [600/607], Loss: 0.0207\n",
            "Training accuracy 100.0 %\n",
            "Time: 442.5368273258209\n",
            "Epoch [18/25], Step [605/607], Loss: 0.0050\n",
            "Training accuracy 100.0 %\n",
            "Time: 442.7176356315613\n",
            "epoch 19\n",
            "Epoch [19/25], Step [5/607], Loss: 0.0040\n",
            "Training accuracy 100.0 %\n",
            "Time: 443.1532828807831\n",
            "Epoch [19/25], Step [10/607], Loss: 0.0013\n",
            "Training accuracy 100.0 %\n",
            "Time: 443.3419370651245\n",
            "Epoch [19/25], Step [15/607], Loss: 0.0045\n",
            "Training accuracy 100.0 %\n",
            "Time: 443.5444278717041\n",
            "Epoch [19/25], Step [20/607], Loss: 0.3056\n",
            "Training accuracy 87.5 %\n",
            "Time: 443.73258233070374\n",
            "Epoch [19/25], Step [25/607], Loss: 0.0075\n",
            "Training accuracy 100.0 %\n",
            "Time: 443.94819593429565\n",
            "Epoch [19/25], Step [30/607], Loss: 0.0212\n",
            "Training accuracy 100.0 %\n",
            "Time: 444.132607460022\n",
            "Epoch [19/25], Step [35/607], Loss: 0.0085\n",
            "Training accuracy 100.0 %\n",
            "Time: 444.3363366127014\n",
            "Epoch [19/25], Step [40/607], Loss: 0.1187\n",
            "Training accuracy 93.75 %\n",
            "Time: 444.522917509079\n",
            "Epoch [19/25], Step [45/607], Loss: 0.0018\n",
            "Training accuracy 100.0 %\n",
            "Time: 444.7311351299286\n",
            "Epoch [19/25], Step [50/607], Loss: 0.0365\n",
            "Training accuracy 100.0 %\n",
            "Time: 444.93042159080505\n",
            "Epoch [19/25], Step [55/607], Loss: 0.0074\n",
            "Training accuracy 100.0 %\n",
            "Time: 445.1311819553375\n",
            "Epoch [19/25], Step [60/607], Loss: 0.0045\n",
            "Training accuracy 100.0 %\n",
            "Time: 445.3252217769623\n",
            "Epoch [19/25], Step [65/607], Loss: 0.0030\n",
            "Training accuracy 100.0 %\n",
            "Time: 445.52365493774414\n",
            "Epoch [19/25], Step [70/607], Loss: 0.1452\n",
            "Training accuracy 93.75 %\n",
            "Time: 445.70579981803894\n",
            "Epoch [19/25], Step [75/607], Loss: 0.0400\n",
            "Training accuracy 100.0 %\n",
            "Time: 445.90126514434814\n",
            "Epoch [19/25], Step [80/607], Loss: 0.0312\n",
            "Training accuracy 100.0 %\n",
            "Time: 446.10402941703796\n",
            "Epoch [19/25], Step [85/607], Loss: 0.0243\n",
            "Training accuracy 100.0 %\n",
            "Time: 446.3466055393219\n",
            "Epoch [19/25], Step [90/607], Loss: 0.0335\n",
            "Training accuracy 100.0 %\n",
            "Time: 446.58324456214905\n",
            "Epoch [19/25], Step [95/607], Loss: 0.0148\n",
            "Training accuracy 100.0 %\n",
            "Time: 446.7730073928833\n",
            "Epoch [19/25], Step [100/607], Loss: 0.0122\n",
            "Training accuracy 100.0 %\n",
            "Time: 446.99266028404236\n",
            "Epoch [19/25], Step [105/607], Loss: 0.0166\n",
            "Training accuracy 100.0 %\n",
            "Time: 447.1880147457123\n",
            "Epoch [19/25], Step [110/607], Loss: 0.1240\n",
            "Training accuracy 93.75 %\n",
            "Time: 447.40717005729675\n",
            "Epoch [19/25], Step [115/607], Loss: 0.0394\n",
            "Training accuracy 100.0 %\n",
            "Time: 447.6175363063812\n",
            "Epoch [19/25], Step [120/607], Loss: 0.0028\n",
            "Training accuracy 100.0 %\n",
            "Time: 447.8033962249756\n",
            "Epoch [19/25], Step [125/607], Loss: 0.0042\n",
            "Training accuracy 100.0 %\n",
            "Time: 448.0019586086273\n",
            "Epoch [19/25], Step [130/607], Loss: 0.0285\n",
            "Training accuracy 100.0 %\n",
            "Time: 448.2003524303436\n",
            "Epoch [19/25], Step [135/607], Loss: 0.1624\n",
            "Training accuracy 93.75 %\n",
            "Time: 448.4076316356659\n",
            "Epoch [19/25], Step [140/607], Loss: 0.0112\n",
            "Training accuracy 100.0 %\n",
            "Time: 448.6003141403198\n",
            "Epoch [19/25], Step [145/607], Loss: 0.0015\n",
            "Training accuracy 100.0 %\n",
            "Time: 448.8126974105835\n",
            "Epoch [19/25], Step [150/607], Loss: 0.0039\n",
            "Training accuracy 100.0 %\n",
            "Time: 449.0004367828369\n",
            "Epoch [19/25], Step [155/607], Loss: 0.1937\n",
            "Training accuracy 93.75 %\n",
            "Time: 449.2051215171814\n",
            "Epoch [19/25], Step [160/607], Loss: 0.0602\n",
            "Training accuracy 100.0 %\n",
            "Time: 449.39598536491394\n",
            "Epoch [19/25], Step [165/607], Loss: 0.0015\n",
            "Training accuracy 100.0 %\n",
            "Time: 449.5972876548767\n",
            "Epoch [19/25], Step [170/607], Loss: 0.0225\n",
            "Training accuracy 100.0 %\n",
            "Time: 449.7910432815552\n",
            "Epoch [19/25], Step [175/607], Loss: 0.0383\n",
            "Training accuracy 100.0 %\n",
            "Time: 450.0116276741028\n",
            "Epoch [19/25], Step [180/607], Loss: 0.0032\n",
            "Training accuracy 100.0 %\n",
            "Time: 450.1989703178406\n",
            "Epoch [19/25], Step [185/607], Loss: 0.1668\n",
            "Training accuracy 93.75 %\n",
            "Time: 450.3923783302307\n",
            "Epoch [19/25], Step [190/607], Loss: 0.0889\n",
            "Training accuracy 93.75 %\n",
            "Time: 450.5904014110565\n",
            "Epoch [19/25], Step [195/607], Loss: 0.0126\n",
            "Training accuracy 100.0 %\n",
            "Time: 450.7888009548187\n",
            "Epoch [19/25], Step [200/607], Loss: 0.0438\n",
            "Training accuracy 100.0 %\n",
            "Time: 450.9857485294342\n",
            "Epoch [19/25], Step [205/607], Loss: 0.0238\n",
            "Training accuracy 100.0 %\n",
            "Time: 451.19239926338196\n",
            "Epoch [19/25], Step [210/607], Loss: 0.0037\n",
            "Training accuracy 100.0 %\n",
            "Time: 451.3925199508667\n",
            "Epoch [19/25], Step [215/607], Loss: 0.0034\n",
            "Training accuracy 100.0 %\n",
            "Time: 451.59394431114197\n",
            "Epoch [19/25], Step [220/607], Loss: 0.1368\n",
            "Training accuracy 93.75 %\n",
            "Time: 451.7848012447357\n",
            "Epoch [19/25], Step [225/607], Loss: 0.0092\n",
            "Training accuracy 100.0 %\n",
            "Time: 451.96563029289246\n",
            "Epoch [19/25], Step [230/607], Loss: 0.0153\n",
            "Training accuracy 100.0 %\n",
            "Time: 452.1800093650818\n",
            "Epoch [19/25], Step [235/607], Loss: 0.0124\n",
            "Training accuracy 100.0 %\n",
            "Time: 452.37469577789307\n",
            "Epoch [19/25], Step [240/607], Loss: 0.0258\n",
            "Training accuracy 100.0 %\n",
            "Time: 452.5967743396759\n",
            "Epoch [19/25], Step [245/607], Loss: 0.0103\n",
            "Training accuracy 100.0 %\n",
            "Time: 452.80114102363586\n",
            "Epoch [19/25], Step [250/607], Loss: 0.0384\n",
            "Training accuracy 100.0 %\n",
            "Time: 453.0259647369385\n",
            "Epoch [19/25], Step [255/607], Loss: 0.0541\n",
            "Training accuracy 100.0 %\n",
            "Time: 453.25073313713074\n",
            "Epoch [19/25], Step [260/607], Loss: 0.0381\n",
            "Training accuracy 100.0 %\n",
            "Time: 453.48282170295715\n",
            "Epoch [19/25], Step [265/607], Loss: 0.0031\n",
            "Training accuracy 100.0 %\n",
            "Time: 453.66866302490234\n",
            "Epoch [19/25], Step [270/607], Loss: 0.0178\n",
            "Training accuracy 100.0 %\n",
            "Time: 453.8791129589081\n",
            "Epoch [19/25], Step [275/607], Loss: 0.0076\n",
            "Training accuracy 100.0 %\n",
            "Time: 454.05828857421875\n",
            "Epoch [19/25], Step [280/607], Loss: 0.0039\n",
            "Training accuracy 100.0 %\n",
            "Time: 454.2474548816681\n",
            "Epoch [19/25], Step [285/607], Loss: 0.0084\n",
            "Training accuracy 100.0 %\n",
            "Time: 454.42865538597107\n",
            "Epoch [19/25], Step [290/607], Loss: 0.0086\n",
            "Training accuracy 100.0 %\n",
            "Time: 454.6467287540436\n",
            "Epoch [19/25], Step [295/607], Loss: 0.0336\n",
            "Training accuracy 100.0 %\n",
            "Time: 454.8592073917389\n",
            "Epoch [19/25], Step [300/607], Loss: 0.0057\n",
            "Training accuracy 100.0 %\n",
            "Time: 455.0710709095001\n",
            "Epoch [19/25], Step [305/607], Loss: 0.0208\n",
            "Training accuracy 100.0 %\n",
            "Time: 455.2605314254761\n",
            "Epoch [19/25], Step [310/607], Loss: 0.0048\n",
            "Training accuracy 100.0 %\n",
            "Time: 455.4840531349182\n",
            "Epoch [19/25], Step [315/607], Loss: 0.0029\n",
            "Training accuracy 100.0 %\n",
            "Time: 455.67705631256104\n",
            "Epoch [19/25], Step [320/607], Loss: 0.0103\n",
            "Training accuracy 100.0 %\n",
            "Time: 455.8944022655487\n",
            "Epoch [19/25], Step [325/607], Loss: 0.1011\n",
            "Training accuracy 93.75 %\n",
            "Time: 456.07237935066223\n",
            "Epoch [19/25], Step [330/607], Loss: 0.0244\n",
            "Training accuracy 100.0 %\n",
            "Time: 456.27879762649536\n",
            "Epoch [19/25], Step [335/607], Loss: 0.0587\n",
            "Training accuracy 100.0 %\n",
            "Time: 456.4716110229492\n",
            "Epoch [19/25], Step [340/607], Loss: 0.0494\n",
            "Training accuracy 100.0 %\n",
            "Time: 456.6535360813141\n",
            "Epoch [19/25], Step [345/607], Loss: 0.0893\n",
            "Training accuracy 93.75 %\n",
            "Time: 456.8395357131958\n",
            "Epoch [19/25], Step [350/607], Loss: 0.0215\n",
            "Training accuracy 100.0 %\n",
            "Time: 457.0801992416382\n",
            "Epoch [19/25], Step [355/607], Loss: 0.1588\n",
            "Training accuracy 93.75 %\n",
            "Time: 457.27898502349854\n",
            "Epoch [19/25], Step [360/607], Loss: 0.0213\n",
            "Training accuracy 100.0 %\n",
            "Time: 457.4921200275421\n",
            "Epoch [19/25], Step [365/607], Loss: 0.0081\n",
            "Training accuracy 100.0 %\n",
            "Time: 457.68052434921265\n",
            "Epoch [19/25], Step [370/607], Loss: 0.0288\n",
            "Training accuracy 100.0 %\n",
            "Time: 457.91096234321594\n",
            "Epoch [19/25], Step [375/607], Loss: 0.0010\n",
            "Training accuracy 100.0 %\n",
            "Time: 458.09941363334656\n",
            "Epoch [19/25], Step [380/607], Loss: 0.0290\n",
            "Training accuracy 100.0 %\n",
            "Time: 458.3210594654083\n",
            "Epoch [19/25], Step [385/607], Loss: 0.0013\n",
            "Training accuracy 100.0 %\n",
            "Time: 458.51516914367676\n",
            "Epoch [19/25], Step [390/607], Loss: 0.0409\n",
            "Training accuracy 100.0 %\n",
            "Time: 458.70698595046997\n",
            "Epoch [19/25], Step [395/607], Loss: 0.0014\n",
            "Training accuracy 100.0 %\n",
            "Time: 458.89978313446045\n",
            "Epoch [19/25], Step [400/607], Loss: 0.0188\n",
            "Training accuracy 100.0 %\n",
            "Time: 459.1105661392212\n",
            "Epoch [19/25], Step [405/607], Loss: 0.0102\n",
            "Training accuracy 100.0 %\n",
            "Time: 459.29314708709717\n",
            "Epoch [19/25], Step [410/607], Loss: 0.0059\n",
            "Training accuracy 100.0 %\n",
            "Time: 459.5265603065491\n",
            "Epoch [19/25], Step [415/607], Loss: 0.0006\n",
            "Training accuracy 100.0 %\n",
            "Time: 459.76795268058777\n",
            "Epoch [19/25], Step [420/607], Loss: 0.0106\n",
            "Training accuracy 100.0 %\n",
            "Time: 459.94679832458496\n",
            "Epoch [19/25], Step [425/607], Loss: 0.0017\n",
            "Training accuracy 100.0 %\n",
            "Time: 460.1226997375488\n",
            "Epoch [19/25], Step [430/607], Loss: 0.0050\n",
            "Training accuracy 100.0 %\n",
            "Time: 460.3329861164093\n",
            "Epoch [19/25], Step [435/607], Loss: 0.1423\n",
            "Training accuracy 93.75 %\n",
            "Time: 460.5329978466034\n",
            "Epoch [19/25], Step [440/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 460.7568440437317\n",
            "Epoch [19/25], Step [445/607], Loss: 0.0200\n",
            "Training accuracy 100.0 %\n",
            "Time: 460.9380226135254\n",
            "Epoch [19/25], Step [450/607], Loss: 0.0199\n",
            "Training accuracy 100.0 %\n",
            "Time: 461.14649391174316\n",
            "Epoch [19/25], Step [455/607], Loss: 0.0247\n",
            "Training accuracy 100.0 %\n",
            "Time: 461.31832790374756\n",
            "Epoch [19/25], Step [460/607], Loss: 0.0376\n",
            "Training accuracy 100.0 %\n",
            "Time: 461.5435619354248\n",
            "Epoch [19/25], Step [465/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 461.7175557613373\n",
            "Epoch [19/25], Step [470/607], Loss: 0.0010\n",
            "Training accuracy 100.0 %\n",
            "Time: 461.9304299354553\n",
            "Epoch [19/25], Step [475/607], Loss: 0.0487\n",
            "Training accuracy 100.0 %\n",
            "Time: 462.1269600391388\n",
            "Epoch [19/25], Step [480/607], Loss: 0.0054\n",
            "Training accuracy 100.0 %\n",
            "Time: 462.31647849082947\n",
            "Epoch [19/25], Step [485/607], Loss: 0.0384\n",
            "Training accuracy 100.0 %\n",
            "Time: 462.49967885017395\n",
            "Epoch [19/25], Step [490/607], Loss: 0.1713\n",
            "Training accuracy 93.75 %\n",
            "Time: 462.7082998752594\n",
            "Epoch [19/25], Step [495/607], Loss: 0.0198\n",
            "Training accuracy 100.0 %\n",
            "Time: 462.9063684940338\n",
            "Epoch [19/25], Step [500/607], Loss: 0.0223\n",
            "Training accuracy 100.0 %\n",
            "Time: 463.1183114051819\n",
            "Epoch [19/25], Step [505/607], Loss: 0.0012\n",
            "Training accuracy 100.0 %\n",
            "Time: 463.29001808166504\n",
            "Epoch [19/25], Step [510/607], Loss: 0.0651\n",
            "Training accuracy 93.75 %\n",
            "Time: 463.49407744407654\n",
            "Epoch [19/25], Step [515/607], Loss: 0.0059\n",
            "Training accuracy 100.0 %\n",
            "Time: 463.6751708984375\n",
            "Epoch [19/25], Step [520/607], Loss: 0.0343\n",
            "Training accuracy 100.0 %\n",
            "Time: 463.8684821128845\n",
            "Epoch [19/25], Step [525/607], Loss: 1.7330\n",
            "Training accuracy 93.75 %\n",
            "Time: 464.047242641449\n",
            "Epoch [19/25], Step [530/607], Loss: 0.0175\n",
            "Training accuracy 100.0 %\n",
            "Time: 464.27645206451416\n",
            "Epoch [19/25], Step [535/607], Loss: 0.0030\n",
            "Training accuracy 100.0 %\n",
            "Time: 464.454252243042\n",
            "Epoch [19/25], Step [540/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 464.65236258506775\n",
            "Epoch [19/25], Step [545/607], Loss: 0.0155\n",
            "Training accuracy 100.0 %\n",
            "Time: 464.8447365760803\n",
            "Epoch [19/25], Step [550/607], Loss: 0.0030\n",
            "Training accuracy 100.0 %\n",
            "Time: 465.0606904029846\n",
            "Epoch [19/25], Step [555/607], Loss: 0.0191\n",
            "Training accuracy 100.0 %\n",
            "Time: 465.24729681015015\n",
            "Epoch [19/25], Step [560/607], Loss: 0.0105\n",
            "Training accuracy 100.0 %\n",
            "Time: 465.4387972354889\n",
            "Epoch [19/25], Step [565/607], Loss: 0.0045\n",
            "Training accuracy 100.0 %\n",
            "Time: 465.61786127090454\n",
            "Epoch [19/25], Step [570/607], Loss: 0.0109\n",
            "Training accuracy 100.0 %\n",
            "Time: 465.8172881603241\n",
            "Epoch [19/25], Step [575/607], Loss: 0.1934\n",
            "Training accuracy 87.5 %\n",
            "Time: 466.00210213661194\n",
            "Epoch [19/25], Step [580/607], Loss: 0.0234\n",
            "Training accuracy 100.0 %\n",
            "Time: 466.21427512168884\n",
            "Epoch [19/25], Step [585/607], Loss: 0.0113\n",
            "Training accuracy 100.0 %\n",
            "Time: 466.39211773872375\n",
            "Epoch [19/25], Step [590/607], Loss: 0.4210\n",
            "Training accuracy 93.75 %\n",
            "Time: 466.6132891178131\n",
            "Epoch [19/25], Step [595/607], Loss: 0.0082\n",
            "Training accuracy 100.0 %\n",
            "Time: 466.80178666114807\n",
            "Epoch [19/25], Step [600/607], Loss: 0.0015\n",
            "Training accuracy 100.0 %\n",
            "Time: 467.0035538673401\n",
            "Epoch [19/25], Step [605/607], Loss: 0.0022\n",
            "Training accuracy 100.0 %\n",
            "Time: 467.19548320770264\n",
            "epoch 20\n",
            "Epoch [20/25], Step [5/607], Loss: 0.0460\n",
            "Training accuracy 93.75 %\n",
            "Time: 467.62159395217896\n",
            "Epoch [20/25], Step [10/607], Loss: 0.0071\n",
            "Training accuracy 100.0 %\n",
            "Time: 467.81235241889954\n",
            "Epoch [20/25], Step [15/607], Loss: 0.0672\n",
            "Training accuracy 100.0 %\n",
            "Time: 468.00414061546326\n",
            "Epoch [20/25], Step [20/607], Loss: 0.2438\n",
            "Training accuracy 87.5 %\n",
            "Time: 468.18966698646545\n",
            "Epoch [20/25], Step [25/607], Loss: 0.0410\n",
            "Training accuracy 100.0 %\n",
            "Time: 468.3928577899933\n",
            "Epoch [20/25], Step [30/607], Loss: 0.1179\n",
            "Training accuracy 93.75 %\n",
            "Time: 468.61200761795044\n",
            "Epoch [20/25], Step [35/607], Loss: 0.0123\n",
            "Training accuracy 100.0 %\n",
            "Time: 468.8190321922302\n",
            "Epoch [20/25], Step [40/607], Loss: 0.0248\n",
            "Training accuracy 100.0 %\n",
            "Time: 469.02014803886414\n",
            "Epoch [20/25], Step [45/607], Loss: 0.0544\n",
            "Training accuracy 100.0 %\n",
            "Time: 469.21099758148193\n",
            "Epoch [20/25], Step [50/607], Loss: 0.2344\n",
            "Training accuracy 93.75 %\n",
            "Time: 469.3998427391052\n",
            "Epoch [20/25], Step [55/607], Loss: 0.0098\n",
            "Training accuracy 100.0 %\n",
            "Time: 469.5957589149475\n",
            "Epoch [20/25], Step [60/607], Loss: 0.0116\n",
            "Training accuracy 100.0 %\n",
            "Time: 469.79266262054443\n",
            "Epoch [20/25], Step [65/607], Loss: 0.0137\n",
            "Training accuracy 100.0 %\n",
            "Time: 470.00800347328186\n",
            "Epoch [20/25], Step [70/607], Loss: 0.0066\n",
            "Training accuracy 100.0 %\n",
            "Time: 470.1952965259552\n",
            "Epoch [20/25], Step [75/607], Loss: 0.0310\n",
            "Training accuracy 100.0 %\n",
            "Time: 470.39415764808655\n",
            "Epoch [20/25], Step [80/607], Loss: 0.0044\n",
            "Training accuracy 100.0 %\n",
            "Time: 470.5835208892822\n",
            "Epoch [20/25], Step [85/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 470.7953767776489\n",
            "Epoch [20/25], Step [90/607], Loss: 0.0505\n",
            "Training accuracy 93.75 %\n",
            "Time: 470.9830117225647\n",
            "Epoch [20/25], Step [95/607], Loss: 0.0228\n",
            "Training accuracy 100.0 %\n",
            "Time: 471.19044303894043\n",
            "Epoch [20/25], Step [100/607], Loss: 0.0072\n",
            "Training accuracy 100.0 %\n",
            "Time: 471.3750410079956\n",
            "Epoch [20/25], Step [105/607], Loss: 0.0860\n",
            "Training accuracy 93.75 %\n",
            "Time: 471.56082701683044\n",
            "Epoch [20/25], Step [110/607], Loss: 0.0037\n",
            "Training accuracy 100.0 %\n",
            "Time: 471.74398374557495\n",
            "Epoch [20/25], Step [115/607], Loss: 0.0559\n",
            "Training accuracy 100.0 %\n",
            "Time: 471.9484369754791\n",
            "Epoch [20/25], Step [120/607], Loss: 0.0218\n",
            "Training accuracy 100.0 %\n",
            "Time: 472.1419520378113\n",
            "Epoch [20/25], Step [125/607], Loss: 0.0098\n",
            "Training accuracy 100.0 %\n",
            "Time: 472.3150370121002\n",
            "Epoch [20/25], Step [130/607], Loss: 0.0162\n",
            "Training accuracy 100.0 %\n",
            "Time: 472.4898772239685\n",
            "Epoch [20/25], Step [135/607], Loss: 0.0157\n",
            "Training accuracy 100.0 %\n",
            "Time: 472.68299174308777\n",
            "Epoch [20/25], Step [140/607], Loss: 0.0022\n",
            "Training accuracy 100.0 %\n",
            "Time: 472.86796045303345\n",
            "Epoch [20/25], Step [145/607], Loss: 0.1154\n",
            "Training accuracy 93.75 %\n",
            "Time: 473.0883288383484\n",
            "Epoch [20/25], Step [150/607], Loss: 0.0047\n",
            "Training accuracy 100.0 %\n",
            "Time: 473.2824983596802\n",
            "Epoch [20/25], Step [155/607], Loss: 0.2498\n",
            "Training accuracy 93.75 %\n",
            "Time: 473.4897449016571\n",
            "Epoch [20/25], Step [160/607], Loss: 0.0147\n",
            "Training accuracy 100.0 %\n",
            "Time: 473.68190836906433\n",
            "Epoch [20/25], Step [165/607], Loss: 0.0061\n",
            "Training accuracy 100.0 %\n",
            "Time: 473.8807487487793\n",
            "Epoch [20/25], Step [170/607], Loss: 0.0122\n",
            "Training accuracy 100.0 %\n",
            "Time: 474.0708122253418\n",
            "Epoch [20/25], Step [175/607], Loss: 0.0048\n",
            "Training accuracy 100.0 %\n",
            "Time: 474.257461309433\n",
            "Epoch [20/25], Step [180/607], Loss: 0.0762\n",
            "Training accuracy 93.75 %\n",
            "Time: 474.4729232788086\n",
            "Epoch [20/25], Step [185/607], Loss: 0.1219\n",
            "Training accuracy 87.5 %\n",
            "Time: 474.6622896194458\n",
            "Epoch [20/25], Step [190/607], Loss: 0.0102\n",
            "Training accuracy 100.0 %\n",
            "Time: 474.8875470161438\n",
            "Epoch [20/25], Step [195/607], Loss: 0.0124\n",
            "Training accuracy 100.0 %\n",
            "Time: 475.073917388916\n",
            "Epoch [20/25], Step [200/607], Loss: 0.0908\n",
            "Training accuracy 93.75 %\n",
            "Time: 475.2855644226074\n",
            "Epoch [20/25], Step [205/607], Loss: 0.0980\n",
            "Training accuracy 93.75 %\n",
            "Time: 475.482248544693\n",
            "Epoch [20/25], Step [210/607], Loss: 0.0029\n",
            "Training accuracy 100.0 %\n",
            "Time: 475.71159982681274\n",
            "Epoch [20/25], Step [215/607], Loss: 0.0066\n",
            "Training accuracy 100.0 %\n",
            "Time: 475.89791560173035\n",
            "Epoch [20/25], Step [220/607], Loss: 0.0004\n",
            "Training accuracy 100.0 %\n",
            "Time: 476.1128923892975\n",
            "Epoch [20/25], Step [225/607], Loss: 0.0227\n",
            "Training accuracy 100.0 %\n",
            "Time: 476.3109829425812\n",
            "Epoch [20/25], Step [230/607], Loss: 0.0224\n",
            "Training accuracy 100.0 %\n",
            "Time: 476.520357131958\n",
            "Epoch [20/25], Step [235/607], Loss: 0.0024\n",
            "Training accuracy 100.0 %\n",
            "Time: 476.7181980609894\n",
            "Epoch [20/25], Step [240/607], Loss: 0.0702\n",
            "Training accuracy 100.0 %\n",
            "Time: 476.94454884529114\n",
            "Epoch [20/25], Step [245/607], Loss: 0.0974\n",
            "Training accuracy 93.75 %\n",
            "Time: 477.13459396362305\n",
            "Epoch [20/25], Step [250/607], Loss: 0.0003\n",
            "Training accuracy 100.0 %\n",
            "Time: 477.3365955352783\n",
            "Epoch [20/25], Step [255/607], Loss: 0.0093\n",
            "Training accuracy 100.0 %\n",
            "Time: 477.518497467041\n",
            "Epoch [20/25], Step [260/607], Loss: 0.0019\n",
            "Training accuracy 100.0 %\n",
            "Time: 477.71960258483887\n",
            "Epoch [20/25], Step [265/607], Loss: 0.0447\n",
            "Training accuracy 100.0 %\n",
            "Time: 477.9136338233948\n",
            "Epoch [20/25], Step [270/607], Loss: 0.0045\n",
            "Training accuracy 100.0 %\n",
            "Time: 478.13713121414185\n",
            "Epoch [20/25], Step [275/607], Loss: 0.0251\n",
            "Training accuracy 100.0 %\n",
            "Time: 478.33622694015503\n",
            "Epoch [20/25], Step [280/607], Loss: 0.0347\n",
            "Training accuracy 100.0 %\n",
            "Time: 478.55281472206116\n",
            "Epoch [20/25], Step [285/607], Loss: 0.0082\n",
            "Training accuracy 100.0 %\n",
            "Time: 478.76759815216064\n",
            "Epoch [20/25], Step [290/607], Loss: 0.0033\n",
            "Training accuracy 100.0 %\n",
            "Time: 478.9782476425171\n",
            "Epoch [20/25], Step [295/607], Loss: 0.0020\n",
            "Training accuracy 100.0 %\n",
            "Time: 479.1774685382843\n",
            "Epoch [20/25], Step [300/607], Loss: 0.0095\n",
            "Training accuracy 100.0 %\n",
            "Time: 479.3877329826355\n",
            "Epoch [20/25], Step [305/607], Loss: 0.0218\n",
            "Training accuracy 100.0 %\n",
            "Time: 479.5988667011261\n",
            "Epoch [20/25], Step [310/607], Loss: 0.0066\n",
            "Training accuracy 100.0 %\n",
            "Time: 479.81012773513794\n",
            "Epoch [20/25], Step [315/607], Loss: 0.0150\n",
            "Training accuracy 100.0 %\n",
            "Time: 479.99793696403503\n",
            "Epoch [20/25], Step [320/607], Loss: 0.0080\n",
            "Training accuracy 100.0 %\n",
            "Time: 480.1745731830597\n",
            "Epoch [20/25], Step [325/607], Loss: 0.0150\n",
            "Training accuracy 100.0 %\n",
            "Time: 480.34995198249817\n",
            "Epoch [20/25], Step [330/607], Loss: 0.0286\n",
            "Training accuracy 100.0 %\n",
            "Time: 480.57486033439636\n",
            "Epoch [20/25], Step [335/607], Loss: 0.1127\n",
            "Training accuracy 93.75 %\n",
            "Time: 480.75827050209045\n",
            "Epoch [20/25], Step [340/607], Loss: 0.0203\n",
            "Training accuracy 100.0 %\n",
            "Time: 480.97231674194336\n",
            "Epoch [20/25], Step [345/607], Loss: 0.0028\n",
            "Training accuracy 100.0 %\n",
            "Time: 481.1598057746887\n",
            "Epoch [20/25], Step [350/607], Loss: 0.0070\n",
            "Training accuracy 100.0 %\n",
            "Time: 481.37347197532654\n",
            "Epoch [20/25], Step [355/607], Loss: 0.0256\n",
            "Training accuracy 100.0 %\n",
            "Time: 481.57266330718994\n",
            "Epoch [20/25], Step [360/607], Loss: 0.0040\n",
            "Training accuracy 100.0 %\n",
            "Time: 481.79922747612\n",
            "Epoch [20/25], Step [365/607], Loss: 0.0026\n",
            "Training accuracy 100.0 %\n",
            "Time: 481.99017000198364\n",
            "Epoch [20/25], Step [370/607], Loss: 0.0097\n",
            "Training accuracy 100.0 %\n",
            "Time: 482.2199196815491\n",
            "Epoch [20/25], Step [375/607], Loss: 0.0006\n",
            "Training accuracy 100.0 %\n",
            "Time: 482.41336965560913\n",
            "Epoch [20/25], Step [380/607], Loss: 0.0231\n",
            "Training accuracy 100.0 %\n",
            "Time: 482.6044445037842\n",
            "Epoch [20/25], Step [385/607], Loss: 0.0605\n",
            "Training accuracy 100.0 %\n",
            "Time: 482.7854468822479\n",
            "Epoch [20/25], Step [390/607], Loss: 0.3167\n",
            "Training accuracy 93.75 %\n",
            "Time: 482.96732473373413\n",
            "Epoch [20/25], Step [395/607], Loss: 0.0031\n",
            "Training accuracy 100.0 %\n",
            "Time: 483.15074348449707\n",
            "Epoch [20/25], Step [400/607], Loss: 0.0143\n",
            "Training accuracy 100.0 %\n",
            "Time: 483.33732748031616\n",
            "Epoch [20/25], Step [405/607], Loss: 0.0008\n",
            "Training accuracy 100.0 %\n",
            "Time: 483.52303075790405\n",
            "Epoch [20/25], Step [410/607], Loss: 0.0208\n",
            "Training accuracy 100.0 %\n",
            "Time: 483.7351050376892\n",
            "Epoch [20/25], Step [415/607], Loss: 0.1018\n",
            "Training accuracy 93.75 %\n",
            "Time: 483.96833872795105\n",
            "Epoch [20/25], Step [420/607], Loss: 0.0078\n",
            "Training accuracy 100.0 %\n",
            "Time: 484.16455078125\n",
            "Epoch [20/25], Step [425/607], Loss: 0.0083\n",
            "Training accuracy 100.0 %\n",
            "Time: 484.368506193161\n",
            "Epoch [20/25], Step [430/607], Loss: 0.0439\n",
            "Training accuracy 100.0 %\n",
            "Time: 484.56438279151917\n",
            "Epoch [20/25], Step [435/607], Loss: 0.4500\n",
            "Training accuracy 93.75 %\n",
            "Time: 484.7892816066742\n",
            "Epoch [20/25], Step [440/607], Loss: 0.0265\n",
            "Training accuracy 100.0 %\n",
            "Time: 484.97721886634827\n",
            "Epoch [20/25], Step [445/607], Loss: 0.0022\n",
            "Training accuracy 100.0 %\n",
            "Time: 485.16081714630127\n",
            "Epoch [20/25], Step [450/607], Loss: 0.0029\n",
            "Training accuracy 100.0 %\n",
            "Time: 485.3380410671234\n",
            "Epoch [20/25], Step [455/607], Loss: 0.1314\n",
            "Training accuracy 93.75 %\n",
            "Time: 485.5640068054199\n",
            "Epoch [20/25], Step [460/607], Loss: 0.0123\n",
            "Training accuracy 100.0 %\n",
            "Time: 485.74609661102295\n",
            "Epoch [20/25], Step [465/607], Loss: 0.0126\n",
            "Training accuracy 100.0 %\n",
            "Time: 485.96939730644226\n",
            "Epoch [20/25], Step [470/607], Loss: 0.0193\n",
            "Training accuracy 100.0 %\n",
            "Time: 486.1422324180603\n",
            "Epoch [20/25], Step [475/607], Loss: 0.0246\n",
            "Training accuracy 100.0 %\n",
            "Time: 486.34107065200806\n",
            "Epoch [20/25], Step [480/607], Loss: 0.6698\n",
            "Training accuracy 93.75 %\n",
            "Time: 486.5616948604584\n",
            "Epoch [20/25], Step [485/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 486.7754180431366\n",
            "Epoch [20/25], Step [490/607], Loss: 0.0074\n",
            "Training accuracy 100.0 %\n",
            "Time: 487.0272305011749\n",
            "Epoch [20/25], Step [495/607], Loss: 0.1313\n",
            "Training accuracy 93.75 %\n",
            "Time: 487.2232666015625\n",
            "Epoch [20/25], Step [500/607], Loss: 0.3125\n",
            "Training accuracy 93.75 %\n",
            "Time: 487.4345352649689\n",
            "Epoch [20/25], Step [505/607], Loss: 0.0108\n",
            "Training accuracy 100.0 %\n",
            "Time: 487.6101903915405\n",
            "Epoch [20/25], Step [510/607], Loss: 0.0134\n",
            "Training accuracy 100.0 %\n",
            "Time: 487.82081627845764\n",
            "Epoch [20/25], Step [515/607], Loss: 0.0023\n",
            "Training accuracy 100.0 %\n",
            "Time: 488.02167320251465\n",
            "Epoch [20/25], Step [520/607], Loss: 0.0135\n",
            "Training accuracy 100.0 %\n",
            "Time: 488.2509319782257\n",
            "Epoch [20/25], Step [525/607], Loss: 0.0122\n",
            "Training accuracy 100.0 %\n",
            "Time: 488.4477689266205\n",
            "Epoch [20/25], Step [530/607], Loss: 0.0017\n",
            "Training accuracy 100.0 %\n",
            "Time: 488.64979362487793\n",
            "Epoch [20/25], Step [535/607], Loss: 0.0041\n",
            "Training accuracy 100.0 %\n",
            "Time: 488.8640613555908\n",
            "Epoch [20/25], Step [540/607], Loss: 0.0278\n",
            "Training accuracy 100.0 %\n",
            "Time: 489.0876109600067\n",
            "Epoch [20/25], Step [545/607], Loss: 0.1791\n",
            "Training accuracy 93.75 %\n",
            "Time: 489.2961537837982\n",
            "Epoch [20/25], Step [550/607], Loss: 0.0041\n",
            "Training accuracy 100.0 %\n",
            "Time: 489.50214862823486\n",
            "Epoch [20/25], Step [555/607], Loss: 0.0586\n",
            "Training accuracy 93.75 %\n",
            "Time: 489.72063541412354\n",
            "Epoch [20/25], Step [560/607], Loss: 0.0126\n",
            "Training accuracy 100.0 %\n",
            "Time: 489.93023777008057\n",
            "Epoch [20/25], Step [565/607], Loss: 0.0039\n",
            "Training accuracy 100.0 %\n",
            "Time: 490.13640308380127\n",
            "Epoch [20/25], Step [570/607], Loss: 0.0107\n",
            "Training accuracy 100.0 %\n",
            "Time: 490.35572147369385\n",
            "Epoch [20/25], Step [575/607], Loss: 0.0032\n",
            "Training accuracy 100.0 %\n",
            "Time: 490.5552279949188\n",
            "Epoch [20/25], Step [580/607], Loss: 0.0489\n",
            "Training accuracy 100.0 %\n",
            "Time: 490.7684557437897\n",
            "Epoch [20/25], Step [585/607], Loss: 0.0099\n",
            "Training accuracy 100.0 %\n",
            "Time: 490.967342376709\n",
            "Epoch [20/25], Step [590/607], Loss: 0.0017\n",
            "Training accuracy 100.0 %\n",
            "Time: 491.18315076828003\n",
            "Epoch [20/25], Step [595/607], Loss: 0.0047\n",
            "Training accuracy 100.0 %\n",
            "Time: 491.37208342552185\n",
            "Epoch [20/25], Step [600/607], Loss: 0.0943\n",
            "Training accuracy 93.75 %\n",
            "Time: 491.5875244140625\n",
            "Epoch [20/25], Step [605/607], Loss: 0.0262\n",
            "Training accuracy 100.0 %\n",
            "Time: 491.7917101383209\n",
            "epoch 21\n",
            "Epoch [21/25], Step [5/607], Loss: 0.0302\n",
            "Training accuracy 100.0 %\n",
            "Time: 492.2223410606384\n",
            "Epoch [21/25], Step [10/607], Loss: 0.1050\n",
            "Training accuracy 93.75 %\n",
            "Time: 492.4191417694092\n",
            "Epoch [21/25], Step [15/607], Loss: 0.0008\n",
            "Training accuracy 100.0 %\n",
            "Time: 492.6420576572418\n",
            "Epoch [21/25], Step [20/607], Loss: 0.0447\n",
            "Training accuracy 100.0 %\n",
            "Time: 492.83419823646545\n",
            "Epoch [21/25], Step [25/607], Loss: 0.0230\n",
            "Training accuracy 100.0 %\n",
            "Time: 493.0538685321808\n",
            "Epoch [21/25], Step [30/607], Loss: 0.0131\n",
            "Training accuracy 100.0 %\n",
            "Time: 493.2473928928375\n",
            "Epoch [21/25], Step [35/607], Loss: 0.0733\n",
            "Training accuracy 93.75 %\n",
            "Time: 493.4772560596466\n",
            "Epoch [21/25], Step [40/607], Loss: 0.0236\n",
            "Training accuracy 100.0 %\n",
            "Time: 493.6634888648987\n",
            "Epoch [21/25], Step [45/607], Loss: 0.1419\n",
            "Training accuracy 93.75 %\n",
            "Time: 493.8520812988281\n",
            "Epoch [21/25], Step [50/607], Loss: 0.0074\n",
            "Training accuracy 100.0 %\n",
            "Time: 494.0594313144684\n",
            "Epoch [21/25], Step [55/607], Loss: 0.0077\n",
            "Training accuracy 100.0 %\n",
            "Time: 494.2707073688507\n",
            "Epoch [21/25], Step [60/607], Loss: 0.0154\n",
            "Training accuracy 100.0 %\n",
            "Time: 494.45839047431946\n",
            "Epoch [21/25], Step [65/607], Loss: 0.0314\n",
            "Training accuracy 100.0 %\n",
            "Time: 494.66094946861267\n",
            "Epoch [21/25], Step [70/607], Loss: 0.0008\n",
            "Training accuracy 100.0 %\n",
            "Time: 494.836079120636\n",
            "Epoch [21/25], Step [75/607], Loss: 0.0129\n",
            "Training accuracy 100.0 %\n",
            "Time: 495.032910823822\n",
            "Epoch [21/25], Step [80/607], Loss: 0.0161\n",
            "Training accuracy 100.0 %\n",
            "Time: 495.2523126602173\n",
            "Epoch [21/25], Step [85/607], Loss: 0.0143\n",
            "Training accuracy 100.0 %\n",
            "Time: 495.4620816707611\n",
            "Epoch [21/25], Step [90/607], Loss: 0.0009\n",
            "Training accuracy 100.0 %\n",
            "Time: 495.65417861938477\n",
            "Epoch [21/25], Step [95/607], Loss: 0.0046\n",
            "Training accuracy 100.0 %\n",
            "Time: 495.84696674346924\n",
            "Epoch [21/25], Step [100/607], Loss: 0.0008\n",
            "Training accuracy 100.0 %\n",
            "Time: 496.0474519729614\n",
            "Epoch [21/25], Step [105/607], Loss: 0.0007\n",
            "Training accuracy 100.0 %\n",
            "Time: 496.25175976753235\n",
            "Epoch [21/25], Step [110/607], Loss: 0.0043\n",
            "Training accuracy 100.0 %\n",
            "Time: 496.48883628845215\n",
            "Epoch [21/25], Step [115/607], Loss: 0.0125\n",
            "Training accuracy 100.0 %\n",
            "Time: 496.67405343055725\n",
            "Epoch [21/25], Step [120/607], Loss: 0.0070\n",
            "Training accuracy 100.0 %\n",
            "Time: 496.8784832954407\n",
            "Epoch [21/25], Step [125/607], Loss: 0.0173\n",
            "Training accuracy 100.0 %\n",
            "Time: 497.06201910972595\n",
            "Epoch [21/25], Step [130/607], Loss: 0.0040\n",
            "Training accuracy 100.0 %\n",
            "Time: 497.2659544944763\n",
            "Epoch [21/25], Step [135/607], Loss: 0.0165\n",
            "Training accuracy 100.0 %\n",
            "Time: 497.4514493942261\n",
            "Epoch [21/25], Step [140/607], Loss: 0.0127\n",
            "Training accuracy 100.0 %\n",
            "Time: 497.6685848236084\n",
            "Epoch [21/25], Step [145/607], Loss: 0.0032\n",
            "Training accuracy 100.0 %\n",
            "Time: 497.8394389152527\n",
            "Epoch [21/25], Step [150/607], Loss: 0.0014\n",
            "Training accuracy 100.0 %\n",
            "Time: 498.04005789756775\n",
            "Epoch [21/25], Step [155/607], Loss: 0.0114\n",
            "Training accuracy 100.0 %\n",
            "Time: 498.23235630989075\n",
            "Epoch [21/25], Step [160/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 498.45568323135376\n",
            "Epoch [21/25], Step [165/607], Loss: 0.0503\n",
            "Training accuracy 93.75 %\n",
            "Time: 498.6545066833496\n",
            "Epoch [21/25], Step [170/607], Loss: 0.0254\n",
            "Training accuracy 100.0 %\n",
            "Time: 498.8407747745514\n",
            "Epoch [21/25], Step [175/607], Loss: 0.0147\n",
            "Training accuracy 100.0 %\n",
            "Time: 499.03850626945496\n",
            "Epoch [21/25], Step [180/607], Loss: 0.0075\n",
            "Training accuracy 100.0 %\n",
            "Time: 499.27227759361267\n",
            "Epoch [21/25], Step [185/607], Loss: 0.0175\n",
            "Training accuracy 100.0 %\n",
            "Time: 499.45850443840027\n",
            "Epoch [21/25], Step [190/607], Loss: 0.0276\n",
            "Training accuracy 100.0 %\n",
            "Time: 499.6618115901947\n",
            "Epoch [21/25], Step [195/607], Loss: 0.0089\n",
            "Training accuracy 100.0 %\n",
            "Time: 499.8550717830658\n",
            "Epoch [21/25], Step [200/607], Loss: 0.0209\n",
            "Training accuracy 100.0 %\n",
            "Time: 500.07765555381775\n",
            "Epoch [21/25], Step [205/607], Loss: 0.0006\n",
            "Training accuracy 100.0 %\n",
            "Time: 500.2534017562866\n",
            "Epoch [21/25], Step [210/607], Loss: 0.0347\n",
            "Training accuracy 100.0 %\n",
            "Time: 500.4586503505707\n",
            "Epoch [21/25], Step [215/607], Loss: 0.1189\n",
            "Training accuracy 93.75 %\n",
            "Time: 500.64288210868835\n",
            "Epoch [21/25], Step [220/607], Loss: 0.0084\n",
            "Training accuracy 100.0 %\n",
            "Time: 500.82376194000244\n",
            "Epoch [21/25], Step [225/607], Loss: 0.0098\n",
            "Training accuracy 100.0 %\n",
            "Time: 501.00730562210083\n",
            "Epoch [21/25], Step [230/607], Loss: 0.2373\n",
            "Training accuracy 93.75 %\n",
            "Time: 501.21140122413635\n",
            "Epoch [21/25], Step [235/607], Loss: 0.0069\n",
            "Training accuracy 100.0 %\n",
            "Time: 501.4178092479706\n",
            "Epoch [21/25], Step [240/607], Loss: 0.0072\n",
            "Training accuracy 100.0 %\n",
            "Time: 501.6260392665863\n",
            "Epoch [21/25], Step [245/607], Loss: 0.0117\n",
            "Training accuracy 100.0 %\n",
            "Time: 501.8285975456238\n",
            "Epoch [21/25], Step [250/607], Loss: 0.5404\n",
            "Training accuracy 93.75 %\n",
            "Time: 502.0454943180084\n",
            "Epoch [21/25], Step [255/607], Loss: 0.0017\n",
            "Training accuracy 100.0 %\n",
            "Time: 502.24927830696106\n",
            "Epoch [21/25], Step [260/607], Loss: 0.0186\n",
            "Training accuracy 100.0 %\n",
            "Time: 502.4384391307831\n",
            "Epoch [21/25], Step [265/607], Loss: 0.0057\n",
            "Training accuracy 100.0 %\n",
            "Time: 502.63763976097107\n",
            "Epoch [21/25], Step [270/607], Loss: 0.0518\n",
            "Training accuracy 100.0 %\n",
            "Time: 502.83420419692993\n",
            "Epoch [21/25], Step [275/607], Loss: 0.0110\n",
            "Training accuracy 100.0 %\n",
            "Time: 503.047372341156\n",
            "Epoch [21/25], Step [280/607], Loss: 0.0303\n",
            "Training accuracy 100.0 %\n",
            "Time: 503.2676351070404\n",
            "Epoch [21/25], Step [285/607], Loss: 0.0305\n",
            "Training accuracy 100.0 %\n",
            "Time: 503.45915508270264\n",
            "Epoch [21/25], Step [290/607], Loss: 0.0625\n",
            "Training accuracy 93.75 %\n",
            "Time: 503.68765592575073\n",
            "Epoch [21/25], Step [295/607], Loss: 0.0158\n",
            "Training accuracy 100.0 %\n",
            "Time: 503.8666217327118\n",
            "Epoch [21/25], Step [300/607], Loss: 0.0080\n",
            "Training accuracy 100.0 %\n",
            "Time: 504.09124207496643\n",
            "Epoch [21/25], Step [305/607], Loss: 0.0036\n",
            "Training accuracy 100.0 %\n",
            "Time: 504.2853012084961\n",
            "Epoch [21/25], Step [310/607], Loss: 0.0613\n",
            "Training accuracy 93.75 %\n",
            "Time: 504.50548934936523\n",
            "Epoch [21/25], Step [315/607], Loss: 0.0031\n",
            "Training accuracy 100.0 %\n",
            "Time: 504.69860887527466\n",
            "Epoch [21/25], Step [320/607], Loss: 0.0101\n",
            "Training accuracy 100.0 %\n",
            "Time: 504.89748549461365\n",
            "Epoch [21/25], Step [325/607], Loss: 0.0137\n",
            "Training accuracy 100.0 %\n",
            "Time: 505.07850337028503\n",
            "Epoch [21/25], Step [330/607], Loss: 0.0159\n",
            "Training accuracy 100.0 %\n",
            "Time: 505.2621283531189\n",
            "Epoch [21/25], Step [335/607], Loss: 0.0147\n",
            "Training accuracy 100.0 %\n",
            "Time: 505.4335072040558\n",
            "Epoch [21/25], Step [340/607], Loss: 0.1777\n",
            "Training accuracy 93.75 %\n",
            "Time: 505.613760471344\n",
            "Epoch [21/25], Step [345/607], Loss: 0.0126\n",
            "Training accuracy 100.0 %\n",
            "Time: 505.78931522369385\n",
            "Epoch [21/25], Step [350/607], Loss: 0.0272\n",
            "Training accuracy 100.0 %\n",
            "Time: 505.97932958602905\n",
            "Epoch [21/25], Step [355/607], Loss: 0.0081\n",
            "Training accuracy 100.0 %\n",
            "Time: 506.1982662677765\n",
            "Epoch [21/25], Step [360/607], Loss: 0.0079\n",
            "Training accuracy 100.0 %\n",
            "Time: 506.41470980644226\n",
            "Epoch [21/25], Step [365/607], Loss: 0.0114\n",
            "Training accuracy 100.0 %\n",
            "Time: 506.62706995010376\n",
            "Epoch [21/25], Step [370/607], Loss: 0.0030\n",
            "Training accuracy 100.0 %\n",
            "Time: 506.82188296318054\n",
            "Epoch [21/25], Step [375/607], Loss: 0.0440\n",
            "Training accuracy 100.0 %\n",
            "Time: 507.02611207962036\n",
            "Epoch [21/25], Step [380/607], Loss: 0.0074\n",
            "Training accuracy 100.0 %\n",
            "Time: 507.2310001850128\n",
            "Epoch [21/25], Step [385/607], Loss: 0.0047\n",
            "Training accuracy 100.0 %\n",
            "Time: 507.4327218532562\n",
            "Epoch [21/25], Step [390/607], Loss: 0.0234\n",
            "Training accuracy 100.0 %\n",
            "Time: 507.6189794540405\n",
            "Epoch [21/25], Step [395/607], Loss: 0.0020\n",
            "Training accuracy 100.0 %\n",
            "Time: 507.8145503997803\n",
            "Epoch [21/25], Step [400/607], Loss: 0.0010\n",
            "Training accuracy 100.0 %\n",
            "Time: 508.01190853118896\n",
            "Epoch [21/25], Step [405/607], Loss: 0.0034\n",
            "Training accuracy 100.0 %\n",
            "Time: 508.20758509635925\n",
            "Epoch [21/25], Step [410/607], Loss: 0.0011\n",
            "Training accuracy 100.0 %\n",
            "Time: 508.438444852829\n",
            "Epoch [21/25], Step [415/607], Loss: 0.0858\n",
            "Training accuracy 93.75 %\n",
            "Time: 508.63323068618774\n",
            "Epoch [21/25], Step [420/607], Loss: 0.0099\n",
            "Training accuracy 100.0 %\n",
            "Time: 508.8516824245453\n",
            "Epoch [21/25], Step [425/607], Loss: 0.0038\n",
            "Training accuracy 100.0 %\n",
            "Time: 509.03003764152527\n",
            "Epoch [21/25], Step [430/607], Loss: 0.0056\n",
            "Training accuracy 100.0 %\n",
            "Time: 509.2582151889801\n",
            "Epoch [21/25], Step [435/607], Loss: 0.0220\n",
            "Training accuracy 100.0 %\n",
            "Time: 509.4504044055939\n",
            "Epoch [21/25], Step [440/607], Loss: 0.0030\n",
            "Training accuracy 100.0 %\n",
            "Time: 509.65660429000854\n",
            "Epoch [21/25], Step [445/607], Loss: 0.0119\n",
            "Training accuracy 100.0 %\n",
            "Time: 509.8451290130615\n",
            "Epoch [21/25], Step [450/607], Loss: 0.0036\n",
            "Training accuracy 100.0 %\n",
            "Time: 510.02613973617554\n",
            "Epoch [21/25], Step [455/607], Loss: 0.0088\n",
            "Training accuracy 100.0 %\n",
            "Time: 510.2277979850769\n",
            "Epoch [21/25], Step [460/607], Loss: 0.0021\n",
            "Training accuracy 100.0 %\n",
            "Time: 510.45107078552246\n",
            "Epoch [21/25], Step [465/607], Loss: 0.0108\n",
            "Training accuracy 100.0 %\n",
            "Time: 510.64012336730957\n",
            "Epoch [21/25], Step [470/607], Loss: 0.0481\n",
            "Training accuracy 100.0 %\n",
            "Time: 510.841299533844\n",
            "Epoch [21/25], Step [475/607], Loss: 0.0194\n",
            "Training accuracy 100.0 %\n",
            "Time: 511.0348126888275\n",
            "Epoch [21/25], Step [480/607], Loss: 0.0004\n",
            "Training accuracy 100.0 %\n",
            "Time: 511.26888394355774\n",
            "Epoch [21/25], Step [485/607], Loss: 0.0076\n",
            "Training accuracy 100.0 %\n",
            "Time: 511.4602720737457\n",
            "Epoch [21/25], Step [490/607], Loss: 0.1306\n",
            "Training accuracy 93.75 %\n",
            "Time: 511.6757457256317\n",
            "Epoch [21/25], Step [495/607], Loss: 0.1633\n",
            "Training accuracy 93.75 %\n",
            "Time: 511.8457841873169\n",
            "Epoch [21/25], Step [500/607], Loss: 0.0161\n",
            "Training accuracy 100.0 %\n",
            "Time: 512.0538568496704\n",
            "Epoch [21/25], Step [505/607], Loss: 0.0022\n",
            "Training accuracy 100.0 %\n",
            "Time: 512.2585339546204\n",
            "Epoch [21/25], Step [510/607], Loss: 0.0091\n",
            "Training accuracy 100.0 %\n",
            "Time: 512.5006492137909\n",
            "Epoch [21/25], Step [515/607], Loss: 0.0046\n",
            "Training accuracy 100.0 %\n",
            "Time: 512.690646648407\n",
            "Epoch [21/25], Step [520/607], Loss: 0.0136\n",
            "Training accuracy 100.0 %\n",
            "Time: 512.8899414539337\n",
            "Epoch [21/25], Step [525/607], Loss: 0.0073\n",
            "Training accuracy 100.0 %\n",
            "Time: 513.0866115093231\n",
            "Epoch [21/25], Step [530/607], Loss: 0.0067\n",
            "Training accuracy 100.0 %\n",
            "Time: 513.31316614151\n",
            "Epoch [21/25], Step [535/607], Loss: 0.0102\n",
            "Training accuracy 100.0 %\n",
            "Time: 513.50172996521\n",
            "Epoch [21/25], Step [540/607], Loss: 0.0006\n",
            "Training accuracy 100.0 %\n",
            "Time: 513.6861023902893\n",
            "Epoch [21/25], Step [545/607], Loss: 0.0264\n",
            "Training accuracy 100.0 %\n",
            "Time: 513.8591349124908\n",
            "Epoch [21/25], Step [550/607], Loss: 0.0064\n",
            "Training accuracy 100.0 %\n",
            "Time: 514.0402054786682\n",
            "Epoch [21/25], Step [555/607], Loss: 0.0123\n",
            "Training accuracy 100.0 %\n",
            "Time: 514.2275688648224\n",
            "Epoch [21/25], Step [560/607], Loss: 0.0018\n",
            "Training accuracy 100.0 %\n",
            "Time: 514.4394714832306\n",
            "Epoch [21/25], Step [565/607], Loss: 0.0076\n",
            "Training accuracy 100.0 %\n",
            "Time: 514.6512730121613\n",
            "Epoch [21/25], Step [570/607], Loss: 0.0051\n",
            "Training accuracy 100.0 %\n",
            "Time: 514.8384962081909\n",
            "Epoch [21/25], Step [575/607], Loss: 0.0065\n",
            "Training accuracy 100.0 %\n",
            "Time: 515.0250515937805\n",
            "Epoch [21/25], Step [580/607], Loss: 0.0061\n",
            "Training accuracy 100.0 %\n",
            "Time: 515.2087259292603\n",
            "Epoch [21/25], Step [585/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 515.4150443077087\n",
            "Epoch [21/25], Step [590/607], Loss: 0.0069\n",
            "Training accuracy 100.0 %\n",
            "Time: 515.6013760566711\n",
            "Epoch [21/25], Step [595/607], Loss: 0.0302\n",
            "Training accuracy 100.0 %\n",
            "Time: 515.7944374084473\n",
            "Epoch [21/25], Step [600/607], Loss: 0.0463\n",
            "Training accuracy 100.0 %\n",
            "Time: 515.9898738861084\n",
            "Epoch [21/25], Step [605/607], Loss: 0.0226\n",
            "Training accuracy 100.0 %\n",
            "Time: 516.190318107605\n",
            "epoch 22\n",
            "Epoch [22/25], Step [5/607], Loss: 0.0022\n",
            "Training accuracy 100.0 %\n",
            "Time: 516.6025309562683\n",
            "Epoch [22/25], Step [10/607], Loss: 0.0392\n",
            "Training accuracy 100.0 %\n",
            "Time: 516.785501241684\n",
            "Epoch [22/25], Step [15/607], Loss: 0.0087\n",
            "Training accuracy 100.0 %\n",
            "Time: 516.9925632476807\n",
            "Epoch [22/25], Step [20/607], Loss: 0.0136\n",
            "Training accuracy 100.0 %\n",
            "Time: 517.197749376297\n",
            "Epoch [22/25], Step [25/607], Loss: 0.0674\n",
            "Training accuracy 93.75 %\n",
            "Time: 517.4151215553284\n",
            "Epoch [22/25], Step [30/607], Loss: 0.0026\n",
            "Training accuracy 100.0 %\n",
            "Time: 517.6156618595123\n",
            "Epoch [22/25], Step [35/607], Loss: 0.0655\n",
            "Training accuracy 100.0 %\n",
            "Time: 517.8113808631897\n",
            "Epoch [22/25], Step [40/607], Loss: 0.0370\n",
            "Training accuracy 100.0 %\n",
            "Time: 518.0038478374481\n",
            "Epoch [22/25], Step [45/607], Loss: 0.0386\n",
            "Training accuracy 100.0 %\n",
            "Time: 518.1866276264191\n",
            "Epoch [22/25], Step [50/607], Loss: 0.0016\n",
            "Training accuracy 100.0 %\n",
            "Time: 518.3884654045105\n",
            "Epoch [22/25], Step [55/607], Loss: 0.0065\n",
            "Training accuracy 100.0 %\n",
            "Time: 518.5955455303192\n",
            "Epoch [22/25], Step [60/607], Loss: 0.0321\n",
            "Training accuracy 100.0 %\n",
            "Time: 518.792916059494\n",
            "Epoch [22/25], Step [65/607], Loss: 0.0544\n",
            "Training accuracy 100.0 %\n",
            "Time: 519.0215277671814\n",
            "Epoch [22/25], Step [70/607], Loss: 0.0333\n",
            "Training accuracy 100.0 %\n",
            "Time: 519.2047724723816\n",
            "Epoch [22/25], Step [75/607], Loss: 0.0052\n",
            "Training accuracy 100.0 %\n",
            "Time: 519.4432346820831\n",
            "Epoch [22/25], Step [80/607], Loss: 0.0095\n",
            "Training accuracy 100.0 %\n",
            "Time: 519.6316511631012\n",
            "Epoch [22/25], Step [85/607], Loss: 0.0062\n",
            "Training accuracy 100.0 %\n",
            "Time: 519.8216092586517\n",
            "Epoch [22/25], Step [90/607], Loss: 0.0473\n",
            "Training accuracy 100.0 %\n",
            "Time: 520.0167253017426\n",
            "Epoch [22/25], Step [95/607], Loss: 0.0151\n",
            "Training accuracy 100.0 %\n",
            "Time: 520.2279431819916\n",
            "Epoch [22/25], Step [100/607], Loss: 0.0277\n",
            "Training accuracy 100.0 %\n",
            "Time: 520.4216129779816\n",
            "Epoch [22/25], Step [105/607], Loss: 0.0071\n",
            "Training accuracy 100.0 %\n",
            "Time: 520.6231479644775\n",
            "Epoch [22/25], Step [110/607], Loss: 0.0336\n",
            "Training accuracy 100.0 %\n",
            "Time: 520.8050606250763\n",
            "Epoch [22/25], Step [115/607], Loss: 0.0010\n",
            "Training accuracy 100.0 %\n",
            "Time: 521.0005052089691\n",
            "Epoch [22/25], Step [120/607], Loss: 0.0040\n",
            "Training accuracy 100.0 %\n",
            "Time: 521.1817262172699\n",
            "Epoch [22/25], Step [125/607], Loss: 0.4198\n",
            "Training accuracy 87.5 %\n",
            "Time: 521.3999183177948\n",
            "Epoch [22/25], Step [130/607], Loss: 0.0272\n",
            "Training accuracy 100.0 %\n",
            "Time: 521.5824148654938\n",
            "Epoch [22/25], Step [135/607], Loss: 0.0408\n",
            "Training accuracy 100.0 %\n",
            "Time: 521.7899658679962\n",
            "Epoch [22/25], Step [140/607], Loss: 0.0367\n",
            "Training accuracy 100.0 %\n",
            "Time: 521.9666316509247\n",
            "Epoch [22/25], Step [145/607], Loss: 0.0281\n",
            "Training accuracy 100.0 %\n",
            "Time: 522.1456360816956\n",
            "Epoch [22/25], Step [150/607], Loss: 0.0094\n",
            "Training accuracy 100.0 %\n",
            "Time: 522.3441886901855\n",
            "Epoch [22/25], Step [155/607], Loss: 0.0017\n",
            "Training accuracy 100.0 %\n",
            "Time: 522.5468666553497\n",
            "Epoch [22/25], Step [160/607], Loss: 0.0172\n",
            "Training accuracy 100.0 %\n",
            "Time: 522.7429687976837\n",
            "Epoch [22/25], Step [165/607], Loss: 0.1939\n",
            "Training accuracy 93.75 %\n",
            "Time: 522.9424564838409\n",
            "Epoch [22/25], Step [170/607], Loss: 0.0057\n",
            "Training accuracy 100.0 %\n",
            "Time: 523.1460068225861\n",
            "Epoch [22/25], Step [175/607], Loss: 0.0253\n",
            "Training accuracy 100.0 %\n",
            "Time: 523.345071554184\n",
            "Epoch [22/25], Step [180/607], Loss: 0.0727\n",
            "Training accuracy 93.75 %\n",
            "Time: 523.5619866847992\n",
            "Epoch [22/25], Step [185/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 523.7646703720093\n",
            "Epoch [22/25], Step [190/607], Loss: 0.0096\n",
            "Training accuracy 100.0 %\n",
            "Time: 523.9786305427551\n",
            "Epoch [22/25], Step [195/607], Loss: 0.0109\n",
            "Training accuracy 100.0 %\n",
            "Time: 524.177139043808\n",
            "Epoch [22/25], Step [200/607], Loss: 0.0449\n",
            "Training accuracy 100.0 %\n",
            "Time: 524.3661835193634\n",
            "Epoch [22/25], Step [205/607], Loss: 0.0791\n",
            "Training accuracy 93.75 %\n",
            "Time: 524.5807123184204\n",
            "Epoch [22/25], Step [210/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 524.7921063899994\n",
            "Epoch [22/25], Step [215/607], Loss: 0.0544\n",
            "Training accuracy 100.0 %\n",
            "Time: 524.9807689189911\n",
            "Epoch [22/25], Step [220/607], Loss: 0.1703\n",
            "Training accuracy 93.75 %\n",
            "Time: 525.1988799571991\n",
            "Epoch [22/25], Step [225/607], Loss: 0.0202\n",
            "Training accuracy 100.0 %\n",
            "Time: 525.4086565971375\n",
            "Epoch [22/25], Step [230/607], Loss: 0.0112\n",
            "Training accuracy 100.0 %\n",
            "Time: 525.6032929420471\n",
            "Epoch [22/25], Step [235/607], Loss: 0.0074\n",
            "Training accuracy 100.0 %\n",
            "Time: 525.7948017120361\n",
            "Epoch [22/25], Step [240/607], Loss: 0.0202\n",
            "Training accuracy 100.0 %\n",
            "Time: 525.9866604804993\n",
            "Epoch [22/25], Step [245/607], Loss: 0.0016\n",
            "Training accuracy 100.0 %\n",
            "Time: 526.1756284236908\n",
            "Epoch [22/25], Step [250/607], Loss: 0.0181\n",
            "Training accuracy 100.0 %\n",
            "Time: 526.3622736930847\n",
            "Epoch [22/25], Step [255/607], Loss: 0.0832\n",
            "Training accuracy 93.75 %\n",
            "Time: 526.5658047199249\n",
            "Epoch [22/25], Step [260/607], Loss: 0.0232\n",
            "Training accuracy 100.0 %\n",
            "Time: 526.7564103603363\n",
            "Epoch [22/25], Step [265/607], Loss: 0.0228\n",
            "Training accuracy 100.0 %\n",
            "Time: 526.9529781341553\n",
            "Epoch [22/25], Step [270/607], Loss: 0.0012\n",
            "Training accuracy 100.0 %\n",
            "Time: 527.1432588100433\n",
            "Epoch [22/25], Step [275/607], Loss: 0.0027\n",
            "Training accuracy 100.0 %\n",
            "Time: 527.3395569324493\n",
            "Epoch [22/25], Step [280/607], Loss: 0.0031\n",
            "Training accuracy 100.0 %\n",
            "Time: 527.5532443523407\n",
            "Epoch [22/25], Step [285/607], Loss: 0.0134\n",
            "Training accuracy 100.0 %\n",
            "Time: 527.7374622821808\n",
            "Epoch [22/25], Step [290/607], Loss: 0.0234\n",
            "Training accuracy 100.0 %\n",
            "Time: 527.9513528347015\n",
            "Epoch [22/25], Step [295/607], Loss: 0.0101\n",
            "Training accuracy 100.0 %\n",
            "Time: 528.1315805912018\n",
            "Epoch [22/25], Step [300/607], Loss: 0.0099\n",
            "Training accuracy 100.0 %\n",
            "Time: 528.36115026474\n",
            "Epoch [22/25], Step [305/607], Loss: 0.0067\n",
            "Training accuracy 100.0 %\n",
            "Time: 528.5484437942505\n",
            "Epoch [22/25], Step [310/607], Loss: 0.1016\n",
            "Training accuracy 93.75 %\n",
            "Time: 528.7577292919159\n",
            "Epoch [22/25], Step [315/607], Loss: 0.0019\n",
            "Training accuracy 100.0 %\n",
            "Time: 528.9463884830475\n",
            "Epoch [22/25], Step [320/607], Loss: 0.0034\n",
            "Training accuracy 100.0 %\n",
            "Time: 529.1571147441864\n",
            "Epoch [22/25], Step [325/607], Loss: 0.0075\n",
            "Training accuracy 100.0 %\n",
            "Time: 529.341225862503\n",
            "Epoch [22/25], Step [330/607], Loss: 0.0430\n",
            "Training accuracy 100.0 %\n",
            "Time: 529.5830929279327\n",
            "Epoch [22/25], Step [335/607], Loss: 0.0084\n",
            "Training accuracy 100.0 %\n",
            "Time: 529.7606391906738\n",
            "Epoch [22/25], Step [340/607], Loss: 0.0019\n",
            "Training accuracy 100.0 %\n",
            "Time: 529.9667069911957\n",
            "Epoch [22/25], Step [345/607], Loss: 0.0017\n",
            "Training accuracy 100.0 %\n",
            "Time: 530.1584093570709\n",
            "Epoch [22/25], Step [350/607], Loss: 0.0083\n",
            "Training accuracy 100.0 %\n",
            "Time: 530.382470369339\n",
            "Epoch [22/25], Step [355/607], Loss: 0.0141\n",
            "Training accuracy 100.0 %\n",
            "Time: 530.5882298946381\n",
            "Epoch [22/25], Step [360/607], Loss: 0.0018\n",
            "Training accuracy 100.0 %\n",
            "Time: 530.7877464294434\n",
            "Epoch [22/25], Step [365/607], Loss: 0.0151\n",
            "Training accuracy 100.0 %\n",
            "Time: 530.9700560569763\n",
            "Epoch [22/25], Step [370/607], Loss: 0.0080\n",
            "Training accuracy 100.0 %\n",
            "Time: 531.181075334549\n",
            "Epoch [22/25], Step [375/607], Loss: 0.1449\n",
            "Training accuracy 93.75 %\n",
            "Time: 531.3563468456268\n",
            "Epoch [22/25], Step [380/607], Loss: 0.0138\n",
            "Training accuracy 100.0 %\n",
            "Time: 531.5635650157928\n",
            "Epoch [22/25], Step [385/607], Loss: 0.0163\n",
            "Training accuracy 100.0 %\n",
            "Time: 531.7559425830841\n",
            "Epoch [22/25], Step [390/607], Loss: 0.0478\n",
            "Training accuracy 93.75 %\n",
            "Time: 531.9529511928558\n",
            "Epoch [22/25], Step [395/607], Loss: 0.0049\n",
            "Training accuracy 100.0 %\n",
            "Time: 532.1640045642853\n",
            "Epoch [22/25], Step [400/607], Loss: 0.0011\n",
            "Training accuracy 100.0 %\n",
            "Time: 532.3719494342804\n",
            "Epoch [22/25], Step [405/607], Loss: 0.0008\n",
            "Training accuracy 100.0 %\n",
            "Time: 532.568470954895\n",
            "Epoch [22/25], Step [410/607], Loss: 0.0024\n",
            "Training accuracy 100.0 %\n",
            "Time: 532.7651317119598\n",
            "Epoch [22/25], Step [415/607], Loss: 0.0018\n",
            "Training accuracy 100.0 %\n",
            "Time: 532.9568934440613\n",
            "Epoch [22/25], Step [420/607], Loss: 0.0072\n",
            "Training accuracy 100.0 %\n",
            "Time: 533.1511540412903\n",
            "Epoch [22/25], Step [425/607], Loss: 0.0022\n",
            "Training accuracy 100.0 %\n",
            "Time: 533.3395793437958\n",
            "Epoch [22/25], Step [430/607], Loss: 0.0326\n",
            "Training accuracy 100.0 %\n",
            "Time: 533.5517723560333\n",
            "Epoch [22/25], Step [435/607], Loss: 0.0093\n",
            "Training accuracy 100.0 %\n",
            "Time: 533.7645959854126\n",
            "Epoch [22/25], Step [440/607], Loss: 1.4387\n",
            "Training accuracy 93.75 %\n",
            "Time: 533.957154750824\n",
            "Epoch [22/25], Step [445/607], Loss: 0.0065\n",
            "Training accuracy 100.0 %\n",
            "Time: 534.1815938949585\n",
            "Epoch [22/25], Step [450/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 534.3844742774963\n",
            "Epoch [22/25], Step [455/607], Loss: 0.0691\n",
            "Training accuracy 93.75 %\n",
            "Time: 534.6375761032104\n",
            "Epoch [22/25], Step [460/607], Loss: 0.0112\n",
            "Training accuracy 100.0 %\n",
            "Time: 534.8277571201324\n",
            "Epoch [22/25], Step [465/607], Loss: 0.0226\n",
            "Training accuracy 100.0 %\n",
            "Time: 535.0462808609009\n",
            "Epoch [22/25], Step [470/607], Loss: 0.0085\n",
            "Training accuracy 100.0 %\n",
            "Time: 535.2366600036621\n",
            "Epoch [22/25], Step [475/607], Loss: 0.0320\n",
            "Training accuracy 100.0 %\n",
            "Time: 535.442697763443\n",
            "Epoch [22/25], Step [480/607], Loss: 0.0090\n",
            "Training accuracy 100.0 %\n",
            "Time: 535.6300439834595\n",
            "Epoch [22/25], Step [485/607], Loss: 0.0008\n",
            "Training accuracy 100.0 %\n",
            "Time: 535.8354697227478\n",
            "Epoch [22/25], Step [490/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 536.0138516426086\n",
            "Epoch [22/25], Step [495/607], Loss: 0.0043\n",
            "Training accuracy 100.0 %\n",
            "Time: 536.2025561332703\n",
            "Epoch [22/25], Step [500/607], Loss: 0.0237\n",
            "Training accuracy 100.0 %\n",
            "Time: 536.3926424980164\n",
            "Epoch [22/25], Step [505/607], Loss: 0.0020\n",
            "Training accuracy 100.0 %\n",
            "Time: 536.6148388385773\n",
            "Epoch [22/25], Step [510/607], Loss: 0.0040\n",
            "Training accuracy 100.0 %\n",
            "Time: 536.7896702289581\n",
            "Epoch [22/25], Step [515/607], Loss: 0.0274\n",
            "Training accuracy 100.0 %\n",
            "Time: 537.0064883232117\n",
            "Epoch [22/25], Step [520/607], Loss: 0.0072\n",
            "Training accuracy 100.0 %\n",
            "Time: 537.1940336227417\n",
            "Epoch [22/25], Step [525/607], Loss: 0.0996\n",
            "Training accuracy 93.75 %\n",
            "Time: 537.423499584198\n",
            "Epoch [22/25], Step [530/607], Loss: 0.0065\n",
            "Training accuracy 100.0 %\n",
            "Time: 537.605840921402\n",
            "Epoch [22/25], Step [535/607], Loss: 0.0040\n",
            "Training accuracy 100.0 %\n",
            "Time: 537.8206100463867\n",
            "Epoch [22/25], Step [540/607], Loss: 0.3036\n",
            "Training accuracy 93.75 %\n",
            "Time: 538.0060522556305\n",
            "Epoch [22/25], Step [545/607], Loss: 0.0248\n",
            "Training accuracy 100.0 %\n",
            "Time: 538.1935348510742\n",
            "Epoch [22/25], Step [550/607], Loss: 0.0365\n",
            "Training accuracy 100.0 %\n",
            "Time: 538.3728363513947\n",
            "Epoch [22/25], Step [555/607], Loss: 0.0039\n",
            "Training accuracy 100.0 %\n",
            "Time: 538.5732090473175\n",
            "Epoch [22/25], Step [560/607], Loss: 0.0569\n",
            "Training accuracy 93.75 %\n",
            "Time: 538.7570872306824\n",
            "Epoch [22/25], Step [565/607], Loss: 0.0049\n",
            "Training accuracy 100.0 %\n",
            "Time: 538.9640111923218\n",
            "Epoch [22/25], Step [570/607], Loss: 0.0012\n",
            "Training accuracy 100.0 %\n",
            "Time: 539.1524531841278\n",
            "Epoch [22/25], Step [575/607], Loss: 0.0048\n",
            "Training accuracy 100.0 %\n",
            "Time: 539.3562359809875\n",
            "Epoch [22/25], Step [580/607], Loss: 0.0039\n",
            "Training accuracy 100.0 %\n",
            "Time: 539.5550863742828\n",
            "Epoch [22/25], Step [585/607], Loss: 0.0012\n",
            "Training accuracy 100.0 %\n",
            "Time: 539.7933750152588\n",
            "Epoch [22/25], Step [590/607], Loss: 0.0005\n",
            "Training accuracy 100.0 %\n",
            "Time: 539.981556892395\n",
            "Epoch [22/25], Step [595/607], Loss: 0.0258\n",
            "Training accuracy 100.0 %\n",
            "Time: 540.2000448703766\n",
            "Epoch [22/25], Step [600/607], Loss: 0.0121\n",
            "Training accuracy 100.0 %\n",
            "Time: 540.4126605987549\n",
            "Epoch [22/25], Step [605/607], Loss: 0.0043\n",
            "Training accuracy 100.0 %\n",
            "Time: 540.645923614502\n",
            "epoch 23\n",
            "Epoch [23/25], Step [5/607], Loss: 0.0143\n",
            "Training accuracy 100.0 %\n",
            "Time: 541.0824308395386\n",
            "Epoch [23/25], Step [10/607], Loss: 0.0011\n",
            "Training accuracy 100.0 %\n",
            "Time: 541.2702784538269\n",
            "Epoch [23/25], Step [15/607], Loss: 0.0054\n",
            "Training accuracy 100.0 %\n",
            "Time: 541.4601986408234\n",
            "Epoch [23/25], Step [20/607], Loss: 0.0983\n",
            "Training accuracy 93.75 %\n",
            "Time: 541.6582720279694\n",
            "Epoch [23/25], Step [25/607], Loss: 0.0088\n",
            "Training accuracy 100.0 %\n",
            "Time: 541.8594903945923\n",
            "Epoch [23/25], Step [30/607], Loss: 0.7633\n",
            "Training accuracy 93.75 %\n",
            "Time: 542.0587317943573\n",
            "Epoch [23/25], Step [35/607], Loss: 0.0201\n",
            "Training accuracy 100.0 %\n",
            "Time: 542.2515892982483\n",
            "Epoch [23/25], Step [40/607], Loss: 0.0311\n",
            "Training accuracy 100.0 %\n",
            "Time: 542.4511919021606\n",
            "Epoch [23/25], Step [45/607], Loss: 0.0055\n",
            "Training accuracy 100.0 %\n",
            "Time: 542.6518652439117\n",
            "Epoch [23/25], Step [50/607], Loss: 0.0045\n",
            "Training accuracy 100.0 %\n",
            "Time: 542.8513147830963\n",
            "Epoch [23/25], Step [55/607], Loss: 0.0280\n",
            "Training accuracy 100.0 %\n",
            "Time: 543.0276975631714\n",
            "Epoch [23/25], Step [60/607], Loss: 0.0031\n",
            "Training accuracy 100.0 %\n",
            "Time: 543.2273306846619\n",
            "Epoch [23/25], Step [65/607], Loss: 0.0175\n",
            "Training accuracy 100.0 %\n",
            "Time: 543.4127447605133\n",
            "Epoch [23/25], Step [70/607], Loss: 0.0041\n",
            "Training accuracy 100.0 %\n",
            "Time: 543.6283056735992\n",
            "Epoch [23/25], Step [75/607], Loss: 0.0274\n",
            "Training accuracy 100.0 %\n",
            "Time: 543.8022885322571\n",
            "Epoch [23/25], Step [80/607], Loss: 0.0012\n",
            "Training accuracy 100.0 %\n",
            "Time: 543.9803338050842\n",
            "Epoch [23/25], Step [85/607], Loss: 0.0316\n",
            "Training accuracy 100.0 %\n",
            "Time: 544.1519894599915\n",
            "Epoch [23/25], Step [90/607], Loss: 0.0469\n",
            "Training accuracy 100.0 %\n",
            "Time: 544.3706455230713\n",
            "Epoch [23/25], Step [95/607], Loss: 0.0129\n",
            "Training accuracy 100.0 %\n",
            "Time: 544.5543820858002\n",
            "Epoch [23/25], Step [100/607], Loss: 0.0136\n",
            "Training accuracy 100.0 %\n",
            "Time: 544.7796776294708\n",
            "Epoch [23/25], Step [105/607], Loss: 0.0040\n",
            "Training accuracy 100.0 %\n",
            "Time: 544.9720773696899\n",
            "Epoch [23/25], Step [110/607], Loss: 0.0008\n",
            "Training accuracy 100.0 %\n",
            "Time: 545.1967787742615\n",
            "Epoch [23/25], Step [115/607], Loss: 0.1220\n",
            "Training accuracy 93.75 %\n",
            "Time: 545.386488199234\n",
            "Epoch [23/25], Step [120/607], Loss: 0.0310\n",
            "Training accuracy 100.0 %\n",
            "Time: 545.600944519043\n",
            "Epoch [23/25], Step [125/607], Loss: 0.0072\n",
            "Training accuracy 100.0 %\n",
            "Time: 545.7869277000427\n",
            "Epoch [23/25], Step [130/607], Loss: 0.0040\n",
            "Training accuracy 100.0 %\n",
            "Time: 545.9904973506927\n",
            "Epoch [23/25], Step [135/607], Loss: 0.0082\n",
            "Training accuracy 100.0 %\n",
            "Time: 546.1905870437622\n",
            "Epoch [23/25], Step [140/607], Loss: 0.0019\n",
            "Training accuracy 100.0 %\n",
            "Time: 546.3970468044281\n",
            "Epoch [23/25], Step [145/607], Loss: 0.0007\n",
            "Training accuracy 100.0 %\n",
            "Time: 546.6028017997742\n",
            "Epoch [23/25], Step [150/607], Loss: 0.0068\n",
            "Training accuracy 100.0 %\n",
            "Time: 546.8137700557709\n",
            "Epoch [23/25], Step [155/607], Loss: 0.0091\n",
            "Training accuracy 100.0 %\n",
            "Time: 546.9953832626343\n",
            "Epoch [23/25], Step [160/607], Loss: 0.0066\n",
            "Training accuracy 100.0 %\n",
            "Time: 547.1706264019012\n",
            "Epoch [23/25], Step [165/607], Loss: 0.0545\n",
            "Training accuracy 100.0 %\n",
            "Time: 547.3462417125702\n",
            "Epoch [23/25], Step [170/607], Loss: 0.0104\n",
            "Training accuracy 100.0 %\n",
            "Time: 547.5284676551819\n",
            "Epoch [23/25], Step [175/607], Loss: 0.0131\n",
            "Training accuracy 100.0 %\n",
            "Time: 547.728033542633\n",
            "Epoch [23/25], Step [180/607], Loss: 0.0047\n",
            "Training accuracy 100.0 %\n",
            "Time: 547.9465992450714\n",
            "Epoch [23/25], Step [185/607], Loss: 0.2919\n",
            "Training accuracy 93.75 %\n",
            "Time: 548.1509866714478\n",
            "Epoch [23/25], Step [190/607], Loss: 0.0076\n",
            "Training accuracy 100.0 %\n",
            "Time: 548.3498146533966\n",
            "Epoch [23/25], Step [195/607], Loss: 0.0049\n",
            "Training accuracy 100.0 %\n",
            "Time: 548.5683822631836\n",
            "Epoch [23/25], Step [200/607], Loss: 0.0022\n",
            "Training accuracy 100.0 %\n",
            "Time: 548.7528779506683\n",
            "Epoch [23/25], Step [205/607], Loss: 0.0039\n",
            "Training accuracy 100.0 %\n",
            "Time: 548.953277349472\n",
            "Epoch [23/25], Step [210/607], Loss: 0.0186\n",
            "Training accuracy 100.0 %\n",
            "Time: 549.1847748756409\n",
            "Epoch [23/25], Step [215/607], Loss: 0.0287\n",
            "Training accuracy 100.0 %\n",
            "Time: 549.3882262706757\n",
            "Epoch [23/25], Step [220/607], Loss: 0.0020\n",
            "Training accuracy 100.0 %\n",
            "Time: 549.6200404167175\n",
            "Epoch [23/25], Step [225/607], Loss: 0.0014\n",
            "Training accuracy 100.0 %\n",
            "Time: 549.8441126346588\n",
            "Epoch [23/25], Step [230/607], Loss: 0.0009\n",
            "Training accuracy 100.0 %\n",
            "Time: 550.0380005836487\n",
            "Epoch [23/25], Step [235/607], Loss: 0.0019\n",
            "Training accuracy 100.0 %\n",
            "Time: 550.2112138271332\n",
            "Epoch [23/25], Step [240/607], Loss: 0.0031\n",
            "Training accuracy 100.0 %\n",
            "Time: 550.393397808075\n",
            "Epoch [23/25], Step [245/607], Loss: 0.0055\n",
            "Training accuracy 100.0 %\n",
            "Time: 550.5914998054504\n",
            "Epoch [23/25], Step [250/607], Loss: 0.0108\n",
            "Training accuracy 100.0 %\n",
            "Time: 550.7862102985382\n",
            "Epoch [23/25], Step [255/607], Loss: 0.0045\n",
            "Training accuracy 100.0 %\n",
            "Time: 550.9885582923889\n",
            "Epoch [23/25], Step [260/607], Loss: 0.0071\n",
            "Training accuracy 100.0 %\n",
            "Time: 551.1904096603394\n",
            "Epoch [23/25], Step [265/607], Loss: 0.0032\n",
            "Training accuracy 100.0 %\n",
            "Time: 551.3946034908295\n",
            "Epoch [23/25], Step [270/607], Loss: 0.0037\n",
            "Training accuracy 100.0 %\n",
            "Time: 551.6010656356812\n",
            "Epoch [23/25], Step [275/607], Loss: 0.0094\n",
            "Training accuracy 100.0 %\n",
            "Time: 551.8211898803711\n",
            "Epoch [23/25], Step [280/607], Loss: 0.0015\n",
            "Training accuracy 100.0 %\n",
            "Time: 552.0276582241058\n",
            "Epoch [23/25], Step [285/607], Loss: 0.0077\n",
            "Training accuracy 100.0 %\n",
            "Time: 552.2282164096832\n",
            "Epoch [23/25], Step [290/607], Loss: 0.0171\n",
            "Training accuracy 100.0 %\n",
            "Time: 552.4299504756927\n",
            "Epoch [23/25], Step [295/607], Loss: 0.0024\n",
            "Training accuracy 100.0 %\n",
            "Time: 552.6077704429626\n",
            "Epoch [23/25], Step [300/607], Loss: 0.0026\n",
            "Training accuracy 100.0 %\n",
            "Time: 552.8286621570587\n",
            "Epoch [23/25], Step [305/607], Loss: 0.0485\n",
            "Training accuracy 100.0 %\n",
            "Time: 553.0023684501648\n",
            "Epoch [23/25], Step [310/607], Loss: 0.0036\n",
            "Training accuracy 100.0 %\n",
            "Time: 553.1814603805542\n",
            "Epoch [23/25], Step [315/607], Loss: 0.0093\n",
            "Training accuracy 100.0 %\n",
            "Time: 553.3599045276642\n",
            "Epoch [23/25], Step [320/607], Loss: 0.0123\n",
            "Training accuracy 100.0 %\n",
            "Time: 553.5628452301025\n",
            "Epoch [23/25], Step [325/607], Loss: 0.0069\n",
            "Training accuracy 100.0 %\n",
            "Time: 553.7542653083801\n",
            "Epoch [23/25], Step [330/607], Loss: 0.0097\n",
            "Training accuracy 100.0 %\n",
            "Time: 553.9750230312347\n",
            "Epoch [23/25], Step [335/607], Loss: 0.0039\n",
            "Training accuracy 100.0 %\n",
            "Time: 554.2110888957977\n",
            "Epoch [23/25], Step [340/607], Loss: 0.0209\n",
            "Training accuracy 100.0 %\n",
            "Time: 554.3820953369141\n",
            "Epoch [23/25], Step [345/607], Loss: 0.0735\n",
            "Training accuracy 93.75 %\n",
            "Time: 554.5721170902252\n",
            "Epoch [23/25], Step [350/607], Loss: 0.0015\n",
            "Training accuracy 100.0 %\n",
            "Time: 554.7633407115936\n",
            "Epoch [23/25], Step [355/607], Loss: 0.0150\n",
            "Training accuracy 100.0 %\n",
            "Time: 554.9933958053589\n",
            "Epoch [23/25], Step [360/607], Loss: 0.0046\n",
            "Training accuracy 100.0 %\n",
            "Time: 555.1897614002228\n",
            "Epoch [23/25], Step [365/607], Loss: 0.0017\n",
            "Training accuracy 100.0 %\n",
            "Time: 555.3859791755676\n",
            "Epoch [23/25], Step [370/607], Loss: 0.9803\n",
            "Training accuracy 93.75 %\n",
            "Time: 555.5830478668213\n",
            "Epoch [23/25], Step [375/607], Loss: 0.0397\n",
            "Training accuracy 100.0 %\n",
            "Time: 555.7774693965912\n",
            "Epoch [23/25], Step [380/607], Loss: 0.0030\n",
            "Training accuracy 100.0 %\n",
            "Time: 555.965788602829\n",
            "Epoch [23/25], Step [385/607], Loss: 0.0028\n",
            "Training accuracy 100.0 %\n",
            "Time: 556.1648514270782\n",
            "Epoch [23/25], Step [390/607], Loss: 0.0046\n",
            "Training accuracy 100.0 %\n",
            "Time: 556.3574848175049\n",
            "Epoch [23/25], Step [395/607], Loss: 0.0018\n",
            "Training accuracy 100.0 %\n",
            "Time: 556.5604832172394\n",
            "Epoch [23/25], Step [400/607], Loss: 0.1147\n",
            "Training accuracy 93.75 %\n",
            "Time: 556.7488090991974\n",
            "Epoch [23/25], Step [405/607], Loss: 0.1948\n",
            "Training accuracy 93.75 %\n",
            "Time: 556.9420144557953\n",
            "Epoch [23/25], Step [410/607], Loss: 0.0066\n",
            "Training accuracy 100.0 %\n",
            "Time: 557.1463203430176\n",
            "Epoch [23/25], Step [415/607], Loss: 0.0092\n",
            "Training accuracy 100.0 %\n",
            "Time: 557.326534986496\n",
            "Epoch [23/25], Step [420/607], Loss: 0.0053\n",
            "Training accuracy 100.0 %\n",
            "Time: 557.5351984500885\n",
            "Epoch [23/25], Step [425/607], Loss: 0.0608\n",
            "Training accuracy 93.75 %\n",
            "Time: 557.720597743988\n",
            "Epoch [23/25], Step [430/607], Loss: 0.2500\n",
            "Training accuracy 93.75 %\n",
            "Time: 557.9097287654877\n",
            "Epoch [23/25], Step [435/607], Loss: 0.0019\n",
            "Training accuracy 100.0 %\n",
            "Time: 558.106166601181\n",
            "Epoch [23/25], Step [440/607], Loss: 0.0008\n",
            "Training accuracy 100.0 %\n",
            "Time: 558.315357208252\n",
            "Epoch [23/25], Step [445/607], Loss: 0.0007\n",
            "Training accuracy 100.0 %\n",
            "Time: 558.4938621520996\n",
            "Epoch [23/25], Step [450/607], Loss: 0.0124\n",
            "Training accuracy 100.0 %\n",
            "Time: 558.7024073600769\n",
            "Epoch [23/25], Step [455/607], Loss: 0.0026\n",
            "Training accuracy 100.0 %\n",
            "Time: 558.9067182540894\n",
            "Epoch [23/25], Step [460/607], Loss: 0.0158\n",
            "Training accuracy 100.0 %\n",
            "Time: 559.1233739852905\n",
            "Epoch [23/25], Step [465/607], Loss: 0.0238\n",
            "Training accuracy 100.0 %\n",
            "Time: 559.335334777832\n",
            "Epoch [23/25], Step [470/607], Loss: 0.0815\n",
            "Training accuracy 93.75 %\n",
            "Time: 559.5626993179321\n",
            "Epoch [23/25], Step [475/607], Loss: 0.0051\n",
            "Training accuracy 100.0 %\n",
            "Time: 559.7724704742432\n",
            "Epoch [23/25], Step [480/607], Loss: 0.0033\n",
            "Training accuracy 100.0 %\n",
            "Time: 559.9786455631256\n",
            "Epoch [23/25], Step [485/607], Loss: 0.0450\n",
            "Training accuracy 100.0 %\n",
            "Time: 560.1964747905731\n",
            "Epoch [23/25], Step [490/607], Loss: 0.0134\n",
            "Training accuracy 100.0 %\n",
            "Time: 560.4059858322144\n",
            "Epoch [23/25], Step [495/607], Loss: 0.0044\n",
            "Training accuracy 100.0 %\n",
            "Time: 560.6136991977692\n",
            "Epoch [23/25], Step [500/607], Loss: 0.0182\n",
            "Training accuracy 100.0 %\n",
            "Time: 560.8023657798767\n",
            "Epoch [23/25], Step [505/607], Loss: 0.0113\n",
            "Training accuracy 100.0 %\n",
            "Time: 561.0259864330292\n",
            "Epoch [23/25], Step [510/607], Loss: 0.0047\n",
            "Training accuracy 100.0 %\n",
            "Time: 561.2152307033539\n",
            "Epoch [23/25], Step [515/607], Loss: 0.0038\n",
            "Training accuracy 100.0 %\n",
            "Time: 561.4439895153046\n",
            "Epoch [23/25], Step [520/607], Loss: 0.0433\n",
            "Training accuracy 100.0 %\n",
            "Time: 561.6243886947632\n",
            "Epoch [23/25], Step [525/607], Loss: 0.0320\n",
            "Training accuracy 100.0 %\n",
            "Time: 561.8232569694519\n",
            "Epoch [23/25], Step [530/607], Loss: 0.0050\n",
            "Training accuracy 100.0 %\n",
            "Time: 562.0032684803009\n",
            "Epoch [23/25], Step [535/607], Loss: 0.0013\n",
            "Training accuracy 100.0 %\n",
            "Time: 562.1985754966736\n",
            "Epoch [23/25], Step [540/607], Loss: 0.0042\n",
            "Training accuracy 100.0 %\n",
            "Time: 562.4006702899933\n",
            "Epoch [23/25], Step [545/607], Loss: 0.0716\n",
            "Training accuracy 93.75 %\n",
            "Time: 562.6157190799713\n",
            "Epoch [23/25], Step [550/607], Loss: 0.0037\n",
            "Training accuracy 100.0 %\n",
            "Time: 562.8039140701294\n",
            "Epoch [23/25], Step [555/607], Loss: 0.0100\n",
            "Training accuracy 100.0 %\n",
            "Time: 563.0389950275421\n",
            "Epoch [23/25], Step [560/607], Loss: 0.0048\n",
            "Training accuracy 100.0 %\n",
            "Time: 563.2308692932129\n",
            "Epoch [23/25], Step [565/607], Loss: 0.0070\n",
            "Training accuracy 100.0 %\n",
            "Time: 563.4173138141632\n",
            "Epoch [23/25], Step [570/607], Loss: 0.0349\n",
            "Training accuracy 100.0 %\n",
            "Time: 563.5969877243042\n",
            "Epoch [23/25], Step [575/607], Loss: 0.0055\n",
            "Training accuracy 100.0 %\n",
            "Time: 563.8027858734131\n",
            "Epoch [23/25], Step [580/607], Loss: 0.0086\n",
            "Training accuracy 100.0 %\n",
            "Time: 563.9930939674377\n",
            "Epoch [23/25], Step [585/607], Loss: 0.0077\n",
            "Training accuracy 100.0 %\n",
            "Time: 564.1766383647919\n",
            "Epoch [23/25], Step [590/607], Loss: 0.0007\n",
            "Training accuracy 100.0 %\n",
            "Time: 564.3532848358154\n",
            "Epoch [23/25], Step [595/607], Loss: 0.0559\n",
            "Training accuracy 93.75 %\n",
            "Time: 564.5609264373779\n",
            "Epoch [23/25], Step [600/607], Loss: 0.0062\n",
            "Training accuracy 100.0 %\n",
            "Time: 564.7543559074402\n",
            "Epoch [23/25], Step [605/607], Loss: 0.0023\n",
            "Training accuracy 100.0 %\n",
            "Time: 564.9462926387787\n",
            "epoch 24\n",
            "Epoch [24/25], Step [5/607], Loss: 0.0107\n",
            "Training accuracy 100.0 %\n",
            "Time: 565.3870775699615\n",
            "Epoch [24/25], Step [10/607], Loss: 0.0225\n",
            "Training accuracy 100.0 %\n",
            "Time: 565.5729296207428\n",
            "Epoch [24/25], Step [15/607], Loss: 0.0059\n",
            "Training accuracy 100.0 %\n",
            "Time: 565.7793116569519\n",
            "Epoch [24/25], Step [20/607], Loss: 1.0149\n",
            "Training accuracy 93.75 %\n",
            "Time: 565.9733939170837\n",
            "Epoch [24/25], Step [25/607], Loss: 0.0085\n",
            "Training accuracy 100.0 %\n",
            "Time: 566.197865486145\n",
            "Epoch [24/25], Step [30/607], Loss: 0.0084\n",
            "Training accuracy 100.0 %\n",
            "Time: 566.40225481987\n",
            "Epoch [24/25], Step [35/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 566.6165778636932\n",
            "Epoch [24/25], Step [40/607], Loss: 0.0147\n",
            "Training accuracy 100.0 %\n",
            "Time: 566.8376893997192\n",
            "Epoch [24/25], Step [45/607], Loss: 0.0355\n",
            "Training accuracy 100.0 %\n",
            "Time: 567.0164031982422\n",
            "Epoch [24/25], Step [50/607], Loss: 0.0404\n",
            "Training accuracy 100.0 %\n",
            "Time: 567.1863355636597\n",
            "Epoch [24/25], Step [55/607], Loss: 0.0015\n",
            "Training accuracy 100.0 %\n",
            "Time: 567.400527715683\n",
            "Epoch [24/25], Step [60/607], Loss: 0.0033\n",
            "Training accuracy 100.0 %\n",
            "Time: 567.593132019043\n",
            "Epoch [24/25], Step [65/607], Loss: 0.0658\n",
            "Training accuracy 93.75 %\n",
            "Time: 567.7931616306305\n",
            "Epoch [24/25], Step [70/607], Loss: 0.0162\n",
            "Training accuracy 100.0 %\n",
            "Time: 567.9784936904907\n",
            "Epoch [24/25], Step [75/607], Loss: 0.0013\n",
            "Training accuracy 100.0 %\n",
            "Time: 568.1866185665131\n",
            "Epoch [24/25], Step [80/607], Loss: 0.0304\n",
            "Training accuracy 100.0 %\n",
            "Time: 568.3698959350586\n",
            "Epoch [24/25], Step [85/607], Loss: 0.0169\n",
            "Training accuracy 100.0 %\n",
            "Time: 568.5694711208344\n",
            "Epoch [24/25], Step [90/607], Loss: 0.0004\n",
            "Training accuracy 100.0 %\n",
            "Time: 568.7553398609161\n",
            "Epoch [24/25], Step [95/607], Loss: 0.0033\n",
            "Training accuracy 100.0 %\n",
            "Time: 568.9521999359131\n",
            "Epoch [24/25], Step [100/607], Loss: 0.0247\n",
            "Training accuracy 100.0 %\n",
            "Time: 569.1423864364624\n",
            "Epoch [24/25], Step [105/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 569.3362767696381\n",
            "Epoch [24/25], Step [110/607], Loss: 0.0043\n",
            "Training accuracy 100.0 %\n",
            "Time: 569.5370364189148\n",
            "Epoch [24/25], Step [115/607], Loss: 0.0017\n",
            "Training accuracy 100.0 %\n",
            "Time: 569.7515914440155\n",
            "Epoch [24/25], Step [120/607], Loss: 0.0064\n",
            "Training accuracy 100.0 %\n",
            "Time: 569.9592654705048\n",
            "Epoch [24/25], Step [125/607], Loss: 0.0280\n",
            "Training accuracy 100.0 %\n",
            "Time: 570.2178103923798\n",
            "Epoch [24/25], Step [130/607], Loss: 0.0308\n",
            "Training accuracy 100.0 %\n",
            "Time: 570.4171640872955\n",
            "Epoch [24/25], Step [135/607], Loss: 0.0064\n",
            "Training accuracy 100.0 %\n",
            "Time: 570.6486265659332\n",
            "Epoch [24/25], Step [140/607], Loss: 0.0212\n",
            "Training accuracy 100.0 %\n",
            "Time: 570.8359527587891\n",
            "Epoch [24/25], Step [145/607], Loss: 0.0207\n",
            "Training accuracy 100.0 %\n",
            "Time: 571.0354998111725\n",
            "Epoch [24/25], Step [150/607], Loss: 0.0757\n",
            "Training accuracy 93.75 %\n",
            "Time: 571.219188451767\n",
            "Epoch [24/25], Step [155/607], Loss: 0.0289\n",
            "Training accuracy 100.0 %\n",
            "Time: 571.4312109947205\n",
            "Epoch [24/25], Step [160/607], Loss: 0.0115\n",
            "Training accuracy 100.0 %\n",
            "Time: 571.6177492141724\n",
            "Epoch [24/25], Step [165/607], Loss: 0.0014\n",
            "Training accuracy 100.0 %\n",
            "Time: 571.8245997428894\n",
            "Epoch [24/25], Step [170/607], Loss: 0.0032\n",
            "Training accuracy 100.0 %\n",
            "Time: 572.0167036056519\n",
            "Epoch [24/25], Step [175/607], Loss: 0.0540\n",
            "Training accuracy 93.75 %\n",
            "Time: 572.2340829372406\n",
            "Epoch [24/25], Step [180/607], Loss: 0.0236\n",
            "Training accuracy 100.0 %\n",
            "Time: 572.4079344272614\n",
            "Epoch [24/25], Step [185/607], Loss: 0.0048\n",
            "Training accuracy 100.0 %\n",
            "Time: 572.5974173545837\n",
            "Epoch [24/25], Step [190/607], Loss: 0.0095\n",
            "Training accuracy 100.0 %\n",
            "Time: 572.7780318260193\n",
            "Epoch [24/25], Step [195/607], Loss: 0.0043\n",
            "Training accuracy 100.0 %\n",
            "Time: 572.9900321960449\n",
            "Epoch [24/25], Step [200/607], Loss: 0.0024\n",
            "Training accuracy 100.0 %\n",
            "Time: 573.1819355487823\n",
            "Epoch [24/25], Step [205/607], Loss: 0.0063\n",
            "Training accuracy 100.0 %\n",
            "Time: 573.4045307636261\n",
            "Epoch [24/25], Step [210/607], Loss: 0.0024\n",
            "Training accuracy 100.0 %\n",
            "Time: 573.5860300064087\n",
            "Epoch [24/25], Step [215/607], Loss: 0.0058\n",
            "Training accuracy 100.0 %\n",
            "Time: 573.779125213623\n",
            "Epoch [24/25], Step [220/607], Loss: 0.0029\n",
            "Training accuracy 100.0 %\n",
            "Time: 573.9564118385315\n",
            "Epoch [24/25], Step [225/607], Loss: 0.0022\n",
            "Training accuracy 100.0 %\n",
            "Time: 574.1698970794678\n",
            "Epoch [24/25], Step [230/607], Loss: 0.0013\n",
            "Training accuracy 100.0 %\n",
            "Time: 574.3579473495483\n",
            "Epoch [24/25], Step [235/607], Loss: 0.0026\n",
            "Training accuracy 100.0 %\n",
            "Time: 574.5768814086914\n",
            "Epoch [24/25], Step [240/607], Loss: 0.0280\n",
            "Training accuracy 100.0 %\n",
            "Time: 574.7569880485535\n",
            "Epoch [24/25], Step [245/607], Loss: 0.0396\n",
            "Training accuracy 100.0 %\n",
            "Time: 574.957389831543\n",
            "Epoch [24/25], Step [250/607], Loss: 0.0011\n",
            "Training accuracy 100.0 %\n",
            "Time: 575.1520195007324\n",
            "Epoch [24/25], Step [255/607], Loss: 0.0278\n",
            "Training accuracy 100.0 %\n",
            "Time: 575.38965010643\n",
            "Epoch [24/25], Step [260/607], Loss: 0.0133\n",
            "Training accuracy 100.0 %\n",
            "Time: 575.5721731185913\n",
            "Epoch [24/25], Step [265/607], Loss: 0.0064\n",
            "Training accuracy 100.0 %\n",
            "Time: 575.7807946205139\n",
            "Epoch [24/25], Step [270/607], Loss: 0.0065\n",
            "Training accuracy 100.0 %\n",
            "Time: 575.9752402305603\n",
            "Epoch [24/25], Step [275/607], Loss: 0.0022\n",
            "Training accuracy 100.0 %\n",
            "Time: 576.1882660388947\n",
            "Epoch [24/25], Step [280/607], Loss: 0.0213\n",
            "Training accuracy 100.0 %\n",
            "Time: 576.3774852752686\n",
            "Epoch [24/25], Step [285/607], Loss: 0.0111\n",
            "Training accuracy 100.0 %\n",
            "Time: 576.5693056583405\n",
            "Epoch [24/25], Step [290/607], Loss: 0.0049\n",
            "Training accuracy 100.0 %\n",
            "Time: 576.7549147605896\n",
            "Epoch [24/25], Step [295/607], Loss: 0.0034\n",
            "Training accuracy 100.0 %\n",
            "Time: 576.9566764831543\n",
            "Epoch [24/25], Step [300/607], Loss: 0.0167\n",
            "Training accuracy 100.0 %\n",
            "Time: 577.1737375259399\n",
            "Epoch [24/25], Step [305/607], Loss: 0.0076\n",
            "Training accuracy 100.0 %\n",
            "Time: 577.3838739395142\n",
            "Epoch [24/25], Step [310/607], Loss: 0.0011\n",
            "Training accuracy 100.0 %\n",
            "Time: 577.6033527851105\n",
            "Epoch [24/25], Step [315/607], Loss: 0.0096\n",
            "Training accuracy 100.0 %\n",
            "Time: 577.8180601596832\n",
            "Epoch [24/25], Step [320/607], Loss: 0.0034\n",
            "Training accuracy 100.0 %\n",
            "Time: 578.0180237293243\n",
            "Epoch [24/25], Step [325/607], Loss: 0.0019\n",
            "Training accuracy 100.0 %\n",
            "Time: 578.2201323509216\n",
            "Epoch [24/25], Step [330/607], Loss: 0.0057\n",
            "Training accuracy 100.0 %\n",
            "Time: 578.412683725357\n",
            "Epoch [24/25], Step [335/607], Loss: 0.0021\n",
            "Training accuracy 100.0 %\n",
            "Time: 578.6168873310089\n",
            "Epoch [24/25], Step [340/607], Loss: 0.0079\n",
            "Training accuracy 100.0 %\n",
            "Time: 578.8028962612152\n",
            "Epoch [24/25], Step [345/607], Loss: 0.0015\n",
            "Training accuracy 100.0 %\n",
            "Time: 579.0019583702087\n",
            "Epoch [24/25], Step [350/607], Loss: 0.0048\n",
            "Training accuracy 100.0 %\n",
            "Time: 579.1997497081757\n",
            "Epoch [24/25], Step [355/607], Loss: 0.0020\n",
            "Training accuracy 100.0 %\n",
            "Time: 579.4090240001678\n",
            "Epoch [24/25], Step [360/607], Loss: 0.0009\n",
            "Training accuracy 100.0 %\n",
            "Time: 579.607782125473\n",
            "Epoch [24/25], Step [365/607], Loss: 0.1004\n",
            "Training accuracy 93.75 %\n",
            "Time: 579.8293154239655\n",
            "Epoch [24/25], Step [370/607], Loss: 0.0089\n",
            "Training accuracy 100.0 %\n",
            "Time: 580.0229935646057\n",
            "Epoch [24/25], Step [375/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 580.2271480560303\n",
            "Epoch [24/25], Step [380/607], Loss: 0.0010\n",
            "Training accuracy 100.0 %\n",
            "Time: 580.443502664566\n",
            "Epoch [24/25], Step [385/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 580.6535696983337\n",
            "Epoch [24/25], Step [390/607], Loss: 0.0013\n",
            "Training accuracy 100.0 %\n",
            "Time: 580.8671061992645\n",
            "Epoch [24/25], Step [395/607], Loss: 0.0130\n",
            "Training accuracy 100.0 %\n",
            "Time: 581.0837569236755\n",
            "Epoch [24/25], Step [400/607], Loss: 0.0050\n",
            "Training accuracy 100.0 %\n",
            "Time: 581.2960238456726\n",
            "Epoch [24/25], Step [405/607], Loss: 0.0058\n",
            "Training accuracy 100.0 %\n",
            "Time: 581.5083148479462\n",
            "Epoch [24/25], Step [410/607], Loss: 0.0043\n",
            "Training accuracy 100.0 %\n",
            "Time: 581.7200541496277\n",
            "Epoch [24/25], Step [415/607], Loss: 0.0078\n",
            "Training accuracy 100.0 %\n",
            "Time: 581.9339847564697\n",
            "Epoch [24/25], Step [420/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 582.1391799449921\n",
            "Epoch [24/25], Step [425/607], Loss: 0.0027\n",
            "Training accuracy 100.0 %\n",
            "Time: 582.3429608345032\n",
            "Epoch [24/25], Step [430/607], Loss: 0.0043\n",
            "Training accuracy 100.0 %\n",
            "Time: 582.5454571247101\n",
            "Epoch [24/25], Step [435/607], Loss: 0.2948\n",
            "Training accuracy 93.75 %\n",
            "Time: 582.7388017177582\n",
            "Epoch [24/25], Step [440/607], Loss: 0.0122\n",
            "Training accuracy 100.0 %\n",
            "Time: 582.9444310665131\n",
            "Epoch [24/25], Step [445/607], Loss: 0.0136\n",
            "Training accuracy 100.0 %\n",
            "Time: 583.1280536651611\n",
            "Epoch [24/25], Step [450/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 583.3406293392181\n",
            "Epoch [24/25], Step [455/607], Loss: 0.0508\n",
            "Training accuracy 100.0 %\n",
            "Time: 583.5400547981262\n",
            "Epoch [24/25], Step [460/607], Loss: 0.0078\n",
            "Training accuracy 100.0 %\n",
            "Time: 583.7325041294098\n",
            "Epoch [24/25], Step [465/607], Loss: 0.0215\n",
            "Training accuracy 100.0 %\n",
            "Time: 583.9103264808655\n",
            "Epoch [24/25], Step [470/607], Loss: 0.0063\n",
            "Training accuracy 100.0 %\n",
            "Time: 584.1167936325073\n",
            "Epoch [24/25], Step [475/607], Loss: 0.0470\n",
            "Training accuracy 93.75 %\n",
            "Time: 584.3140497207642\n",
            "Epoch [24/25], Step [480/607], Loss: 0.0032\n",
            "Training accuracy 100.0 %\n",
            "Time: 584.5221331119537\n",
            "Epoch [24/25], Step [485/607], Loss: 0.0079\n",
            "Training accuracy 100.0 %\n",
            "Time: 584.6991124153137\n",
            "Epoch [24/25], Step [490/607], Loss: 0.3557\n",
            "Training accuracy 93.75 %\n",
            "Time: 584.9023349285126\n",
            "Epoch [24/25], Step [495/607], Loss: 0.0024\n",
            "Training accuracy 100.0 %\n",
            "Time: 585.1006648540497\n",
            "Epoch [24/25], Step [500/607], Loss: 0.0289\n",
            "Training accuracy 100.0 %\n",
            "Time: 585.3285782337189\n",
            "Epoch [24/25], Step [505/607], Loss: 0.0011\n",
            "Training accuracy 100.0 %\n",
            "Time: 585.5643081665039\n",
            "Epoch [24/25], Step [510/607], Loss: 0.0118\n",
            "Training accuracy 100.0 %\n",
            "Time: 585.7939326763153\n",
            "Epoch [24/25], Step [515/607], Loss: 0.0224\n",
            "Training accuracy 100.0 %\n",
            "Time: 586.0112869739532\n",
            "Epoch [24/25], Step [520/607], Loss: 0.0028\n",
            "Training accuracy 100.0 %\n",
            "Time: 586.2437310218811\n",
            "Epoch [24/25], Step [525/607], Loss: 0.0011\n",
            "Training accuracy 100.0 %\n",
            "Time: 586.4682946205139\n",
            "Epoch [24/25], Step [530/607], Loss: 0.0058\n",
            "Training accuracy 100.0 %\n",
            "Time: 586.6862251758575\n",
            "Epoch [24/25], Step [535/607], Loss: 0.0048\n",
            "Training accuracy 100.0 %\n",
            "Time: 586.899304151535\n",
            "Epoch [24/25], Step [540/607], Loss: 0.0120\n",
            "Training accuracy 100.0 %\n",
            "Time: 587.1185503005981\n",
            "Epoch [24/25], Step [545/607], Loss: 0.0190\n",
            "Training accuracy 100.0 %\n",
            "Time: 587.3344678878784\n",
            "Epoch [24/25], Step [550/607], Loss: 0.0023\n",
            "Training accuracy 100.0 %\n",
            "Time: 587.5436170101166\n",
            "Epoch [24/25], Step [555/607], Loss: 0.0071\n",
            "Training accuracy 100.0 %\n",
            "Time: 587.7513444423676\n",
            "Epoch [24/25], Step [560/607], Loss: 0.0017\n",
            "Training accuracy 100.0 %\n",
            "Time: 587.9509325027466\n",
            "Epoch [24/25], Step [565/607], Loss: 0.0016\n",
            "Training accuracy 100.0 %\n",
            "Time: 588.1584768295288\n",
            "Epoch [24/25], Step [570/607], Loss: 0.0051\n",
            "Training accuracy 100.0 %\n",
            "Time: 588.3702149391174\n",
            "Epoch [24/25], Step [575/607], Loss: 0.0087\n",
            "Training accuracy 100.0 %\n",
            "Time: 588.5563411712646\n",
            "Epoch [24/25], Step [580/607], Loss: 0.0222\n",
            "Training accuracy 100.0 %\n",
            "Time: 588.7801833152771\n",
            "Epoch [24/25], Step [585/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 588.971715927124\n",
            "Epoch [24/25], Step [590/607], Loss: 0.0060\n",
            "Training accuracy 100.0 %\n",
            "Time: 589.1963911056519\n",
            "Epoch [24/25], Step [595/607], Loss: 0.0070\n",
            "Training accuracy 100.0 %\n",
            "Time: 589.39200258255\n",
            "Epoch [24/25], Step [600/607], Loss: 0.0026\n",
            "Training accuracy 100.0 %\n",
            "Time: 589.6092014312744\n",
            "Epoch [24/25], Step [605/607], Loss: 0.0050\n",
            "Training accuracy 100.0 %\n",
            "Time: 589.7955408096313\n",
            "epoch 25\n",
            "Epoch [25/25], Step [5/607], Loss: 0.0344\n",
            "Training accuracy 100.0 %\n",
            "Time: 590.2174363136292\n",
            "Epoch [25/25], Step [10/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 590.4146416187286\n",
            "Epoch [25/25], Step [15/607], Loss: 0.0008\n",
            "Training accuracy 100.0 %\n",
            "Time: 590.6282753944397\n",
            "Epoch [25/25], Step [20/607], Loss: 0.0073\n",
            "Training accuracy 100.0 %\n",
            "Time: 590.8254005908966\n",
            "Epoch [25/25], Step [25/607], Loss: 0.0038\n",
            "Training accuracy 100.0 %\n",
            "Time: 591.0433835983276\n",
            "Epoch [25/25], Step [30/607], Loss: 0.0235\n",
            "Training accuracy 100.0 %\n",
            "Time: 591.2430210113525\n",
            "Epoch [25/25], Step [35/607], Loss: 0.0195\n",
            "Training accuracy 100.0 %\n",
            "Time: 591.4556424617767\n",
            "Epoch [25/25], Step [40/607], Loss: 0.0261\n",
            "Training accuracy 100.0 %\n",
            "Time: 591.665239572525\n",
            "Epoch [25/25], Step [45/607], Loss: 0.0331\n",
            "Training accuracy 100.0 %\n",
            "Time: 591.8801603317261\n",
            "Epoch [25/25], Step [50/607], Loss: 0.0840\n",
            "Training accuracy 93.75 %\n",
            "Time: 592.0842041969299\n",
            "Epoch [25/25], Step [55/607], Loss: 0.0271\n",
            "Training accuracy 100.0 %\n",
            "Time: 592.2966315746307\n",
            "Epoch [25/25], Step [60/607], Loss: 0.0335\n",
            "Training accuracy 100.0 %\n",
            "Time: 592.517612695694\n",
            "Epoch [25/25], Step [65/607], Loss: 0.0068\n",
            "Training accuracy 100.0 %\n",
            "Time: 592.6892855167389\n",
            "Epoch [25/25], Step [70/607], Loss: 0.0049\n",
            "Training accuracy 100.0 %\n",
            "Time: 592.8742842674255\n",
            "Epoch [25/25], Step [75/607], Loss: 0.0043\n",
            "Training accuracy 100.0 %\n",
            "Time: 593.0607998371124\n",
            "Epoch [25/25], Step [80/607], Loss: 0.0014\n",
            "Training accuracy 100.0 %\n",
            "Time: 593.2688112258911\n",
            "Epoch [25/25], Step [85/607], Loss: 0.0062\n",
            "Training accuracy 100.0 %\n",
            "Time: 593.4651763439178\n",
            "Epoch [25/25], Step [90/607], Loss: 0.0257\n",
            "Training accuracy 100.0 %\n",
            "Time: 593.6764833927155\n",
            "Epoch [25/25], Step [95/607], Loss: 0.0089\n",
            "Training accuracy 100.0 %\n",
            "Time: 593.8738582134247\n",
            "Epoch [25/25], Step [100/607], Loss: 0.0347\n",
            "Training accuracy 100.0 %\n",
            "Time: 594.0667836666107\n",
            "Epoch [25/25], Step [105/607], Loss: 0.0009\n",
            "Training accuracy 100.0 %\n",
            "Time: 594.244366645813\n",
            "Epoch [25/25], Step [110/607], Loss: 0.0013\n",
            "Training accuracy 100.0 %\n",
            "Time: 594.4560821056366\n",
            "Epoch [25/25], Step [115/607], Loss: 0.0497\n",
            "Training accuracy 100.0 %\n",
            "Time: 594.6403796672821\n",
            "Epoch [25/25], Step [120/607], Loss: 0.0023\n",
            "Training accuracy 100.0 %\n",
            "Time: 594.833404302597\n",
            "Epoch [25/25], Step [125/607], Loss: 0.0023\n",
            "Training accuracy 100.0 %\n",
            "Time: 595.0342741012573\n",
            "Epoch [25/25], Step [130/607], Loss: 0.0037\n",
            "Training accuracy 100.0 %\n",
            "Time: 595.2350015640259\n",
            "Epoch [25/25], Step [135/607], Loss: 0.0396\n",
            "Training accuracy 100.0 %\n",
            "Time: 595.4464828968048\n",
            "Epoch [25/25], Step [140/607], Loss: 0.0271\n",
            "Training accuracy 100.0 %\n",
            "Time: 595.6593675613403\n",
            "Epoch [25/25], Step [145/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 595.8500690460205\n",
            "Epoch [25/25], Step [150/607], Loss: 0.0031\n",
            "Training accuracy 100.0 %\n",
            "Time: 596.0575151443481\n",
            "Epoch [25/25], Step [155/607], Loss: 0.0003\n",
            "Training accuracy 100.0 %\n",
            "Time: 596.266622543335\n",
            "Epoch [25/25], Step [160/607], Loss: 0.0121\n",
            "Training accuracy 100.0 %\n",
            "Time: 596.459525346756\n",
            "Epoch [25/25], Step [165/607], Loss: 0.0033\n",
            "Training accuracy 100.0 %\n",
            "Time: 596.6669890880585\n",
            "Epoch [25/25], Step [170/607], Loss: 0.0023\n",
            "Training accuracy 100.0 %\n",
            "Time: 596.8587522506714\n",
            "Epoch [25/25], Step [175/607], Loss: 0.0031\n",
            "Training accuracy 100.0 %\n",
            "Time: 597.0473480224609\n",
            "Epoch [25/25], Step [180/607], Loss: 0.0065\n",
            "Training accuracy 100.0 %\n",
            "Time: 597.2580139636993\n",
            "Epoch [25/25], Step [185/607], Loss: 0.1102\n",
            "Training accuracy 93.75 %\n",
            "Time: 597.4512946605682\n",
            "Epoch [25/25], Step [190/607], Loss: 0.0018\n",
            "Training accuracy 100.0 %\n",
            "Time: 597.6604564189911\n",
            "Epoch [25/25], Step [195/607], Loss: 0.0062\n",
            "Training accuracy 100.0 %\n",
            "Time: 597.8516738414764\n",
            "Epoch [25/25], Step [200/607], Loss: 0.0015\n",
            "Training accuracy 100.0 %\n",
            "Time: 598.0396480560303\n",
            "Epoch [25/25], Step [205/607], Loss: 0.0024\n",
            "Training accuracy 100.0 %\n",
            "Time: 598.2174217700958\n",
            "Epoch [25/25], Step [210/607], Loss: 0.0032\n",
            "Training accuracy 100.0 %\n",
            "Time: 598.411783695221\n",
            "Epoch [25/25], Step [215/607], Loss: 0.0024\n",
            "Training accuracy 100.0 %\n",
            "Time: 598.6017687320709\n",
            "Epoch [25/25], Step [220/607], Loss: 0.0022\n",
            "Training accuracy 100.0 %\n",
            "Time: 598.7955884933472\n",
            "Epoch [25/25], Step [225/607], Loss: 0.0159\n",
            "Training accuracy 100.0 %\n",
            "Time: 598.9890117645264\n",
            "Epoch [25/25], Step [230/607], Loss: 0.0217\n",
            "Training accuracy 100.0 %\n",
            "Time: 599.1747028827667\n",
            "Epoch [25/25], Step [235/607], Loss: 0.0019\n",
            "Training accuracy 100.0 %\n",
            "Time: 599.3644022941589\n",
            "Epoch [25/25], Step [240/607], Loss: 0.0299\n",
            "Training accuracy 100.0 %\n",
            "Time: 599.5741484165192\n",
            "Epoch [25/25], Step [245/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 599.7497668266296\n",
            "Epoch [25/25], Step [250/607], Loss: 0.0049\n",
            "Training accuracy 100.0 %\n",
            "Time: 599.9531319141388\n",
            "Epoch [25/25], Step [255/607], Loss: 0.0032\n",
            "Training accuracy 100.0 %\n",
            "Time: 600.1585915088654\n",
            "Epoch [25/25], Step [260/607], Loss: 0.0242\n",
            "Training accuracy 100.0 %\n",
            "Time: 600.3806669712067\n",
            "Epoch [25/25], Step [265/607], Loss: 0.0009\n",
            "Training accuracy 100.0 %\n",
            "Time: 600.5954101085663\n",
            "Epoch [25/25], Step [270/607], Loss: 0.0002\n",
            "Training accuracy 100.0 %\n",
            "Time: 600.8287281990051\n",
            "Epoch [25/25], Step [275/607], Loss: 0.0011\n",
            "Training accuracy 100.0 %\n",
            "Time: 601.0382349491119\n",
            "Epoch [25/25], Step [280/607], Loss: 0.0185\n",
            "Training accuracy 100.0 %\n",
            "Time: 601.2229166030884\n",
            "Epoch [25/25], Step [285/607], Loss: 0.0308\n",
            "Training accuracy 100.0 %\n",
            "Time: 601.4180543422699\n",
            "Epoch [25/25], Step [290/607], Loss: 0.0256\n",
            "Training accuracy 100.0 %\n",
            "Time: 601.6006634235382\n",
            "Epoch [25/25], Step [295/607], Loss: 0.0056\n",
            "Training accuracy 100.0 %\n",
            "Time: 601.8057420253754\n",
            "Epoch [25/25], Step [300/607], Loss: 0.0069\n",
            "Training accuracy 100.0 %\n",
            "Time: 602.0045928955078\n",
            "Epoch [25/25], Step [305/607], Loss: 0.0070\n",
            "Training accuracy 100.0 %\n",
            "Time: 602.192200422287\n",
            "Epoch [25/25], Step [310/607], Loss: 0.0017\n",
            "Training accuracy 100.0 %\n",
            "Time: 602.3986003398895\n",
            "Epoch [25/25], Step [315/607], Loss: 0.0109\n",
            "Training accuracy 100.0 %\n",
            "Time: 602.6069536209106\n",
            "Epoch [25/25], Step [320/607], Loss: 0.0134\n",
            "Training accuracy 100.0 %\n",
            "Time: 602.8240184783936\n",
            "Epoch [25/25], Step [325/607], Loss: 0.0015\n",
            "Training accuracy 100.0 %\n",
            "Time: 603.0199692249298\n",
            "Epoch [25/25], Step [330/607], Loss: 0.0077\n",
            "Training accuracy 100.0 %\n",
            "Time: 603.2221004962921\n",
            "Epoch [25/25], Step [335/607], Loss: 0.0020\n",
            "Training accuracy 100.0 %\n",
            "Time: 603.4094548225403\n",
            "Epoch [25/25], Step [340/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 603.6171350479126\n",
            "Epoch [25/25], Step [345/607], Loss: 0.0168\n",
            "Training accuracy 100.0 %\n",
            "Time: 603.8068788051605\n",
            "Epoch [25/25], Step [350/607], Loss: 0.0066\n",
            "Training accuracy 100.0 %\n",
            "Time: 604.0286347866058\n",
            "Epoch [25/25], Step [355/607], Loss: 0.0006\n",
            "Training accuracy 100.0 %\n",
            "Time: 604.208039522171\n",
            "Epoch [25/25], Step [360/607], Loss: 0.0052\n",
            "Training accuracy 100.0 %\n",
            "Time: 604.3918218612671\n",
            "Epoch [25/25], Step [365/607], Loss: 0.0116\n",
            "Training accuracy 100.0 %\n",
            "Time: 604.59148645401\n",
            "Epoch [25/25], Step [370/607], Loss: 0.0012\n",
            "Training accuracy 100.0 %\n",
            "Time: 604.7948021888733\n",
            "Epoch [25/25], Step [375/607], Loss: 0.0022\n",
            "Training accuracy 100.0 %\n",
            "Time: 604.9929571151733\n",
            "Epoch [25/25], Step [380/607], Loss: 0.0037\n",
            "Training accuracy 100.0 %\n",
            "Time: 605.2259359359741\n",
            "Epoch [25/25], Step [385/607], Loss: 0.0078\n",
            "Training accuracy 100.0 %\n",
            "Time: 605.4141659736633\n",
            "Epoch [25/25], Step [390/607], Loss: 0.0051\n",
            "Training accuracy 100.0 %\n",
            "Time: 605.6160221099854\n",
            "Epoch [25/25], Step [395/607], Loss: 0.0458\n",
            "Training accuracy 100.0 %\n",
            "Time: 605.8292622566223\n",
            "Epoch [25/25], Step [400/607], Loss: 0.0007\n",
            "Training accuracy 100.0 %\n",
            "Time: 606.0218725204468\n",
            "Epoch [25/25], Step [405/607], Loss: 0.1095\n",
            "Training accuracy 93.75 %\n",
            "Time: 606.2341845035553\n",
            "Epoch [25/25], Step [410/607], Loss: 0.0019\n",
            "Training accuracy 100.0 %\n",
            "Time: 606.4632596969604\n",
            "Epoch [25/25], Step [415/607], Loss: 0.0015\n",
            "Training accuracy 100.0 %\n",
            "Time: 606.6755874156952\n",
            "Epoch [25/25], Step [420/607], Loss: 0.0005\n",
            "Training accuracy 100.0 %\n",
            "Time: 606.8772387504578\n",
            "Epoch [25/25], Step [425/607], Loss: 0.0036\n",
            "Training accuracy 100.0 %\n",
            "Time: 607.0705869197845\n",
            "Epoch [25/25], Step [430/607], Loss: 0.0038\n",
            "Training accuracy 100.0 %\n",
            "Time: 607.272750377655\n",
            "Epoch [25/25], Step [435/607], Loss: 0.0078\n",
            "Training accuracy 100.0 %\n",
            "Time: 607.4593904018402\n",
            "Epoch [25/25], Step [440/607], Loss: 0.0021\n",
            "Training accuracy 100.0 %\n",
            "Time: 607.6572136878967\n",
            "Epoch [25/25], Step [445/607], Loss: 0.0494\n",
            "Training accuracy 100.0 %\n",
            "Time: 607.8416707515717\n",
            "Epoch [25/25], Step [450/607], Loss: 0.0035\n",
            "Training accuracy 100.0 %\n",
            "Time: 608.0379762649536\n",
            "Epoch [25/25], Step [455/607], Loss: 0.0649\n",
            "Training accuracy 93.75 %\n",
            "Time: 608.2303400039673\n",
            "Epoch [25/25], Step [460/607], Loss: 0.0019\n",
            "Training accuracy 100.0 %\n",
            "Time: 608.4546949863434\n",
            "Epoch [25/25], Step [465/607], Loss: 0.0012\n",
            "Training accuracy 100.0 %\n",
            "Time: 608.6487965583801\n",
            "Epoch [25/25], Step [470/607], Loss: 0.0005\n",
            "Training accuracy 100.0 %\n",
            "Time: 608.8496341705322\n",
            "Epoch [25/25], Step [475/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 609.026255607605\n",
            "Epoch [25/25], Step [480/607], Loss: 0.0048\n",
            "Training accuracy 100.0 %\n",
            "Time: 609.2476241588593\n",
            "Epoch [25/25], Step [485/607], Loss: 0.0007\n",
            "Training accuracy 100.0 %\n",
            "Time: 609.435652256012\n",
            "Epoch [25/25], Step [490/607], Loss: 0.0041\n",
            "Training accuracy 100.0 %\n",
            "Time: 609.647036075592\n",
            "Epoch [25/25], Step [495/607], Loss: 0.0042\n",
            "Training accuracy 100.0 %\n",
            "Time: 609.9409925937653\n",
            "Epoch [25/25], Step [500/607], Loss: 0.0030\n",
            "Training accuracy 100.0 %\n",
            "Time: 610.1641952991486\n",
            "Epoch [25/25], Step [505/607], Loss: 0.0040\n",
            "Training accuracy 100.0 %\n",
            "Time: 610.391284942627\n",
            "Epoch [25/25], Step [510/607], Loss: 0.0160\n",
            "Training accuracy 100.0 %\n",
            "Time: 610.6695485115051\n",
            "Epoch [25/25], Step [515/607], Loss: 0.0013\n",
            "Training accuracy 100.0 %\n",
            "Time: 610.870477437973\n",
            "Epoch [25/25], Step [520/607], Loss: 0.0253\n",
            "Training accuracy 100.0 %\n",
            "Time: 611.1253955364227\n",
            "Epoch [25/25], Step [525/607], Loss: 0.0016\n",
            "Training accuracy 100.0 %\n",
            "Time: 611.2997992038727\n",
            "Epoch [25/25], Step [530/607], Loss: 0.0009\n",
            "Training accuracy 100.0 %\n",
            "Time: 611.5004889965057\n",
            "Epoch [25/25], Step [535/607], Loss: 0.0016\n",
            "Training accuracy 100.0 %\n",
            "Time: 611.7106399536133\n",
            "Epoch [25/25], Step [540/607], Loss: 0.0055\n",
            "Training accuracy 100.0 %\n",
            "Time: 611.92041015625\n",
            "Epoch [25/25], Step [545/607], Loss: 0.0264\n",
            "Training accuracy 100.0 %\n",
            "Time: 612.1280784606934\n",
            "Epoch [25/25], Step [550/607], Loss: 0.0103\n",
            "Training accuracy 100.0 %\n",
            "Time: 612.3500008583069\n",
            "Epoch [25/25], Step [555/607], Loss: 0.0041\n",
            "Training accuracy 100.0 %\n",
            "Time: 612.6007180213928\n",
            "Epoch [25/25], Step [560/607], Loss: 0.0066\n",
            "Training accuracy 100.0 %\n",
            "Time: 612.7896058559418\n",
            "Epoch [25/25], Step [565/607], Loss: 0.0009\n",
            "Training accuracy 100.0 %\n",
            "Time: 612.9680361747742\n",
            "Epoch [25/25], Step [570/607], Loss: 0.0012\n",
            "Training accuracy 100.0 %\n",
            "Time: 613.1538054943085\n",
            "Epoch [25/25], Step [575/607], Loss: 0.0077\n",
            "Training accuracy 100.0 %\n",
            "Time: 613.3637919425964\n",
            "Epoch [25/25], Step [580/607], Loss: 0.1719\n",
            "Training accuracy 93.75 %\n",
            "Time: 613.5900988578796\n",
            "Epoch [25/25], Step [585/607], Loss: 0.0006\n",
            "Training accuracy 100.0 %\n",
            "Time: 613.7998225688934\n",
            "Epoch [25/25], Step [590/607], Loss: 0.0029\n",
            "Training accuracy 100.0 %\n",
            "Time: 614.021101474762\n",
            "Epoch [25/25], Step [595/607], Loss: 0.0025\n",
            "Training accuracy 100.0 %\n",
            "Time: 614.2202253341675\n",
            "Epoch [25/25], Step [600/607], Loss: 0.0358\n",
            "Training accuracy 100.0 %\n",
            "Time: 614.4144222736359\n",
            "Epoch [25/25], Step [605/607], Loss: 0.0010\n",
            "Training accuracy 100.0 %\n",
            "Time: 614.6293911933899\n",
            "Starting validation\n",
            "Validation 5\n",
            "Validation 10\n",
            "Validation 15\n",
            "Validation 20\n",
            "Validation 25\n",
            "Validation 30\n",
            "Validation 35\n",
            "Validation 40\n",
            "Validation accuracy 96.2099 %\n",
            "Validation ROC Curve\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Threshold: 0.0244\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99       668\n",
            "           1       0.53      1.00      0.69        18\n",
            "\n",
            "    accuracy                           0.98       686\n",
            "   macro avg       0.76      0.99      0.84       686\n",
            "weighted avg       0.99      0.98      0.98       686\n",
            "\n",
            "Starting testing\n",
            "Testing 5\n",
            "Testing 10\n",
            "Testing 15\n",
            "Testing 20\n",
            "Testing 25\n",
            "Testing 30\n",
            "Testing 35\n",
            "Testing 40\n",
            "Testing accuracy 95.3353 %\n",
            "Testing ROC Curve\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using Validation Threshold: 0.0244\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99       672\n",
            "           1       0.48      1.00      0.65        14\n",
            "\n",
            "    accuracy                           0.98       686\n",
            "   macro avg       0.74      0.99      0.82       686\n",
            "weighted avg       0.99      0.98      0.98       686\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3y1txT0YgMg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}