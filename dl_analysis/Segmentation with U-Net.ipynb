{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Segmentation with U-Net.ipynb","provenance":[{"file_id":"1pXukL6EzYKcwWy3WgzGEpdx_0dIjqFHm","timestamp":1617409687667}],"collapsed_sections":[],"authorship_tag":"ABX9TyNtZPNrg28kcOHnM157za8R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"oYa1_N2RIg-q"},"source":["# Segmentation Model for scI Datasets\n","\n","This model performs segmentation on separation lanes that were classified as containing protein bands, from the classification model.\n","\n","We gratefully acknowledge Prof. Jonathan Shewchuk and the teaching assistants of the Spring 2019 offering of UC Berkeleyâ€™s CS289A course for helpful discussions and initial code for some of this work.  "]},{"cell_type":"markdown","metadata":{"id":"BGnuoKza0FBc"},"source":["\n","\n","# Notebook Initialization\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eebTKwi5wa3D","executionInfo":{"status":"ok","timestamp":1617412964757,"user_tz":420,"elapsed":275,"user":{"displayName":"Anjali Gopal","photoUrl":"","userId":"16177467827720494104"}},"outputId":"090b5e4b-ca45-4792-af8b-3871038bbbb4"},"source":["## Mounting to Google Colab\n","## Comment out if not using Colab\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","## Loading libraries\n","\n","# Classic libraries\n","import os\n","import numpy as np\n","import pandas as pd\n","import time\n","import matplotlib.pyplot as plt\n","\n","# Scikit learn libraries\n","from skimage import io, transform\n","from PIL import Image\n","from sklearn import metrics\n","\n","# Pytorch libraries\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms\n","import torchvision.models as models\n","from torchsummary import summary\n","\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ipIeSXuHOf8c","executionInfo":{"status":"ok","timestamp":1617412993878,"user_tz":420,"elapsed":281,"user":{"displayName":"Anjali Gopal","photoUrl":"","userId":"16177467827720494104"}}},"source":["## Dataset Files\n","\n","# Change this to the working directory\n","os.chdir('/content/gdrive/My Drive/Herrlab/projects/segmentationproject/experiments')\n","\n","# Change this to where the data is stored\n","dataset_path = '/content/gdrive/My Drive/Herrlab/projects/segmentationproject/datasets/mtl/'\n","\n","# Datasets\n","train_file = dataset_path+'unet_train.csv';\n","val_file = dataset_path+'unet_validate.csv';\n","test_file = dataset_path+'unet_test.csv';\n","\n","# Final model output\n","model_save_file = '/content/gdrive/My Drive/Herrlab/projects/segmentationproject/experiments/exp5-18/unet_github.ckpt'\n"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sm68-CMDJVtZ"},"source":["## Dataset Loading Functions"]},{"cell_type":"code","metadata":{"id":"cjLvfGMjFjw3","executionInfo":{"status":"ok","timestamp":1617412994780,"user_tz":420,"elapsed":258,"user":{"displayName":"Anjali Gopal","photoUrl":"","userId":"16177467827720494104"}}},"source":["def imgLoad(imagePath):\n","    '''\n","    Loads an image using the PIL Image library\n","    Converts it to int64 and to x-dim * y-dim * 1 \n","    to make its size compatible with pytorch tensors.\n","    '''\n","    \n","    image = np.array(Image.open(imagePath)).astype('int64');\n","    return image\n","\n","def div_n(x,n):\n","    '''\n","    Figures out how much padding needs to be added to\n","    x, in order to make it divisible by n \n","    ''' \n","    x = float(x);\n","    n = float(n);\n","\n","    if (x % n == 0):\n","      padding = 0;\n","\n","    else:\n","      x_padded = np.ceil(x/n) * n; \n","      padding = (x_padded-x)/2.0;\n","\n","    return(int(padding));\n","\n","class roiData(data.Dataset):\n","    ''' Object representing a dataset. Code adapted from\n","    CS289 HW6 assignment, Spring 2019 offering @ UC Berkeley.\n","\n","    This object stores an input data file. When called as roidata[i], \n","    then the object will return the sample name and label of the ith\n","    data point.\n","\n","    NOTE: PyTorch kept having issues with normalization, so I just decided\n","    to do it myself.\n","    '''   \n","    def __init__(self, label_file, img_load_function, transform=None, dataset_path=None, normalize=None,ndivisible=0):\n","        'Initialization'\n","\n","        self.label_file = label_file\n","        self.loader = img_load_function\n","        self.data = pd.read_csv(self.label_file,header=0)[['roiPath','segmentPath']]\n","        self.transform = transform;\n","        self.normalize = normalize; \n","\n","        # Option to add a path to the image files, if located in some other\n","        # folder\n","\n","        if dataset_path is not None:\n","          self.data['roiPath'] = dataset_path+self.data['roiPath'];\n","          self.data['segmentPath'] = dataset_path+self.data['segmentPath'];\n","\n","        # Getting padding info\n","\n","        if ndivisible is not 0:\n","          path,label = self.data.iloc[0];\n","          sample = self.loader(path);\n","          [x,y] = sample.shape;\n","\n","          x_padding = div_n(x,16);\n","          y_padding = div_n(y,16);\n","\n","          self.padding = [x_padding,y_padding];\n","          \n","        else:\n","          self.padding = [0,0];\n","\n","\n","    def __len__(self):\n","        'Denotes the total number of samples'\n","        return len(self.data)\n","\n","    def __getitem__(self,idx):\n","        'Generates one sample of data'\n","        path,label = self.data.iloc[idx]\n","        label = self.loader(label);\n","        sample = self.loader(path)\n","\n","        if self.normalize is not None:\n","          sample = (sample - self.normalize[0])/self.normalize[1];\n","\n","        # We should not be transforming the samples unless we want to \n","        # transform the labels as well... \n","        # Might be worth doing in the future. \n","        if self.transform is not None:\n","          sample = self.transform(sample);\n","          label = self.transform(label);\n","\n","        sample = F.pad(sample,pad=(self.padding[1],self.padding[1],self.padding[0],self.padding[0]));\n","        label = F.pad(label,pad=(self.padding[1],self.padding[1],self.padding[0],self.padding[0]));\n","\n","        return sample,label\n","\n"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wQzmg1yDNRkV"},"source":["# Normalization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ene16huVcdPR","executionInfo":{"status":"ok","timestamp":1617412996823,"user_tz":420,"elapsed":852,"user":{"displayName":"Anjali Gopal","photoUrl":"","userId":"16177467827720494104"}},"outputId":"d69b0298-026c-42da-84c5-35edd2f2b5ac"},"source":["batchSize = 10;\n","\n","tensor_transform_normalization = transforms.Compose([transforms.ToTensor()]);\n","train_dataset_normalization = roiData(train_file,imgLoad,transform=tensor_transform_normalization,dataset_path=dataset_path);\n","train_loader_normalization = data.DataLoader(train_dataset_normalization, batch_size = batchSize, shuffle = True, num_workers = 2);\n","\n","mean = [];\n","meansq = [];\n","sample_length = [];\n","\n","print(\"Training samples: {}\".format(len(train_dataset_normalization.data)));\n","\n","for sample, label in train_loader_normalization:\n","  mean.append(np.mean(np.array(sample)))\n","  meansq.append(np.mean(np.array(sample**2)))\n","  sample_length.append(len(label))\n","\n","batch_p = np.array(sample_length)/len(train_dataset_normalization);\n","\n","sample_mean = np.sum(batch_p*np.array(mean));\n","sample_var = np.sum(batch_p*np.array(meansq)) - (sample_mean ** 2);\n","sample_std = np.sqrt(sample_var);\n","\n","print(\"Sample mean: {}\".format(sample_mean))\n","print(\"Sample stdev: {}\".format(sample_std))\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Training samples: 153\n","Sample mean: 13502.82817769608\n","Sample stdev: 8676.41096301769\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7NHbvEENSFyj"},"source":["## Loading Data"]},{"cell_type":"code","metadata":{"id":"97dTXASrSGZ-","executionInfo":{"status":"ok","timestamp":1617413783180,"user_tz":420,"elapsed":272,"user":{"displayName":"Anjali Gopal","photoUrl":"","userId":"16177467827720494104"}}},"source":["# Dataset transformations for training\n","train_transform = transforms.Compose([\n","                                       transforms.ToTensor(),\n","                                       transforms.RandomHorizontalFlip()\n","                                       ]);\n","\n","# Dataset transformation into tensor\n","tensor_transform = transforms.Compose([transforms.ToTensor()]);\n","\n","\n","# Number of workers\n","workers = 2; \n","\n","# Batch size\n","batchSize = 16;\n","\n","train_dataset = roiData(train_file,imgLoad,transform=tensor_transform,dataset_path=dataset_path,normalize=[sample_mean,sample_std],ndivisible=16);\n","test_dataset = roiData(test_file,imgLoad,transform=tensor_transform,dataset_path=dataset_path,normalize=[sample_mean,sample_std],ndivisible=16);\n","val_dataset = roiData(val_file,imgLoad,transform=tensor_transform,dataset_path=dataset_path,normalize=[sample_mean,sample_std],ndivisible=16);\n","\n","train_loader = data.DataLoader(train_dataset, batch_size = batchSize, shuffle = True, num_workers = workers);\n","test_loader = data.DataLoader(test_dataset, batch_size = batchSize, shuffle = False, num_workers = workers);\n","val_loader = data.DataLoader(val_dataset, batch_size = batchSize, shuffle = False, num_workers = workers);\n"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fB81CSgkNW9F"},"source":["# Model Tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_rFakKxRJYY","executionInfo":{"status":"ok","timestamp":1611294878544,"user_tz":480,"elapsed":1002,"user":{"displayName":"Anjali Gopal","photoUrl":"","userId":"16177467827720494104"}},"outputId":"d482a576-2c8d-4741-c5f6-1e627eec61a1"},"source":["# This is just a check to make sure it properly loaded\n","train_dataset = roiData(train_file,imgLoad,transform=None,dataset_path=dataset_path);\n","print(\"Training dataset length: \"+str(len(train_dataset)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training dataset length: 153\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N1y_S7DQ0Hcu","executionInfo":{"status":"ok","timestamp":1617413015683,"user_tz":420,"elapsed":11223,"user":{"displayName":"Anjali Gopal","photoUrl":"","userId":"16177467827720494104"}}},"source":["## Model Hyperparameters\n","\n","## Hyperparameters\n","\n","# Number of epochs\n","num_epochs = 100;\n","\n","# Learning Rate\n","learning_rate = 1E-5\n","\n","# Loss\n","criterion = nn.CrossEntropyLoss()\n","\n","# Model\n","\n","class NeuralNet(nn.Module):\n","  def convblock(self,input_layers,output_layers):\n","    block = nn.Sequential(\n","        nn.Conv2d(input_layers,output_layers,kernel_size=3, stride = 1, padding=1),\n","        nn.ReLU(),\n","        nn.Conv2d(output_layers,output_layers,kernel_size=3, stride = 1, padding=1),\n","        nn.ReLU()\n","    );\n","    return(block);\n","\n","  def copy_and_crop(self,x,x_skip):\n","    '''\n","    Ensures that the copy and crop step produces and output of a \n","    suitable shape. \n","    '''\n","\n","    x_pad = (x_skip.shape[2] - x.shape[2]);\n","    y_pad = (x_skip.shape[3] - x.shape[3]);\n","\n","    if (x_pad % 2 == 0): #if even\n","      x_pad_left = int(x_pad/2);\n","      x_pad_right = int(x_pad/2);\n","    else: \n","      x_pad_left = int(x_pad/2)+1;\n","      x_pad_right = int(x_pad/2);\n","\n","    if (y_pad % 2 == 0): #if even\n","      y_pad_left = int(y_pad/2);\n","      y_pad_right = int(y_pad/2);\n","    else: \n","      y_pad_left = int(y_pad/2);\n","      y_pad_right = int(y_pad/2)-1;\n","\n","    x_skip_cropped = F.pad(x_skip,pad=(-y_pad_left,-y_pad_right,-x_pad_left,-x_pad_right,0,0,0,0));\n","\n","    final_tensor = torch.cat((x_skip_cropped,x),dim=1); \n","\n","    return(final_tensor)\n","      \n","  def __init__(self):\n","      super(NeuralNet, self).__init__()\n","\n","      self.pool = nn.MaxPool2d(2,2);\n","\n","      self.d1 = self.convblock(1,64);\n","      self.d2 = self.convblock(64,128); # Layer 2\n","      self.d3 = self.convblock(128,256); # Layer 3\n","      self.d4 = self.convblock(256,512); # Layer 4\n","      self.d5 = self.convblock(512,1024); # Layer 5\n","\n","      self.u6 = self.convblock(1024,512); # Layer 6\n","      self.u7 = self.convblock(512,256); # Layer 7\n","      self.u8 = self.convblock(256,128); # Layer 8\n","      self.u9 = self.convblock(128,64); # Layer 9\n","\n","      self.upconv5 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2, padding=0)\n","      self.upconv6 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2, padding=0)\n","      self.upconv7 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2, padding=0)\n","      self.upconv8 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2, padding=0)\n","      \n","      self.final_cov = nn.Conv2d(64,2,kernel_size=1,stride=1,padding=0);\n","\n","  def forward(self, x):\n","\n","      # Left Network\n","      x1 = self.d1(x);    \n","      x1_pool = self.pool(x1);\n","\n","      x2 = self.d2(x1_pool);\n","      x2_pool = self.pool(x2);\n","\n","      x3 = self.d3(x2_pool);\n","      x3_pool = self.pool(x3);\n","\n","      x4 = self.d4(x3_pool);\n","      x4_pool = self.pool(x4);\n","   \n","      x5 = self.d5(x4_pool);        \n","      x5_upconv = self.upconv5(x5);\n","\n","      # Right network   \n","      x6_tensor = self.copy_and_crop(x5_upconv,x4);\n","      x6 = self.u6(x6_tensor);\n","      x6_upconv = self.upconv6(x6);\n","\n","      x7_tensor = self.copy_and_crop(x6_upconv,x3);\n","      x7 = self.u7(x7_tensor);\n","      x7_upconv = self.upconv7(x7);\n","\n","      x8_tensor = self.copy_and_crop(x7_upconv,x2);\n","      x8 = self.u8(x8_tensor);\n","      x8_upconv = self.upconv8(x8);\n","\n","      x9_tensor = self.copy_and_crop(x8_upconv,x1);\n","      x9 = self.u9(x9_tensor);\n","      x_final = self.final_cov(x9);\n","\n","      return(x_final)\n","\n","# CUDA for PyTorch\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","model = NeuralNet().to(device)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uP3JNqJGVUfn"},"source":["## Running the Model"]},{"cell_type":"code","metadata":{"id":"56hHX1F0_Njs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617414079724,"user_tz":420,"elapsed":261017,"user":{"displayName":"Anjali Gopal","photoUrl":"","userId":"16177467827720494104"}},"outputId":"9d6805e8-7cab-46f3-e5dc-1be0b317ed69"},"source":["##################################\n","#                                #\n","#            TRAINING            #\n","#                                #\n","##################################\n","\n","# Looping over epochs\n","\n","\n","# Initializing variables\n","training_loss = [];\n","training_predictions = [];\n","training_labels = [];\n","training_accuracy = [];\n","training_micrographs = [];\n","\n","print('Beginning training..')\n","total_step = len(train_loader) # To calculate total number of steps. \n","start = time.time();\n","\n","for epoch in np.arange(num_epochs):\n","  \n","    # Batch training\n","    model.train()  \n","    print('epoch {}'.format(epoch+1))\n","\n","    for i, (local_batch,local_labels) in enumerate(train_loader):\n","        local_batch = local_batch.float();\n","\n","        # Transfer to GPU\n","        local_ims, local_labels = local_batch.to(device), local_labels.to(device)  \n","\n","        # Forward pass\n","        outputs = model.forward(local_ims)\n","\n","        # Reshaping for loss\n","\n","        if epoch == 0:\n","          x = local_labels.shape[-1];\n","          y = local_labels.shape[-2];\n","\n","        local_batch_size = len(local_labels);\n","        \n","        # Loss\n","        local_labels_for_loss = torch.reshape(local_labels,(local_batch_size,x*y));\n","        outputs_for_loss = torch.reshape(outputs,(local_batch_size,outputs.shape[1],x*y))\n","\n","        loss = criterion(outputs_for_loss, local_labels_for_loss)\n","        training_loss.append(loss.tolist())\n","\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        # Accuracy score\n","        predicted_reshape = torch.reshape(predicted,(local_batch_size*x*y,1)).cpu().numpy();\n","        local_labels_for_score = torch.reshape(local_labels_for_loss,(local_batch_size*x*y,1)).cpu().numpy();\n","\n","        score = metrics.accuracy_score(predicted_reshape,local_labels_for_score);\n","        training_accuracy.append(score)\n","\n","        # If last epoch, save the predictions\n","        if epoch == num_epochs-1:\n","          training_predictions.append(predicted.cpu().numpy());\n","          training_labels.append(local_labels.cpu().numpy()); \n","          training_micrographs.append(local_ims.cpu().numpy());\n","        \n","        # Backward pass and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Printing results\n","        if (i+1) % 5 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","            print('Training accuracy {:.4f} %'.format(100*score))\n","            print('Time: '+str(time.time()-start))\n","\n","\n","torch.save(model.state_dict(), model_save_file)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Beginning training..\n","epoch 1\n","Epoch [1/100], Step [5/10], Loss: 0.3712\n","Training accuracy 89.5466 %\n","Time: 1.4224250316619873\n","Epoch [1/100], Step [10/10], Loss: 0.3596\n","Training accuracy 89.8571 %\n","Time: 2.5181751251220703\n","epoch 2\n","Epoch [2/100], Step [5/10], Loss: 0.3399\n","Training accuracy 89.0155 %\n","Time: 3.907350778579712\n","Epoch [2/100], Step [10/10], Loss: 0.3319\n","Training accuracy 89.0905 %\n","Time: 5.008713245391846\n","epoch 3\n","Epoch [3/100], Step [5/10], Loss: 0.3030\n","Training accuracy 89.2097 %\n","Time: 6.4027228355407715\n","Epoch [3/100], Step [10/10], Loss: 0.2414\n","Training accuracy 90.9014 %\n","Time: 7.505269765853882\n","epoch 4\n","Epoch [4/100], Step [5/10], Loss: 0.2525\n","Training accuracy 89.5940 %\n","Time: 8.907092332839966\n","Epoch [4/100], Step [10/10], Loss: 0.2532\n","Training accuracy 87.7457 %\n","Time: 10.023545742034912\n","epoch 5\n","Epoch [5/100], Step [5/10], Loss: 0.2188\n","Training accuracy 89.1241 %\n","Time: 11.471235752105713\n","Epoch [5/100], Step [10/10], Loss: 0.2058\n","Training accuracy 89.7983 %\n","Time: 12.587789297103882\n","epoch 6\n","Epoch [6/100], Step [5/10], Loss: 0.2054\n","Training accuracy 89.3945 %\n","Time: 13.985857963562012\n","Epoch [6/100], Step [10/10], Loss: 0.1929\n","Training accuracy 87.7951 %\n","Time: 15.10321569442749\n","epoch 7\n","Epoch [7/100], Step [5/10], Loss: 0.1783\n","Training accuracy 90.2389 %\n","Time: 16.51835060119629\n","Epoch [7/100], Step [10/10], Loss: 0.1998\n","Training accuracy 87.8666 %\n","Time: 17.633864641189575\n","epoch 8\n","Epoch [8/100], Step [5/10], Loss: 0.1646\n","Training accuracy 89.4926 %\n","Time: 19.045557022094727\n","Epoch [8/100], Step [10/10], Loss: 0.1724\n","Training accuracy 90.3339 %\n","Time: 20.168774127960205\n","epoch 9\n","Epoch [9/100], Step [5/10], Loss: 0.1795\n","Training accuracy 89.1158 %\n","Time: 21.592409372329712\n","Epoch [9/100], Step [10/10], Loss: 0.1910\n","Training accuracy 89.8411 %\n","Time: 22.720829010009766\n","epoch 10\n","Epoch [10/100], Step [5/10], Loss: 0.1630\n","Training accuracy 90.4586 %\n","Time: 24.158980131149292\n","Epoch [10/100], Step [10/10], Loss: 0.1728\n","Training accuracy 93.3066 %\n","Time: 25.28656029701233\n","epoch 11\n","Epoch [11/100], Step [5/10], Loss: 0.1610\n","Training accuracy 92.6705 %\n","Time: 26.71026086807251\n","Epoch [11/100], Step [10/10], Loss: 0.1678\n","Training accuracy 90.8941 %\n","Time: 27.841123819351196\n","epoch 12\n","Epoch [12/100], Step [5/10], Loss: 0.1633\n","Training accuracy 91.6534 %\n","Time: 29.277556896209717\n","Epoch [12/100], Step [10/10], Loss: 0.1343\n","Training accuracy 91.8129 %\n","Time: 30.415322065353394\n","epoch 13\n","Epoch [13/100], Step [5/10], Loss: 0.1482\n","Training accuracy 91.8307 %\n","Time: 31.860414266586304\n","Epoch [13/100], Step [10/10], Loss: 0.1508\n","Training accuracy 90.7318 %\n","Time: 32.9965283870697\n","epoch 14\n","Epoch [14/100], Step [5/10], Loss: 0.1526\n","Training accuracy 92.0718 %\n","Time: 34.419275999069214\n","Epoch [14/100], Step [10/10], Loss: 0.1778\n","Training accuracy 92.1848 %\n","Time: 35.55847358703613\n","epoch 15\n","Epoch [15/100], Step [5/10], Loss: 0.1186\n","Training accuracy 94.8539 %\n","Time: 36.99384808540344\n","Epoch [15/100], Step [10/10], Loss: 0.1528\n","Training accuracy 93.6351 %\n","Time: 38.137057304382324\n","epoch 16\n","Epoch [16/100], Step [5/10], Loss: 0.1608\n","Training accuracy 92.9507 %\n","Time: 39.55302572250366\n","Epoch [16/100], Step [10/10], Loss: 0.1601\n","Training accuracy 92.5007 %\n","Time: 40.70637392997742\n","epoch 17\n","Epoch [17/100], Step [5/10], Loss: 0.1622\n","Training accuracy 93.3196 %\n","Time: 42.150946378707886\n","Epoch [17/100], Step [10/10], Loss: 0.1156\n","Training accuracy 94.7516 %\n","Time: 43.29895496368408\n","epoch 18\n","Epoch [18/100], Step [5/10], Loss: 0.1600\n","Training accuracy 91.6188 %\n","Time: 44.78202962875366\n","Epoch [18/100], Step [10/10], Loss: 0.1227\n","Training accuracy 93.9984 %\n","Time: 45.93801522254944\n","epoch 19\n","Epoch [19/100], Step [5/10], Loss: 0.1339\n","Training accuracy 93.5408 %\n","Time: 47.414305686950684\n","Epoch [19/100], Step [10/10], Loss: 0.1088\n","Training accuracy 95.9729 %\n","Time: 48.57393455505371\n","epoch 20\n","Epoch [20/100], Step [5/10], Loss: 0.1173\n","Training accuracy 94.9880 %\n","Time: 50.03601360321045\n","Epoch [20/100], Step [10/10], Loss: 0.1169\n","Training accuracy 94.6100 %\n","Time: 51.194549322128296\n","epoch 21\n","Epoch [21/100], Step [5/10], Loss: 0.1171\n","Training accuracy 95.5146 %\n","Time: 52.65408134460449\n","Epoch [21/100], Step [10/10], Loss: 0.1080\n","Training accuracy 97.2670 %\n","Time: 53.81745767593384\n","epoch 22\n","Epoch [22/100], Step [5/10], Loss: 0.1294\n","Training accuracy 95.8252 %\n","Time: 55.264169454574585\n","Epoch [22/100], Step [10/10], Loss: 0.1312\n","Training accuracy 96.2186 %\n","Time: 56.421860218048096\n","epoch 23\n","Epoch [23/100], Step [5/10], Loss: 0.1040\n","Training accuracy 95.8947 %\n","Time: 57.90188670158386\n","Epoch [23/100], Step [10/10], Loss: 0.0998\n","Training accuracy 96.6346 %\n","Time: 59.0693142414093\n","epoch 24\n","Epoch [24/100], Step [5/10], Loss: 0.0777\n","Training accuracy 97.0290 %\n","Time: 60.56019902229309\n","Epoch [24/100], Step [10/10], Loss: 0.0751\n","Training accuracy 97.5040 %\n","Time: 61.73155641555786\n","epoch 25\n","Epoch [25/100], Step [5/10], Loss: 0.0919\n","Training accuracy 96.0066 %\n","Time: 63.18464946746826\n","Epoch [25/100], Step [10/10], Loss: 0.1160\n","Training accuracy 94.9219 %\n","Time: 64.36200046539307\n","epoch 26\n","Epoch [26/100], Step [5/10], Loss: 0.0837\n","Training accuracy 96.7578 %\n","Time: 65.80764985084534\n","Epoch [26/100], Step [10/10], Loss: 0.0721\n","Training accuracy 97.1321 %\n","Time: 66.97790002822876\n","epoch 27\n","Epoch [27/100], Step [5/10], Loss: 0.0762\n","Training accuracy 97.0185 %\n","Time: 68.45590162277222\n","Epoch [27/100], Step [10/10], Loss: 0.0698\n","Training accuracy 97.5761 %\n","Time: 69.62165451049805\n","epoch 28\n","Epoch [28/100], Step [5/10], Loss: 0.0746\n","Training accuracy 97.0106 %\n","Time: 71.08739376068115\n","Epoch [28/100], Step [10/10], Loss: 0.0960\n","Training accuracy 96.1238 %\n","Time: 72.25107550621033\n","epoch 29\n","Epoch [29/100], Step [5/10], Loss: 0.0694\n","Training accuracy 97.4621 %\n","Time: 73.72687315940857\n","Epoch [29/100], Step [10/10], Loss: 0.0689\n","Training accuracy 97.1942 %\n","Time: 74.89114546775818\n","epoch 30\n","Epoch [30/100], Step [5/10], Loss: 0.0667\n","Training accuracy 97.3982 %\n","Time: 76.34763979911804\n","Epoch [30/100], Step [10/10], Loss: 0.0796\n","Training accuracy 96.8429 %\n","Time: 77.5089180469513\n","epoch 31\n","Epoch [31/100], Step [5/10], Loss: 0.0769\n","Training accuracy 96.9467 %\n","Time: 78.97090172767639\n","Epoch [31/100], Step [10/10], Loss: 0.0718\n","Training accuracy 97.1448 %\n","Time: 80.1279706954956\n","epoch 32\n","Epoch [32/100], Step [5/10], Loss: 0.0909\n","Training accuracy 96.3785 %\n","Time: 81.58605289459229\n","Epoch [32/100], Step [10/10], Loss: 0.0720\n","Training accuracy 97.1835 %\n","Time: 82.73812007904053\n","epoch 33\n","Epoch [33/100], Step [5/10], Loss: 0.0818\n","Training accuracy 96.9663 %\n","Time: 84.18693971633911\n","Epoch [33/100], Step [10/10], Loss: 0.0739\n","Training accuracy 96.9358 %\n","Time: 85.35005950927734\n","epoch 34\n","Epoch [34/100], Step [5/10], Loss: 0.0666\n","Training accuracy 97.3227 %\n","Time: 86.80588984489441\n","Epoch [34/100], Step [10/10], Loss: 0.0772\n","Training accuracy 96.8697 %\n","Time: 87.96840953826904\n","epoch 35\n","Epoch [35/100], Step [5/10], Loss: 0.0610\n","Training accuracy 97.5875 %\n","Time: 89.39485573768616\n","Epoch [35/100], Step [10/10], Loss: 0.0841\n","Training accuracy 96.5238 %\n","Time: 90.54841661453247\n","epoch 36\n","Epoch [36/100], Step [5/10], Loss: 0.0733\n","Training accuracy 97.1785 %\n","Time: 91.98333144187927\n","Epoch [36/100], Step [10/10], Loss: 0.0994\n","Training accuracy 96.4870 %\n","Time: 93.13099575042725\n","epoch 37\n","Epoch [37/100], Step [5/10], Loss: 0.0783\n","Training accuracy 96.8878 %\n","Time: 94.57643985748291\n","Epoch [37/100], Step [10/10], Loss: 0.0558\n","Training accuracy 97.6302 %\n","Time: 95.7286229133606\n","epoch 38\n","Epoch [38/100], Step [5/10], Loss: 0.0632\n","Training accuracy 97.4106 %\n","Time: 97.15633463859558\n","Epoch [38/100], Step [10/10], Loss: 0.0824\n","Training accuracy 96.8042 %\n","Time: 98.30353307723999\n","epoch 39\n","Epoch [39/100], Step [5/10], Loss: 0.0785\n","Training accuracy 96.8311 %\n","Time: 99.74463605880737\n","Epoch [39/100], Step [10/10], Loss: 0.0607\n","Training accuracy 97.8739 %\n","Time: 100.8905999660492\n","epoch 40\n","Epoch [40/100], Step [5/10], Loss: 0.0727\n","Training accuracy 97.1244 %\n","Time: 102.34083390235901\n","Epoch [40/100], Step [10/10], Loss: 0.0759\n","Training accuracy 96.9738 %\n","Time: 103.48842930793762\n","epoch 41\n","Epoch [41/100], Step [5/10], Loss: 0.0783\n","Training accuracy 96.6117 %\n","Time: 104.94345951080322\n","Epoch [41/100], Step [10/10], Loss: 0.0590\n","Training accuracy 97.5728 %\n","Time: 106.08992886543274\n","epoch 42\n","Epoch [42/100], Step [5/10], Loss: 0.0603\n","Training accuracy 97.5781 %\n","Time: 107.56162548065186\n","Epoch [42/100], Step [10/10], Loss: 0.0894\n","Training accuracy 96.6814 %\n","Time: 108.7109911441803\n","epoch 43\n","Epoch [43/100], Step [5/10], Loss: 0.0572\n","Training accuracy 97.8268 %\n","Time: 110.17387175559998\n","Epoch [43/100], Step [10/10], Loss: 0.0516\n","Training accuracy 97.9093 %\n","Time: 111.33030819892883\n","epoch 44\n","Epoch [44/100], Step [5/10], Loss: 0.0501\n","Training accuracy 98.0600 %\n","Time: 112.7900161743164\n","Epoch [44/100], Step [10/10], Loss: 0.0681\n","Training accuracy 97.2997 %\n","Time: 113.94026398658752\n","epoch 45\n","Epoch [45/100], Step [5/10], Loss: 0.0611\n","Training accuracy 97.5740 %\n","Time: 115.37788581848145\n","Epoch [45/100], Step [10/10], Loss: 0.0797\n","Training accuracy 96.6914 %\n","Time: 116.52458643913269\n","epoch 46\n","Epoch [46/100], Step [5/10], Loss: 0.0599\n","Training accuracy 97.5744 %\n","Time: 117.99139547348022\n","Epoch [46/100], Step [10/10], Loss: 0.0792\n","Training accuracy 97.1848 %\n","Time: 119.14730405807495\n","epoch 47\n","Epoch [47/100], Step [5/10], Loss: 0.0642\n","Training accuracy 97.5627 %\n","Time: 120.58501935005188\n","Epoch [47/100], Step [10/10], Loss: 0.0594\n","Training accuracy 97.6349 %\n","Time: 121.74883604049683\n","epoch 48\n","Epoch [48/100], Step [5/10], Loss: 0.0741\n","Training accuracy 97.1548 %\n","Time: 123.21631717681885\n","Epoch [48/100], Step [10/10], Loss: 0.0714\n","Training accuracy 97.2449 %\n","Time: 124.37401247024536\n","epoch 49\n","Epoch [49/100], Step [5/10], Loss: 0.0644\n","Training accuracy 97.2746 %\n","Time: 125.81622099876404\n","Epoch [49/100], Step [10/10], Loss: 0.0987\n","Training accuracy 95.9622 %\n","Time: 126.97358226776123\n","epoch 50\n","Epoch [50/100], Step [5/10], Loss: 0.0641\n","Training accuracy 97.6085 %\n","Time: 128.41611504554749\n","Epoch [50/100], Step [10/10], Loss: 0.0464\n","Training accuracy 97.9494 %\n","Time: 129.57166266441345\n","epoch 51\n","Epoch [51/100], Step [5/10], Loss: 0.0692\n","Training accuracy 97.1064 %\n","Time: 131.03649854660034\n","Epoch [51/100], Step [10/10], Loss: 0.0879\n","Training accuracy 96.4784 %\n","Time: 132.20358037948608\n","epoch 52\n","Epoch [52/100], Step [5/10], Loss: 0.0677\n","Training accuracy 97.3516 %\n","Time: 133.65283966064453\n","Epoch [52/100], Step [10/10], Loss: 0.0540\n","Training accuracy 97.8706 %\n","Time: 134.8147692680359\n","epoch 53\n","Epoch [53/100], Step [5/10], Loss: 0.0566\n","Training accuracy 97.8039 %\n","Time: 136.2466413974762\n","Epoch [53/100], Step [10/10], Loss: 0.0454\n","Training accuracy 98.2298 %\n","Time: 137.40943694114685\n","epoch 54\n","Epoch [54/100], Step [5/10], Loss: 0.0719\n","Training accuracy 97.2637 %\n","Time: 138.85768842697144\n","Epoch [54/100], Step [10/10], Loss: 0.0500\n","Training accuracy 97.9287 %\n","Time: 140.02133107185364\n","epoch 55\n","Epoch [55/100], Step [5/10], Loss: 0.0515\n","Training accuracy 97.9289 %\n","Time: 141.46957302093506\n","Epoch [55/100], Step [10/10], Loss: 0.0563\n","Training accuracy 97.5721 %\n","Time: 142.63628816604614\n","epoch 56\n","Epoch [56/100], Step [5/10], Loss: 0.0722\n","Training accuracy 96.9223 %\n","Time: 144.07430601119995\n","Epoch [56/100], Step [10/10], Loss: 0.0639\n","Training accuracy 97.5367 %\n","Time: 145.2309935092926\n","epoch 57\n","Epoch [57/100], Step [5/10], Loss: 0.0587\n","Training accuracy 97.5954 %\n","Time: 146.70748114585876\n","Epoch [57/100], Step [10/10], Loss: 0.0644\n","Training accuracy 97.3498 %\n","Time: 147.87887001037598\n","epoch 58\n","Epoch [58/100], Step [5/10], Loss: 0.0549\n","Training accuracy 97.7287 %\n","Time: 149.3171260356903\n","Epoch [58/100], Step [10/10], Loss: 0.0638\n","Training accuracy 97.5781 %\n","Time: 150.47135663032532\n","epoch 59\n","Epoch [59/100], Step [5/10], Loss: 0.0534\n","Training accuracy 97.8309 %\n","Time: 151.9246997833252\n","Epoch [59/100], Step [10/10], Loss: 0.0605\n","Training accuracy 97.6035 %\n","Time: 153.08390498161316\n","epoch 60\n","Epoch [60/100], Step [5/10], Loss: 0.0701\n","Training accuracy 97.2476 %\n","Time: 154.54042744636536\n","Epoch [60/100], Step [10/10], Loss: 0.0552\n","Training accuracy 97.9874 %\n","Time: 155.7023148536682\n","epoch 61\n","Epoch [61/100], Step [5/10], Loss: 0.0688\n","Training accuracy 97.0910 %\n","Time: 157.15983200073242\n","Epoch [61/100], Step [10/10], Loss: 0.0487\n","Training accuracy 97.8626 %\n","Time: 158.31918144226074\n","epoch 62\n","Epoch [62/100], Step [5/10], Loss: 0.0549\n","Training accuracy 97.5796 %\n","Time: 159.7811794281006\n","Epoch [62/100], Step [10/10], Loss: 0.0551\n","Training accuracy 97.7310 %\n","Time: 160.94212818145752\n","epoch 63\n","Epoch [63/100], Step [5/10], Loss: 0.0553\n","Training accuracy 97.7385 %\n","Time: 162.40627098083496\n","Epoch [63/100], Step [10/10], Loss: 0.0470\n","Training accuracy 97.8966 %\n","Time: 163.56791019439697\n","epoch 64\n","Epoch [64/100], Step [5/10], Loss: 0.0690\n","Training accuracy 97.0876 %\n","Time: 165.0256564617157\n","Epoch [64/100], Step [10/10], Loss: 0.0645\n","Training accuracy 97.4760 %\n","Time: 166.18135237693787\n","epoch 65\n","Epoch [65/100], Step [5/10], Loss: 0.0434\n","Training accuracy 98.2238 %\n","Time: 167.64151644706726\n","Epoch [65/100], Step [10/10], Loss: 0.0469\n","Training accuracy 98.1624 %\n","Time: 168.79916977882385\n","epoch 66\n","Epoch [66/100], Step [5/10], Loss: 0.0553\n","Training accuracy 97.7317 %\n","Time: 170.22594571113586\n","Epoch [66/100], Step [10/10], Loss: 0.0616\n","Training accuracy 97.4379 %\n","Time: 171.38705205917358\n","epoch 67\n","Epoch [67/100], Step [5/10], Loss: 0.0645\n","Training accuracy 97.1976 %\n","Time: 172.82982516288757\n","Epoch [67/100], Step [10/10], Loss: 0.0690\n","Training accuracy 97.4753 %\n","Time: 173.9845826625824\n","epoch 68\n","Epoch [68/100], Step [5/10], Loss: 0.0548\n","Training accuracy 97.6262 %\n","Time: 175.42209839820862\n","Epoch [68/100], Step [10/10], Loss: 0.0463\n","Training accuracy 98.1550 %\n","Time: 176.57788014411926\n","epoch 69\n","Epoch [69/100], Step [5/10], Loss: 0.0506\n","Training accuracy 97.8662 %\n","Time: 178.02921795845032\n","Epoch [69/100], Step [10/10], Loss: 0.0794\n","Training accuracy 96.8623 %\n","Time: 179.18816661834717\n","epoch 70\n","Epoch [70/100], Step [5/10], Loss: 0.0642\n","Training accuracy 97.3712 %\n","Time: 180.66086530685425\n","Epoch [70/100], Step [10/10], Loss: 0.0530\n","Training accuracy 97.7878 %\n","Time: 181.81533861160278\n","epoch 71\n","Epoch [71/100], Step [5/10], Loss: 0.0698\n","Training accuracy 96.9738 %\n","Time: 183.27146220207214\n","Epoch [71/100], Step [10/10], Loss: 0.0527\n","Training accuracy 97.8025 %\n","Time: 184.42966151237488\n","epoch 72\n","Epoch [72/100], Step [5/10], Loss: 0.0429\n","Training accuracy 98.3752 %\n","Time: 185.86530780792236\n","Epoch [72/100], Step [10/10], Loss: 0.0545\n","Training accuracy 97.8392 %\n","Time: 187.0154469013214\n","epoch 73\n","Epoch [73/100], Step [5/10], Loss: 0.0527\n","Training accuracy 97.7779 %\n","Time: 188.47184705734253\n","Epoch [73/100], Step [10/10], Loss: 0.0679\n","Training accuracy 97.0025 %\n","Time: 189.62879037857056\n","epoch 74\n","Epoch [74/100], Step [5/10], Loss: 0.0477\n","Training accuracy 98.0277 %\n","Time: 191.06977796554565\n","Epoch [74/100], Step [10/10], Loss: 0.0502\n","Training accuracy 97.9674 %\n","Time: 192.22326183319092\n","epoch 75\n","Epoch [75/100], Step [5/10], Loss: 0.0486\n","Training accuracy 98.0495 %\n","Time: 193.70076704025269\n","Epoch [75/100], Step [10/10], Loss: 0.0416\n","Training accuracy 98.3060 %\n","Time: 194.85550332069397\n","epoch 76\n","Epoch [76/100], Step [5/10], Loss: 0.0591\n","Training accuracy 97.6236 %\n","Time: 196.30124378204346\n","Epoch [76/100], Step [10/10], Loss: 0.0505\n","Training accuracy 97.8466 %\n","Time: 197.45725512504578\n","epoch 77\n","Epoch [77/100], Step [5/10], Loss: 0.0403\n","Training accuracy 98.2824 %\n","Time: 198.92159271240234\n","Epoch [77/100], Step [10/10], Loss: 0.0678\n","Training accuracy 97.0686 %\n","Time: 200.0861461162567\n","epoch 78\n","Epoch [78/100], Step [5/10], Loss: 0.0504\n","Training accuracy 97.8737 %\n","Time: 201.529137134552\n","Epoch [78/100], Step [10/10], Loss: 0.0538\n","Training accuracy 97.7330 %\n","Time: 202.6890377998352\n","epoch 79\n","Epoch [79/100], Step [5/10], Loss: 0.0480\n","Training accuracy 97.8794 %\n","Time: 204.14299535751343\n","Epoch [79/100], Step [10/10], Loss: 0.0623\n","Training accuracy 97.6436 %\n","Time: 205.30137944221497\n","epoch 80\n","Epoch [80/100], Step [5/10], Loss: 0.0472\n","Training accuracy 98.0326 %\n","Time: 206.74939918518066\n","Epoch [80/100], Step [10/10], Loss: 0.0649\n","Training accuracy 97.2483 %\n","Time: 207.9101369380951\n","epoch 81\n","Epoch [81/100], Step [5/10], Loss: 0.0462\n","Training accuracy 98.0844 %\n","Time: 209.3707673549652\n","Epoch [81/100], Step [10/10], Loss: 0.0725\n","Training accuracy 97.0954 %\n","Time: 210.53815627098083\n","epoch 82\n","Epoch [82/100], Step [5/10], Loss: 0.0419\n","Training accuracy 98.2595 %\n","Time: 211.9895088672638\n","Epoch [82/100], Step [10/10], Loss: 0.0627\n","Training accuracy 97.3317 %\n","Time: 213.15290474891663\n","epoch 83\n","Epoch [83/100], Step [5/10], Loss: 0.0490\n","Training accuracy 98.0059 %\n","Time: 214.61070132255554\n","Epoch [83/100], Step [10/10], Loss: 0.0696\n","Training accuracy 97.0793 %\n","Time: 215.77294373512268\n","epoch 84\n","Epoch [84/100], Step [5/10], Loss: 0.0412\n","Training accuracy 98.3549 %\n","Time: 217.21252584457397\n","Epoch [84/100], Step [10/10], Loss: 0.0523\n","Training accuracy 97.6769 %\n","Time: 218.37065768241882\n","epoch 85\n","Epoch [85/100], Step [5/10], Loss: 0.0515\n","Training accuracy 97.8181 %\n","Time: 219.814208984375\n","Epoch [85/100], Step [10/10], Loss: 0.0708\n","Training accuracy 96.8476 %\n","Time: 220.9739797115326\n","epoch 86\n","Epoch [86/100], Step [5/10], Loss: 0.0402\n","Training accuracy 98.2640 %\n","Time: 222.42984676361084\n","Epoch [86/100], Step [10/10], Loss: 0.0538\n","Training accuracy 97.8933 %\n","Time: 223.5883927345276\n","epoch 87\n","Epoch [87/100], Step [5/10], Loss: 0.0486\n","Training accuracy 97.8951 %\n","Time: 225.02605509757996\n","Epoch [87/100], Step [10/10], Loss: 0.0589\n","Training accuracy 97.3965 %\n","Time: 226.18326377868652\n","epoch 88\n","Epoch [88/100], Step [5/10], Loss: 0.0494\n","Training accuracy 97.8504 %\n","Time: 227.65060448646545\n","Epoch [88/100], Step [10/10], Loss: 0.0428\n","Training accuracy 98.2031 %\n","Time: 228.81637525558472\n","epoch 89\n","Epoch [89/100], Step [5/10], Loss: 0.0402\n","Training accuracy 98.2985 %\n","Time: 230.26019549369812\n","Epoch [89/100], Step [10/10], Loss: 0.0485\n","Training accuracy 97.7718 %\n","Time: 231.4187548160553\n","epoch 90\n","Epoch [90/100], Step [5/10], Loss: 0.0463\n","Training accuracy 97.9804 %\n","Time: 232.88018131256104\n","Epoch [90/100], Step [10/10], Loss: 0.0470\n","Training accuracy 98.0041 %\n","Time: 234.04256916046143\n","epoch 91\n","Epoch [91/100], Step [5/10], Loss: 0.0544\n","Training accuracy 97.5785 %\n","Time: 235.4757947921753\n","Epoch [91/100], Step [10/10], Loss: 0.0438\n","Training accuracy 98.0696 %\n","Time: 236.64445734024048\n","epoch 92\n","Epoch [92/100], Step [5/10], Loss: 0.0416\n","Training accuracy 98.2279 %\n","Time: 238.111576795578\n","Epoch [92/100], Step [10/10], Loss: 0.0405\n","Training accuracy 98.3447 %\n","Time: 239.2695813179016\n","epoch 93\n","Epoch [93/100], Step [5/10], Loss: 0.0448\n","Training accuracy 98.0611 %\n","Time: 240.75209140777588\n","Epoch [93/100], Step [10/10], Loss: 0.0441\n","Training accuracy 98.1484 %\n","Time: 241.9120454788208\n","epoch 94\n","Epoch [94/100], Step [5/10], Loss: 0.0476\n","Training accuracy 97.9481 %\n","Time: 243.35244345664978\n","Epoch [94/100], Step [10/10], Loss: 0.0629\n","Training accuracy 97.2783 %\n","Time: 244.5126223564148\n","epoch 95\n","Epoch [95/100], Step [5/10], Loss: 0.0446\n","Training accuracy 98.1111 %\n","Time: 245.960294008255\n","Epoch [95/100], Step [10/10], Loss: 0.0465\n","Training accuracy 98.0950 %\n","Time: 247.11547446250916\n","epoch 96\n","Epoch [96/100], Step [5/10], Loss: 0.0455\n","Training accuracy 97.9627 %\n","Time: 248.57420372962952\n","Epoch [96/100], Step [10/10], Loss: 0.0378\n","Training accuracy 98.3874 %\n","Time: 249.734623670578\n","epoch 97\n","Epoch [97/100], Step [5/10], Loss: 0.0491\n","Training accuracy 97.8591 %\n","Time: 251.1883945465088\n","Epoch [97/100], Step [10/10], Loss: 0.0391\n","Training accuracy 98.3701 %\n","Time: 252.35375213623047\n","epoch 98\n","Epoch [98/100], Step [5/10], Loss: 0.0415\n","Training accuracy 98.2362 %\n","Time: 253.7958710193634\n","Epoch [98/100], Step [10/10], Loss: 0.0435\n","Training accuracy 98.1157 %\n","Time: 254.95259284973145\n","epoch 99\n","Epoch [99/100], Step [5/10], Loss: 0.0428\n","Training accuracy 98.1806 %\n","Time: 256.4071135520935\n","Epoch [99/100], Step [10/10], Loss: 0.0390\n","Training accuracy 98.2873 %\n","Time: 257.56322598457336\n","epoch 100\n","Epoch [100/100], Step [5/10], Loss: 0.0429\n","Training accuracy 98.1765 %\n","Time: 259.0134074687958\n","Epoch [100/100], Step [10/10], Loss: 0.0408\n","Training accuracy 98.1851 %\n","Time: 260.1901617050171\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhqrQrDoeA0x","executionInfo":{"status":"ok","timestamp":1617414192535,"user_tz":420,"elapsed":5643,"user":{"displayName":"Anjali Gopal","photoUrl":"","userId":"16177467827720494104"}},"outputId":"e443480c-6485-4426-ba9a-74f41a7af4ba"},"source":["##################################\n","#                                #\n","#          VALIDATION            #\n","#                                #\n","##################################\n","\n","model.eval()\n","\n","# Initializing variables\n","validation_loss = [];\n","validation_predictions = [];\n","validation_labels = [];\n","validation_accuracy = [];\n","validation_micrographs = [];\n","\n","print(\"Starting validation\")\n","for i, (local_batch,local_labels) in enumerate(val_loader):\n","    \n","    # Loading data\n","    local_batch = local_batch.float();\n","    local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n","    \n","    # Evaluation\n","    outputs = model.forward(local_ims)\n","\n","    # Reshaping for loss\n","    x = local_labels.shape[-1];\n","    y = local_labels.shape[-2];\n","    local_batch_size = len(local_labels);\n","\n","    # Validation loss\n","    local_labels_for_loss = torch.reshape(local_labels,(local_batch_size,x*y));\n","    outputs_for_loss = torch.reshape(outputs,(local_batch_size,outputs.shape[1],x*y))\n","\n","    loss = criterion(outputs_for_loss, local_labels_for_loss)\n","    validation_loss.append(loss.tolist())\n","    \n","\n","    # Predictions via max     \n","    _, predicted = torch.max(outputs.data, 1)\n","    predicted_reshape = torch.reshape(predicted,(local_batch_size*x*y,1)).cpu().numpy();\n","    local_labels_for_score = torch.reshape(local_labels_for_loss,(local_batch_size*x*y,1)).cpu().numpy();\n","\n","    score = metrics.accuracy_score(predicted_reshape,local_labels_for_score);\n","    validation_accuracy.append(score)\n","\n","    validation_predictions.append(predicted.cpu().numpy());\n","    validation_labels.append(local_labels.cpu().numpy()); \n","    validation_micrographs.append(local_ims.cpu().numpy());\n","\n","# Calculating Accuracy Score\n","val_weights = [];\n","\n","for i in validation_predictions:\n","  val_weights.append(len(i)/len(val_dataset))\n","\n","print('Validation accuracy {:.4f} %'.format(100 * np.average(validation_accuracy,weights=val_weights)))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Starting validation\n","Validation accuracy 97.8208 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ISM00HfnqWJt","executionInfo":{"status":"ok","timestamp":1617414222652,"user_tz":420,"elapsed":773,"user":{"displayName":"Anjali Gopal","photoUrl":"","userId":"16177467827720494104"}},"outputId":"88ee96ac-df7c-40ce-ef4c-5311f8ae7574"},"source":["##################################\n","#                                #\n","#          TESTING               #\n","#                                #\n","##################################\n","\n","print(\"Test dataset length: {}\".format(len(test_dataset)));\n"," \n","model.eval()\n","\n","# Initializing variables\n","test_loss = [];\n","test_predictions = [];\n","test_labels = [];\n","test_accuracy = [];\n","test_micrographs = [];\n","\n","print(\"Starting testing\")\n","for i, (local_batch,local_labels) in enumerate(test_loader):\n","    \n","    # Loading data\n","    local_batch = local_batch.float();\n","    local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n","    \n","    # Evaluation\n","    outputs = model.forward(local_ims)\n","\n","    # Reshaping for loss\n","    x = local_labels.shape[-1];\n","    y = local_labels.shape[-2];\n","    local_batch_size = len(local_labels);\n","\n","    # Validation loss\n","    local_labels_for_loss = torch.reshape(local_labels,(local_batch_size,x*y));\n","    outputs_for_loss = torch.reshape(outputs,(local_batch_size,outputs.shape[1],x*y))\n","\n","    loss = criterion(outputs_for_loss, local_labels_for_loss)\n","    test_loss.append(loss.tolist())\n","    \n","\n","    # Predictions via max     \n","    _, predicted = torch.max(outputs.data, 1)\n","    predicted_reshape = torch.reshape(predicted,(local_batch_size*x*y,1)).cpu().numpy();\n","    local_labels_for_score = torch.reshape(local_labels_for_loss,(local_batch_size*x*y,1)).cpu().numpy();\n","\n","    score = metrics.accuracy_score(predicted_reshape,local_labels_for_score);\n","    test_accuracy.append(score)\n","\n","    test_predictions.append(predicted.cpu().numpy());\n","    test_labels.append(local_labels.cpu().numpy()); # Don't need this if we never shuffle the validation set, but added for consistency\n","    test_micrographs.append(local_ims.cpu().numpy());\n","\n","# Calculating Accuracy Score\n","test_weights = [];\n","\n","for i in test_predictions:\n","  test_weights.append(len(i)/len(test_dataset))\n","\n","print('Test accuracy {:.4f} %'.format(100 * np.average(test_accuracy,weights=test_weights)))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Test dataset length: 27\n","Starting testing\n","Test accuracy 94.2025 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nUBdz3p3pyDc","executionInfo":{"status":"ok","timestamp":1617414339914,"user_tz":420,"elapsed":260,"user":{"displayName":"Anjali Gopal","photoUrl":"","userId":"16177467827720494104"}}},"source":["# Saving the test predictions output\n","\n","test_predictions_array = [];\n","\n","for i in range(len(test_predictions)):\n","  for j in range(test_predictions[i].shape[0]):\n","\n","    test_predictions_array.append(test_predictions[i][j,4:204,:])\n","\n","np.save(\"/content/gdrive/My Drive/Herrlab/projects/segmentationproject/datasets/mtl/test_predictions_array\",test_predictions_array)"],"execution_count":31,"outputs":[]}]}